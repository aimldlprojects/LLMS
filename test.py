
(50) LLM Pretraining Course: Build LLMs From Ground Up - YouTube
https://www.youtube.com/watch?v=21EejfdJYIU

(00:00) hello everyone welcome to the day seven of the llm Mastery course in this video I'm going to show you all how you can pre-train your own llm from groundup with the help of this Zero to Hero llm pretaining guide this guide is all you need for you to pre-train your llm today itself Yes you heard it right you can pre-train your own llm today with the help of this guide even if you are an Enterprise or an individual doesn't matter matter let's start with the video now in pre-training nlm the most important part is creating a training
(00:38) pipeline rather than coding it framing a pipeline which is very scalable and also efficient enough for your model to perform well is very important so that your llm becomes very robust let's see how a training pipeline should be constructed for pre-training any custom llm any pre-training of of llm will contain five stages starting with data collection data processing training llm llm evaluation finding LM we'll be seeing these sections in detail where what you need to do in each of the section let's start with data collection
(01:20) what is data collection data collection is the process of gathering large volumes of diverse text Data with pre-training this definition can't be like more this is the most AB definition when you come to data collection for pre-training because two keywords are very important you need large volumes of data and also it should be very diverse in nature because that is when your llm will understand what is the language and what are the semantics involved in the language how it varies syntactically everything is understood only when you
(01:53) provide diverse variety of large volumes of data so what is the importance of this like I said this will form the foundation of your language model so if the foundation is not good you are not able to adapt for any of your use case so this is going to form the foundation for your llm what is the goal of the stage it is to acquire a broad range of human knowledge sample data so the data should represent a broad human knowledge and also how to use a language okay it should contain more about knowledge also say you know how to construct a language
(02:32) what are the sources in which you can collect a pretaining data first you can crawl it from the websites by using common crawl libraries or you can also use custom web scraping you can take books data s uh from guten bag library or also you can go for uh Google Books there are some other options as well which you can uh check it out in open source uh data set hubs like hugging face and kaggle you'll have a wide variety of data those can be considered as well and if you are going for news kind of a data so that your llm gets
(03:11) updated with current affairs you can go for news apis and blogs and when you go for domain related for example what if I want to pre-train my own legal llm okay so I need to First find legal documents there are lots of documents like government reports uh SEC filings there are lots of reports like that which you can cover for your legal llm and when you go for a scientific kind of an llm you can obviously go for arive arive is open source and then you have uh PubMed as a big source for medical domain now what are the thing you need
(03:49) to consider when you are collecting your data first volume much must be in terabytes usually it will be in terabytes if you collect how it is supposed to be collected so here there are two challenges first you need to collect terabytes of Text data that is one you need to process terabytes of Text data that is two and then you need to store terabytes of Text data that is three okay all of these are something you need to consider and make an uh infrastructure such that it can handle this volume next like I said large volumes of
(04:26) diverse data so that becomes your next consideration make sure that your your data is having wide variety of domains and dialects and if you are training multi- language multilingual model so it should contain multiple uh languages and even in multiple languages it should cover a wide variety of domains and dialects that is important okay and then next we have the quality so when you are providing data like I said this is going to form the foundation of your llm so make sure that the quality is very High by removing the low quality spam content
(05:04) from your pre-training data set or even if you collected from any data source we won't know right uh the reliability of those data set is always in question so make sure that that is considered and if you are working with uh some organization then lot of ethical considerations will uh come into the picture like for example you need to see the license of the data set and commonly you need to see the Privacy uh it is a very important ethical consideration make sure that no private informations are went for
(05:38) example personal user information and so on and so forth and also when you go for website datas there will be always a lot of bias inside that if you see in real world in Internet itself there are lots of bias which is shown uh in any blogs if you search for okay for example if you search a Content everyone is giving their perspective which means there is an indirect bias induced into it so make sure that those bias are reduced as much as possible so what are the challenges here when you're considering these like
(06:10) I said first storage and processing becomes a big Challenge and then when you have multiple data sources there is a high possibility that your data might be uh available in multiple sources same data in multiple sources so you need to write a complex logic to remove all those duplicates and then write a processing pipeline which will handle like different file formats regardless of how complex it is HTML PDF text CSV it doesn't matter what kind of a file format it is and then file encoding so there are possibilities for
(06:45) example if you go to charb it can generate emojis right so those it are able to generate because it is UTF 8 encoded so if you want your model also to do that so you need to make sure that you can process UTF encoded file format datas and all and then you have asky file format as well that is uh another format next very important thing is data sets with updated information if you want your llm to be relevant to the situation which is there in the current world you need to ensure that it is updated with a lot of information so
(07:25) that is why if you see uh any big organization will say this is their cut off dat date the more recent the cut of date the better it will be adapted to current world scenario and legal and ethics so you need to make sure that you abide by the copyright law and also make sure that the personal information is anonymized so that you will use it for fair next you know how you can collect the data you know the challeng is invol in it but like I said there are lots of processing which is required that is what we discussed here right d
(07:58) duplication D duplication is one of the processing methods but there are many other processing because your data source is not always reliable so the next session is about data pre-processing how will you process your data so first what is data pre-processing transforming a raw text Data into a clean structured format which is suitable for model trading is known as data preprocessing again a short very descriptive and self-explanatory definition follow by which you have the importance of the stage I don't think we need to emphasize
(08:35) a lot on this important stage because how much ever we create a very complex architecture with whatever state of art technology it is there at that time if you don't have a good data you are not going to get good model right so imagine you are a very intelligent guy or like you are an intelligent person and your teacher is teaching you something wrong okay you are so intelligent so you'll know okay like this is the teachers guideline so for me that is correct okay if I ask you something about that you'll
(09:12) be able to answer it because you're intelligent you can remember a lot you'll answer me correctly according to your teacher but you are fed wrong information which means you're saying something wrong with high confidence that is not a mistake of you it's a mistake of your teacher right so similarly if you give wrong data or low qu data to the llm it is not the mistake of the llm to say wrong it's the mistake of the person who is creating that right so make sure that the data is quality ured okay so that is the most important
(09:46) uh aspect of data preprocessing along with that you need to ensure consistency and relevancy your data should be consistent with your um use case in which you want to train it for for example if I want to train it for a legal domain and if I'm training it for a medical domain it doesn't make any sense right and let's say you're instruction tuning it on medical domain but you pre-trained it with General domain how it will be able to adapt to the uh small terms which is not something it would have seen in the
(10:18) pretending stage right so to maintain that consistency a simple format for example one might be HTML one might be uh an XML one might be nor text if you're giving different formats your LM will be confused what to do right so those are the things which are coming under consistency and relevancy like I said you need to make sure that your model is relevant right to any scenario so those are the importance of the stage what are your goals first removing noise and irrelevant information if that is irrelevant you'll just remove it and if
(10:55) it is a noise you'll remove it like I said if that is not something related to your use case just remove it again that is one example of relevance and consistency there are lots of other examples I'm just saying a domain specific nature because if you're going for a general pre-training right um maybe if you want to do a learning out of it you can pre-rain your own llm uh but most of the pre-training of llm will be happening only if you want a custom domain pre- training so I'm more emphasizing on custom domain
(11:27) okay probably you know uh in days upcoming days will custom pre-train our own llm for a custom domain of our choice all right so let's see how that is done as well probably something like a financial domain or legal domain or Healthcare we'll try to do that maybe you can put it in the comment section on which domain you want the video to be okay so that is the first goal second standardized text format this is to ensure the consistency right so if you have a standardized text format it means your data is consistent
(12:04) among its uh samples and then next you have uh preparing data which is efficient for uh processing by the model so this ensures that your model has high data quality and it is very uh process very much processible by your model so in data processing uh one of the most important step is text cleaning okay in text cleaning there are lots of steps which you can do for example removing HTML tags removing stop words handling special characters either removing it or encoding it in some different format and then you have
(12:39) standardization of text um and then D duplication of content all of these uh fall under text cleaning and these are simple steps right you can just do uh like each one of these can be done with the help of regex probably not D duplication D duplication is something you need to do U with some complex logic if you do a simple DX it won't work but others can be done with a simple reex itself now the next thing is vocabulary creation vocabulary is something like your model's knowledge uh to some extent uh can be understood with help of the
(13:14) vocabulary right it is like if I ask you what are the words you know you list out your words right the more you know the better you can express what you know uh that is the idea of any language right the language is there to convey your knowledge so more the words you have better you'll be able to express what you know that can be understood by vocabulary so vocabulary creation all almost becomes a very crucial stage here what is vocabulary creation first it is a process of compiling unique tokens from the data set so whatever data set
(13:50) you have you'll take all those unique tokens and that becomes your vocabulary here you need to determine your vocabulary size first okay and for example uh only I I can remember only 25,000 words all right I'm speaking in Indian English here so if someone like an American is speaking you'll be able to understand that it is an American who is speaking here there are two reasons for that one is the accent of the person that is one one thing and the another one is the way they use their words it will be very much different than how an
(14:24) Indian will use his words right so vocabulary shows a lot of things and here what differentiat is it their vocabulary is different my vocabulary is different I can speak Tamil I can speak English I can speak Hindi to a bit which means my vocabulary is this much okay so if I I'm going to learn new words I'll forget some of my old words right so determining that vocabulary size is very important how much words I can remember efficiently so common uh vocabulary size now it has become around 128k um but it
(15:03) differs um among the llms and what the use case demands so for example if it is a legal domain or a medical domain you need to make sure that those technical terms are there as a single token right that will reduce the complexity of the llm to generate tokens correctly and also understand a sequence of tokens correctly for example if it is pneumonia in medical domain if you're having new as one uh token and then monia as another token maybe Mo and then yeah three tokens right three tokens understanding three tokens in a sequence
(15:42) is harder than understanding a single token so make sure that your vocabulary has you know uh the correct number of tokens because that is going to determine the knowledge of the llm and then uh handling out of vocabulary words is very important so for this uh there are special tokens first padding you need to add those tokens padding is used for uh padding sequences to a fixed length because llms can be trained only with the fixed length across the bad size so if there is a length mismatch you need to add pad tokens and then you
(16:18) have Unk tokens which is to which is used to represent unknown and rare words and then we have the CLS token which is often used to uh as a special classification token basically this token will have an overall information about the whole sequence and then SCB token which is to separate different parts of the input for example I want to separate that this is a paragraph and then this is the next paragraph to show that we'll have a separate token next we have another important stage which is tokenization we have already made a
(16:49) detailed video on different types of tokenization which you can do so if you haven't checked that video please make sure that you have checked it because this is a very important stage um because you need to create your own tokenizer vocabulary creation also comes under tokenization because based on your vocabulary only tokenization will happen Okay so make sure that you watch that video The Links will be in the description so what is tokenization tokenization is the process of breaking the text into smaller units and this
(17:16) smaller units is known as tokens what are the types of tokenization which are there word based subword based and then character based these are the most three common uh formats of tokenization so so while you're creating your tokenizer what you'll do is first you'll take the data you'll train it on your domain and then you'll reduce the number of words to your vocabulary and then what you will do is you will uh pre- toonize your sentence okay so it can involve uh normalization reducing the sequence
(17:49) length or so on and so forth once that is done you will perform your splits and then your vocabulary will be created okay and based on this vocabulary you will tokenize your uh data set at run time and also at training okay so I hope you understood uh why vocabulary creation and tokenization are together so what is the importance of tokenization in the model performance first it will balance the vocabulary size with the token's informativeness like I said if it is a domain specific one right you need to make sure that
(18:23) your vocabul covers like you know the token informativeness correctly if there is a technical term make sure that those are single tokens as much as possible or even at two tokens okay so and then we have another important step uh reason which is to handle out of vocabulary tokens so there are lots of ways you can handle out of vocabulary words and one of those ways is to use bad encoding method again if you haven't watched the video please watch it because I've shown how B per encoding Works theoretically
(18:55) and also have shown it programmatically okay so please please make sure that you have watched that video Once you complete this video next you need to implement your tokenizers first either you can use preent organizers which are there in hugging pH nltk Spacey depends on your use case so if you're going for word based or character based you have your options in Spacey and nltk but if you're going for sub kind of advanced dooners you have your options in hugging face or you can use your own custom tokenizer that is more than fine next it
(19:28) is to encode your input at run time because you are now created your tokenizer you need to use this to encode your input at real time so what is this step all about it is about creating your tokens into numerical IDs so you'll have your vocabulary in your vocabulary each token will have a number to it a numerical ID so to speak which will be commonly turned termed as input ID okay and so basically it is to create the input IDs for your tokens and then once that is done you'll create the input sequence for the model itself where raw
(20:02) text will be converted into sequences of tokens IDs and uh then what you'll do is you'll handle the variable length inputs so like I said nlm needs every sequence to be of the same length in the batch okay across batches it is not important but inside the batch you need to have every input to be at the same length so if it is lesser than the length you want so it should be padded so if it is greater than the length you want it should be truncated and if you do padding you need to make sure the attention masks are
(20:36) there because that is the one which will say like you know you have padded these tokens so that becomes very important so when you're handling variable length inputs these will come into the picture and then next is to implement efficient encoding pipelines when you're having a large number of data for training it is very important that you paralyze this encoding process because when you pre-training it the encoding process happens in CPU so you need to make sure that that is fast enough and it doesn't
(21:08) slow your pre-training pipeline okay and then another thing is when you have large number of data sets like I said you're using CPU and RAM you need to make sure that this process is optimized for memory uh which is RAM and then processing speed how to use the CPU efficiently so these are like you know runtime tokenization step or which is normally known as encoding input stage what are the ways you can augment your data because like I said diversity is very important right so how you can create the diversity that stage is named
(21:43) as data augmentation first what you can do is you can do synonym replacement so for that you can use something like word Nets or wordings to find the words which are closer to it which uh replace those words but doesn't change the semantic meaning okay semantically both are same but the words are different so that the model will understand okay it is not like I should always use the one word there I can also use another word to show that okay and then there is another stage known as back translation most of
(22:16) these data augmentation are for increasing the amount of data and make it more diverse okay so for example you have uh data what you can do is you can translate to another language for example I have some in English I can translate it to Tamil and then back translate into English so depends on the knowledge of uh llm or whatever model I have for translation it will try to translate it in a different way okay same meaning is converted in a different uh sequence but both are right right so in this case you have two data which
(22:49) means you're increasing the volume of data and you're increasing the diversity of the data as well and then next comes random insertion or deletion so this step is known as know like noising or Den noising kind of stage where what you will do is you'll just insert random words from the vocabulary uh insert into your data set okay so it shouldn't make any sense because in real time when you work you don't explain your things correctly right for example if you give to char with some spelling mistakes or
(23:19) even some filler words it is able to understand it because it is trained in a robust manner that it can handle those kind of sequences as well okay that is what is going to sh your different writing styles like I said Indians will have a lot of fillers in it and their English will be very simple but uh an Australian English is something very different than an Indian English which is very very different than an American English because they will be more eloquent right so to simulate these different writing styles you can just
(23:54) randomly insert and delete some words from the vocabulary but make sure that you do it uh correctly because if you you do something wrong your llm is going to be like you know uh I'm not able to understand what are you trying to say here something like that now uh what are the importance for you to do the data augmentation you need to increase your data size and diversity that is one like I said already and then uh when you do these two your model like I said it will be able to handle various inputs so it
(24:25) is becoming more robust and since you have a variety it prevents overfitting as well and here I left a point for Generation you can also use bigger llms okay so for example proprietary apis like Char or Claud make sure that you follow their licenses that is there but those can be used as well now we are completed with the main stage which is data related data collection and data processing at the next stage is to train your llm the first stage in training the llm is designing your architecture so what can you do to design Anam
(25:02) architecture for you to know that you need to First understand the evaluation evolution of your language model architecture first it was just n gr model just predict the next uh gr like you know two two words we just try to understand and predict the next two words something like that then it came with the neural network architecture and then came Transformer architecture in 2017 and the world is now revolutionized with llms like Lama 2 mistal and then llama 3 there are a lot of things L lately there was a model releas known as
(25:34) Minal something like that there are lots of models which are released on these lines right these are the ways the architectures are revolutionized so you need to make sure that you follow the evolution as well and adapt your llm to those Evolutions so the design goals so whatever goals uh these llms had to revolutionize the arch ures and revolutionize the worlds again is first it needs to capture the long dependencies long range dependencies which is nothing but the contact length longer the contact length better it can
(26:09) remember a lot of things for a longer time right so that is very important second it should be scalable okay it should be scalable for billions of parameters the architecture should be scalable for billion parameters what does it mean here you see uh when you are training your llm right make sure that you know why Transformers were released Transformers were released because it has this paralyzation capability okay without paralyzation if you're going to make it everything sequential it is not going to be uh easy
(26:43) for you to train a billion parameters okay make sure that your architecture is scalable in that way for example if you go through codes of Facebook right they would have used something known as row parallel linear column parallel linear so they would have done a lot of paralyzation they would have emphasized on that that is why they are one of the market leaders right and also your model should be able to process large amounts of data so what does that mean so if you are having a large amount of data right your
(27:13) model should be able to remember that much so the approximate SI that might involve the approximate sizing of your number of layers the embedding Dimension lot of things will come under finding the efficient processing of large amounts of data so as an overview for Transformer architecture it contains embeddings layer which will convert token IDs into D Vector representations which is nothing but embeddings personal encoding which will provide the sequence order information saying like this token comes after this token and then you have
(27:46) multi-ad attention mechanism which will allow you to focus on different parts of the input and then you have feed forward uh which will process the attention output combine the information which was obtained from multiple attention heads together and then finally layer normalization which is to stabilize the training process so I'm just giving an overview here because I've made two detailed videos on Transformer architecture one was to make you understand how Transformers work and then another one is to implement that
(28:17) Transformers from scratch line by line where I would have shown the math equation and then I would have converted into code so if you haven't checked those two videos as well please make sure that you check those two videos for all the videos the playlist link is in the description So lately there have been a lot of architectural advancements so here are some of those the main components in feed forward Network sorry in your Transformers are position encoding attention and feed forward right in all of these components there
(28:51) have been uh let's say huge number of advancements to increase the performance and eff efficiency of the architecture what does it what does the efficiency of the architecture mean it is about uh the latency and the memory consumption when it comes to the efficiency and performance means the accuracy okay so here the position encoding for uh for personal encoding there are varieties of personal encoding and some of those important types are absolute encoding in which there are types like sinusoidal binary
(29:27) algebric and then relative you have um Transformer Excel method shal method and so on and so forth in there and in those relative encoding methods there have been a huge number of advancements because with relative encoding only you'll be able to handle a longrange dependency of text all right so for those came rope and Alibi two of the most commonly used and most advanced methods and then uh to bridge the gap between absolute and relative there is mixture potion in coding as well don't worry about these types if
(30:02) you don't know about these types no worries Again the video links are in the description please make sure that you watch those whatever I speak about architectural advancements now for all of those architectural advancements I've explained it detailed in detail in the previous days okay so make sure that you watch that same goes for attention multi-ad multiquery Multi Group query so multier and group query came for increasing the efficiency of the Ure and sliding window andent dangled came for increasing the performance of the
(30:32) architecture okay in feet forward directly there is not a lot of optimization because feed forward is just linear layers but feed forward consists of activation functions and lat there have been a lot of work around activation functions there is a family of activation function which came recently that known as glue activation functions there is J glue and swg glue which is common which are just glue versions of J and swish okay so you can also use jelu and swish those are very common but if you want you can also drive with j jlu and swi
(31:06) loop the second thing is using those feed forward as Moe okay this is something very common right now if you have multiple languages to handle those people go for a mixture of export kind of an approach that can be done as well all right choice of advancement can be customized as per your requirement if you want it to be more on the performance side you'll go for a more aggressive approach and if you want it to be the more on the efficient side you'll be more focused on like you know reducing the complexity of the
(31:40) architecture reducing the memory memory consumption and so on and so forth make sure that you select the appropriate components next comes the model scaling now you have a basic architecture you need to scale the model right in that the first parameter is to increasing the model size large language model the name just that it should be large right so what are you going to do to increase the model size first thing is to add more layers to increase the depth why you need to do that whenever you do something make sure
(32:11) that you ask the question why what when how if there is an answer possible for all the questions and in these kind of questions as steps right you need to ask why you need to do that first why you need to do adding of more layers to increase the depth I can just increase the embedding Dimension it would have been uh bigger right why you need to increase the depth it is to increase the reasoning level the more deep it goes it can drill down to different nuances to the minute bit okay so that is why you need to increase the depth if you see uh
(32:47) the architecture which are very good with reasoning right they'll have a very deep Network so even with small parameters people are now using 20 to 25 layers why is it important to have that much layers because the more down you go you'll go to the minute details okay but it is also important that you increase the embedding size or the hidden size because with increasing of the embedding or hidden size you'll capture more information you'll remember more information all right so that becomes a
(33:19) very important step as well so you need to find a tradeoff between these two like you know I have an optimal number of let's say three billion parameters I need to find an optimal number of uh layers and the hidden size of it okay these These are under the consideration that you want to create your own architecture if you're referring someone's architecture they have been already optimized so you need not do a lot of things there but if you want to do something by yourself this are something you need to consider and then
(33:46) next is to make sure that the trade-offs are correct between the model size and the computational requirements because with the increase in the model size your trade-offs uh are going to be high because the computation requirements are always going to be a bottleneck right so make sure that those trade-offs are correctly managed and then we have efficient scaling techniques so don't worry if you are just like you know confused how I can make sure that these trade-offs are uh efficiently done because we are going to prain our own
(34:17) llm very soon so one once we do that you will understand uh as a code part how I'll do that something like that okay next efficient scaling techniques when you have a large number of parameters you need to make sure that your model is scalable right what are the things you do you can do for that first add SP attention mechanisms if you go for a dense method always it is going to be more complex to compute so you can go for a sparse attention mechanism and then you can share your parameters across layers and
(34:52) also like I said is a very good method because at WR time only The Limited number of experts is going to be loaded in the memory now this is how uh you need to design your architecture you have your scalings uh part completed you need what are you know like what are the architectural advancements you need to include in your pipeline based on your requirement all of those are done right you're making it scalable next what you need to train this model right that is what we are going to see now so introduction to llm training so here we
(35:27) are focusing on on pre trining because pre-training is the process of teaching the model to understand and generate language that is the main idea of pre-training okay any task you do in pre-training the aim of this task should be to understand and generate language SL domain because there are lots of scenarios where you'll pre-train it just for the domain okay so for example I want to pre-train it for medical domain something like that so it is to either pre-train uh to extend and generate tokens related to The Domain or the
(36:00) language what is the importance of this step this is the core step in creating a versatile language model Char is very versatile right how it is possible because they correct they covered wide range of domain wide range of uh languages everything wide range of dialects and so on and so forth and they trained it on top of those versatile data set right so that is why again if you see I'm talking about training and emphasizing on data set so that is why data processing uh section had that much emphasis by having more number of uh
(36:36) time dedicated to it so what is the goal of pre-training here it is to develop a model which has a broad language understanding and you need to enable zero shot and few shot learning capabilities so if you for example if you give to charb if Char doesn't know something right even if it is not in the training data set it will be able to answer your question even if it is not able to answer it directly if you give some references to it it will be able to generalize itself so that is what is enabling zero shot and few shot learning
(37:09) capabilities next is to achieve good performance across various NLP tasks in NLP there are uh tasks like for example you can put um something like summarization translation question answering and so on and so forth as tasks you need to adaptable you need to be adaptable for all those tasks all right so that is very important what are the tasks in which you can prain on first there is mask language modeling where you'll uh randomly mask tokens in your input and the model will be trained to predict those tokens next is to predict the
(37:45) sequence so here the model will predict which is the sequence which will follow one another so by this it will have a sentence level understanding with mask language modeling it will have a word level understanding or token level understanding for that matter and then there is cost language modeling where what it will do is it will just try to predict the next token based on the previous token so now M language model also is about predicting random tokens and here it is about next token that is the difference in M language modeling you
(38:16) will do it use it for encoder models okay so what the task is is that the llm will be expected to understand and say what is the token based on on the tokens before and after the Mas token so that can be used with architectures like b t and so on and so forth but costal language modeling is about ensuring that your llm can say what is the next best token based on the previous token okay now let's see about Cal language modeling in detail because that is the task when which we'll pre-train our own llm what is Costa language modeling but
(38:52) cost language modeling is a task where the model predicts the next token in a SE quence using the preceding tokens so whatever tokens are there before it those are the ca see that is what is the good thing about technical terms mostly it will be um you know self-explanatory so to speak costell language modeling based on the cost your language model will say the next token all right example the sky is if I say the sky is right most of you would have say like the sky is blue the sky is beautiful something like that all of you say that
(39:31) because the sky is is a common sentence but if in future I would have said something like the sky is blue but with some Cloud covering mask Cloud covering okay now you'll say like black right how white is that but with some if I just left it without cloud covering you would say something else so if you give a context to what is the future it becomes MK language modeling which is an easier task to do because it has the context of what it needs to generate and then it will generate accordingly or predict rather here it is to generate right so
(40:08) it is a bit harder next why is this task important this is the task which is core to many NLP task and almost any task can be uh ped down to a AO regressive task some of the most common tasks in NLP uh which can be done with the help of cost language modeling are normal text generation you can make chat Bots you can make Auto completion and so on and so forth why you need to do this like I said this is goore to many LP Tas that is one and it will also enable your model to learn the structure grammar and context because if you see if you are
(40:43) speaking a language right you'll say it in a sequence you'll not think it forward and then answer it right you you won't do that you think it and then you'll just deliver it so your understanding of language as a structure as a grammar as a cont text is like a decoder model which will work on a cal language modeling task not like an encoder model okay so how does this work like I said it is an autor regressive process so it will predict tokens one by one so here in this task we train the model to minimize the cross entropy loss
(41:16) the cross entropy loss will say how much uh your model is uh differing from predicting the best token at a given time stamp okay because if you see language modeling if you to uh see it is just classification at the final layer okay it will try to classify what is the next token okay um here we'll use only pass tokens like I said that is what it makes it auto regressive and applications it can be text generation you can use it with uh Auto completion and so on and so forth but the challenge is like I said it is a more harder task
(41:49) because it has limited context and when you speak only the past there is an inbuilt uh bias to it okay if you don't explain why you are saying one thing in the future you are going to be biased to it all right so there is an inbuilt bias as well now with training of the llm right in a training loob it is not just that you'll have your uh llm you'll have uh other things like loss functions optimizers your uh learning rate schedulers everything will be there you need to make sure that you have initialized all of those correctly so
(42:24) the first thing is cross entropy laws for laws we'll use cross entropy loss because that is a standard loss function for classification tasks and here it is just that your model is going to predict or classify the next token based on the previous token okay so that becomes the standard loss for your uh costal language modeling training and here uh this will be applied for token wise in language model okay so at each token you'll try to apply this and that is very important okay and and then next you have optimization algorithm
(43:00) optimization algorithm is responsible for the back propagation and updation of weights and here there are two options either you can use Adam which is a normal uh Optimizer or a variant of it with the weight DK and that is admw and for learning rate scheduling we just to change the learning rate schedule uh learning rate in the future for example even if you are learning right at initial stages you learn it faster but going uh forward you learn it slower because you know a lot of things so you'll take time to understand those
(43:31) complex Concepts uh slowly and for that you'll have a learning rate slower right so that is done with the help of the learning R scheduling method and for that uh you need to choose your warming warm-up period which will uh gradually increase across uh time and then you have your DK strategies as well which will basically reduce your learning rates learning rates okay uh linear cosine St dek there are lot of methods for that as far as the training process is concerned um this is how a simple pyas training Lo will look like first you'll
(44:05) pass the data through the model for a number of EPO and then uh you will pass that data through the model which will perform the forward function forward function is to just uh get the model output and based on the model output which is known as logits you'll calculate the loss values you'll uh make sure that the optim optimiz the gradients are zero and then uh you will perform your back propagation once that is done you'll step the optimizer which will update the uh model parameters based on the law gradients you
(44:38) calculated with the l. backward and then the process will repeat this is how a typical py trining Loop will look like for any training uh processes even if it is an llm or not doesn't matter the process Remains the Same and for llm also the training Loop Remains the Same now for the training process details in detail right we already saw uh the training Loop in simple what happens the first step is to batching and input preparation so for this you'll use data set and data loader okay so the data set
(45:09) and data loader is basically used to create manyi batches of sequences which are processed tokenized and so on and so forth uh which is applied with padding masks attention mask uh everything like you know everything is done and it is ready for model next uh you coming on to the forward pass where you will send the model which sorry send the data which you created by batching and then you'll send it to the model right that is forward pass you'll get the model predictions which are logits and then you'll calculate the loss with the help
(45:40) of it comparing it to the ground truth and what you'll do is you will aggregate the loss across the batch so for a batch let's say you have 10 sequences for 10 sequences together you'll have a loss and then your back propagation will start where you will compute gradients for the loss with respect to the model parameters and for this you can use a automatic differentiation Frameworks like py senser flow so py senser flow uh they by default have that automatic differentiation running behind that is what it makes them a good
(46:10) deep planning framework right because uh the back propagation is one of the most important aspects of uh deep learning and here this makes them uh very good framework for deep planning next we have parameter update where your back propagation is done your gradients are calculated now you are ready to update your model parameters which is nothing but the weights and biases so that your loss is ready here what you'll do is you'll Implement a gradient clipping as well so that you can prevent exploding gradient
(46:43) issues now we are coming on to the some real stuffs um those training Loop we discussed till now it is very common right but with pre-training you have a thing which is very high uh Compu ation okay so you have a lot of bottleneck to it there comes the distributed training strategies you will train it in a distributed manner uh with a lot of gpus being involved in there okay so for example if I'm not wrong uh 32,000 gpus were used for llama 3 training if I'm not wrong 32,000 h00 okay so the hardware requirements
(47:21) are very high so you need high performance gpus or dpus and also you need fast interconnected Network bands for your distribut training because the data will be distributed through these networks so there are uh methods like like you know at behind in torch and all there is NV link and all okay and there is infin band and that is another option and you also require a large amount of RAM and fast storage because let me tell you how DDP Works in simple so DDP is a method where you'll create instances of your llm across gpus and you will create
(47:58) instances of your data sets across gpus again all right so across gpus now your data is shared right when you have 10 data tokenized together you're using the same CPU okay so it will consume a large amount of ram large amount of storage and so on and so forth so your Hardware re are requirements are going to be so high next coming on to the software stack what is the stack you need to are going to use for your training first you need to choose your deep learning framework what is the Deep learning framework in which you're going to train
(48:34) your llm on so like I said there is options like pych tens of flow PCH being my favorite you can also go for tens oflow that is completely your wish next comes an important uh library in your stat which is distributed training Library what is what are the things you're going to use for your distributed training there is Torin which is a default distributed training library of PCH but my favorite is deep speed Because deep speed handles a lot of things like for example if your memory is not enough they'll do something like
(49:04) CPU offloading and all making sure that uh your model is fitting in those consumer grade limited gpus that is why I said uh we'll pre-rain your llm regardless of what is your GPU because we are going to try to pre- chain an llm in collab Google collab okay we'll write the code in uh vs code but we'll train it in Google collab uh um next uh these are going to be trained for months uh weeks or even months like I said so you need to track your experiments right how the loss converges how is your gradient
(49:39) uh coming up so all of these needs to be noted for that you can use tracking tools like ml flow weights and biases or the most common one which is denser board also you need to decide whether you're going for cloud or on premises infrastructure based on your flexibility cost and control okay so if you want more flexibility less cost less control you'll go for cloud if you want less flexibility more control cost is doesn't matter then you'll go for a cloud on premise infrastructure that depends on
(50:11) your requirement right next like I said you'll do a lot of tracking right what are the things you'll track you'll track the training laws validation loss perplexity perplexity is a parameter which will say how much your model is confused when it says a new word for example if I'm saying uh let's say you understand Hindi or any languages to some extent certain extent right but people are now speaking it with English a lot so if I see some word new for example in French okay I I have a limited knowledge in French let's say if
(50:50) I see a new word I'll try to understand what could be the new word based on The Words which was available to me and if there are some English words in between I'll get the context from there right so my confusion rate will be lesser right so that is perplexity how much your model is perplexed seeing a new word so lesser the value better the performance and then you will monitor your learning rates lesser the better and then gradients as well how can you debug your model first you can check the gradients
(51:21) are correct okay it's not exploded or something like that let's say like if it is hundreds and thousands it means that there something going on wrong so you need to normalize something like that and also you can visualize the attention weights so mostly like you know uh it is more easier to visualize the attention weights with images with TT it is it is a bit harder okay in attention Maps you will see like you know where your llm is focusing because uh to see like you know where your model is focusing we are
(51:49) using C like the word as C which means you are having some Vision to it so visualization of attention weights with uh images are more easier than text but still you can do it for text as well and then there are possibilities for training instabilities so how you can handle that is first you can use gradient accumulation steps so gradient accumulation steps will ensure that uh your training time is getting lesser so basically the idea is that instead of doing B progression at every step you'll do it uh let's say after four steps okay
(52:22) something like that uh is what gradient accomplishment steps until then the gradient will be accumulated just accumulation is nothing but just getting added together okay and then you have mixed pression training so like again this is to make it more efficient and then uh make sure that your initialization of model parameters is very uh carefully you have initialized it because if you don't initialize it very well you'll get a lot of issues later and your loss will be something like Nan okay loss will be Nan or zero
(52:51) like you know if the gradient becomes n your loss will be zero so your model will not get trained another important aspect while you do pre-training is making sure that there is a checkpointing and resuming training uh capability in the pipeline because regardless of how much you handle the training pipeline correctly there is a high possibility that you will get some issues and the pre-training will be stopped right so you should be able to resume your training from that stage for example you trained it for 2 weeks okay
(53:20) and then there is a sudden issue and then your model got stopped maybe it is because of data maybe it is because because of ram overload maybe it is because someone restart your server anything right so you should be able to restart it so for that you need to First ensure that there is a regular model checkpointing done where you need to save your model weights along with Optimizer States because without op Optimizer States you can't uh resume your training okay so the frequency concentration should be there because
(53:50) reading and writing is always something you need to consider if you are writing let's say in every 100 steps it is going to take some time for it right so make sure that there is an optimal uh number where you are trying to save mostly I feel like you know uh one or like let's say 2 to 3% of steps is an optimal number is what I feel let's say you are training it for 10,000 steps right 1 person would be 100 if I'm not wrong yeah so 200 uh steps once is something very good number while it comes to pre
(54:24) trining okay and then uh make sure that you check uh you save the checkpoints in an efficient format for example shter checkpoints is something uh very good because uh it will be able to load efficiently across gpus and uh preing is something we do it in multi GPU right so that is something you need to consider and also make sure that you have compression techniques to store across uh based on your storage requirement and then like I said resuming training uh should be uh something you need to enable in your pipeline if you're using
(54:55) trainers like hugging phase or prou uh uh lightning those are something which have that capability by detail by default but if you're doing something by yourself then you need to make sure that that is handled correctly so for that you need to handle your learning grate schedulers data iterators and also making sure that the reproducibility of results is there like setting up the seed values correctly now we are now we are done with llm training but like I said llm pre-training is a very big process all right
(55:27) what if your model is not performing accordingly like you know you're expecting a result but down the line you're seeing after let's say five weeks or 6 weeks of training you're seeing that your model is not working your six weeks are wasted right so that is why people evaluate the llm at run time so there's a checkpoint saved they'll try to run on an evaluation pipeline how we need to do that we'll see now so before we go into LM evaluation I would just like to ask you all to just subscribe to
(55:54) the channel if you haven't subscribed and hit the Bell icon to get the notifications to get uh workshops kind of video like these because here you can see we have almost if you'll think like you know we have almost come to the end of the video right yeah like evaluation is come but no no no you haven't just see here out of 60 62 slides just 36 is completed which means you are around half way there right that is why I said this video is all you need for you to pre-train your own llm right so what are
(56:32) the methods in which you can pre-train your llm sorry evaluate your llm which is free train yeah that is that is the correct one there are five main methods one it is to use human evaluation the second one is to use llm assisted evaluation third one is to use match based evaluation the fourth one is to use benchmarks and then four fifth one is to use common metrics okay thanks to ARR AI for uploading such a good image uh to make my life easier explaining you all about evaluation because if you see um if it is green it
(57:10) means it is very easy if it is hello it is like not easy not tough and if it is red it is bad okay when it comes to cost if you see the most efficient method is to use metrics in every way it is to use metric except versatility because you must the the metrics you have right for example if it is accuracy or your recall you can rely on it but it is not versatile enough right so that is metrix for you all when it come to humans the cost is going to be very high imagine hiring a human to evaluate the llm outputs for let's say 6 months you'll be
(57:49) paying him monthly salary right so human evaluation is always costly but if you see that is the most versatile method right it is the most versatile method and the most reliable in my opinion okay after you have something like metrics and all because metrics are something which you have defined so you'll rely on that all as much as you rely on metrics you can also rely on human but human is given yellow here because human has their bias all right for example if I'm asking you who is your favorite cricketer
(58:27) or something like if your favorite cricketer let's say x is a worst cricketer is what the LM says what you will do no how can he say that it's wrong right so that's where humans become less reliable to some extent because the bias is there right when you go to llm assisted evaluation the cost becomes lesser than human the scalability becomes less issue than uh humans um still it is very versatile as much as human but the bias will be a bit higher right because here there is the bias of one person but the llms which are
(59:09) already there and trained it has the bias of Y people for example if y people worships one person as a good person if the reality is that he's a bad person that llm won't accept even if our llm says like know it is a bad person right so that is the issue with llms State evaluation next we have match based um as much as uh metrics is there match based is also a very good method um it is like exact match that's all okay using regx or exact matches is what uh match base methods are and then benchmarks it's like um how can I put it
(59:53) um if you have no way right to Inc in orporate any of these methods then you'll go for benchmarks benchmarks should be your like you know least starting point so to speak because it is average in everything it will be average in cost it will be average in scalability it will be average in versatility average in reliability right because all are done by someone now what is the importance of llm evaluation it is to assess the model performance and capabilities because if you don't do that what is the point of
(1:00:24) training you know if I'm if I'm going to say I just trained it correctly without any backing for it assessing the model performance and it fails in real time what is the point of training right and while you evaluate efficiently only you will know the strengths and weaknesses how is that because when you're evaluating and if you find that your model is failing at one place and it is working at one place it means failing is weakness and working is strength right so identifying that is an issue and then you have uh guiding for
(1:00:56) improvements and iteration because even if in if you see in Lama 3.2 paper they would have emphasized on iterative training right so iterative training works on the fact that you will identify the strengths and weaknesses and based on those weaknesses you will suggest improvements and that improvements will be uh used for further stages of training and then you should ensure that there is a responsibil development for example NSFW content should not be generated something like that right so uring these also comes under llm
(1:01:28) evaluation because uh you are um obligated to follow some mythics right so that is why next what are perplexity and what are the other LW metrics which you can use like I said uh while monitoring you can use perplexity and perplexity is defined as technically uh exponential of the Cross entropy loss Okay so the interpretation is that um lower the perplexity better the model performance is here the limitations is that it is not always correlated to the downstream task right um because it is just going to see how
(1:02:08) much it is confused how much it follows the task it doesn't matter for it so that becomes a limitation for it so that is why perplexity is used as just say another Advanced version of loss function okay and then you'll also monitor your training and validation loss that is there what are the Benchmark data sets and leader boards you can use first you can use glue and super glue that is something very common it is to uh show show like you know see how much your model understands natural language it includes uh task like
(1:02:43) sentiment analysis textual entailment and question answering and so on and so forth there is another variant of this data set itself which is super glue and that is a more challenging uh form of glue let's put it that that way because it will emphasize more on the complex reasoning tasks okay so here you know the understanding next how much it has reading comprehension like you know how much it can understand the context and then answer based on it for that you can do a question answering kind of an evaluation based on context and for that
(1:03:15) you have Squad Stanford question answering data set next there is Lambada which is language modeling Benchmark I don't know why they kept it as Lambada still now I don't know I call it as Lambda actually so what is this Lambada Benchmark used for it is used to test the models capability to understand longr dependencies because if you see one of the aspects of or the goals of llm pre-training is to make sure that the llm understands and holds context for a long length right so that can be ensured with the help of Lambada next you want
(1:03:52) to make your model multilingual right so multilingual benchmarks are there extreme X glue these are benchmarks uh which are used for cross lingual understanding and then you have uh emerging benchmarks new benchmarks uh like big bench which will see like you know how much your model has a wide range of knowledge because big bench has different uh tasks uh and that is on different domains okay so big bench you can just say it as uh domain related uh Benchmark because if you see multilingual related Benchmark is
(1:04:26) Extreme glue ex glue and then uh for long context you have Lambada for um natural language understanding you have glue for reading comprehension you have Squad and then for domain understanding you have a big bench and uh adapting to wide variety of Downstream tasks can be done with the help of Hell which is known as a holistic valuation uh framework for language models right it has around 40 tasks apparently now if you see what and all goals we had for pre-training right all metrics are evaluated and now when you have a task
(1:05:05) specific evaluation and a downstream level right when you have question answering you can use metrics like exact match or F1 score exact mat is nothing but accuracy score okay and for data sets you have square natural questions trivia QA Hotpot QA and so on and so forth and here you will evaluate both answers correctness and relevance for Tex summarization you have Rog which is abbreviated as recall oriented understudy for gting evaluation so so this is basically actually created for uh summarization itself this metric
(1:05:38) was created for that and this can be used for you to ass say uh how much the data is compressed and what content is selected is to uh like you know to make it compressed and the fluency in which the compressed content is expressed so for that you can use Rog and for data sets of evalu you can use CN andily mail XM MLM if you are using multilingual tasks right next if it is translation you can use blue mute and then uh for data sets you can use WMT for name recognition Precision recall F1 score these can be used uh which is to
(1:06:12) identify the models ability to identify and classify named entities for this you can use conal data set now coming on to human evaluation here if you see actually we are going through each of those method right we already seen benchmarks we already see some metrics now we are seeing human evaluation so why is human evaluation or human judgment important because humans can capture uh something which automated metrics might miss for example I might have shared a word which represents the same answer but the exact
(1:06:48) answer would have been different right so the automated metric will say no I'm wrong but technically I'm right correct so that is why human judgment is important and also assessing overall quality and coherence of model output is done better with the help of human rather than ANM or any other methods for that matter anything which even comes closer to assessing the overall quality or coherence is llm evaluation that is why these two are very versatile all right as far as evaluation Protocols are concerned you need to First have a clear
(1:07:21) evaluation criteria making sure that there is very less room for buyers or margin of error because human tends to make mistakes same goes for llms so make sure that your elevation criteria is clear and also you need to make sure that your evaluators are trained for consistency they should provide a consistent result right for one thing they're saying one uh kind of result and then for a similar thing you should have said a similar result but they're seeing out another result so that consistency Miss should not be there so make sure
(1:07:55) that your evaluators know what kind of evaluation should be done that consistency is very important now types of female evaluation what can you do first direct assessment so whatever model like you know for example our LM provides output we'll evaluate those that is one and then comparative evaluation so we'll have different models ranking the outputs of ourm for example let's say charb CLA Ai and then uh cla a means Sonet and then gini all of these will evaluate our llm and we'll evaluate those evaluation okay based on like that
(1:08:33) we'll be just ranking it and then uh error analysis where and where and all our model failed we'll just analyze and categorize those mistakes so that we can retrain accordingly here the challenges are uh the subjectivity because humans are uh subjective not objective they have their own biases and then their own uh principles philosophies so to speak and and that will always have a impact on the judgment and then it is highly cost uh like you know costly and then highly time intensive and also scaling human
(1:09:07) evaluation to last dat asset is very hard the ethical considerations you have is eval evaluating your model outputs for gender racial and other biases because these kind of bias will be there in common web website datas so you need to ensure that your model is not bu it biased on these ways and then you have uh data sets for these like you know to check if your model is biased for that you can check it against V bias kind of data set and all and then uh you need to see how much it is uh Fair across different demographic groups
(1:09:46) so you can't just say like you know I'll be just doing English even if you go for multilingual yes there will be a low resource language but still you should be demographic balanced to some extent then comes uh privacy and security implications so you need not you should not leak any training data to the user and also uh you should be vulnerable uh like you know your vulnerability should be very very less to prompt injunctions like for example Jail uh jailbreaking is a method where it will uh break the
(1:10:21) characteristics of the llm and it will be allowing the llm to speak in the way it was uh not allowed to speak so it was not uh protective in was enough like you know it was vulnerable to that prompt so you need to assess those vulnerabilities and then also it should be truthful and factually accurate so that you can uh reduce hallucination for that you can use benchmarks like truthful QA which will allow you to assess the model tendency to generate false or misleading information now why you need to do evaluation it is to iteratively improve
(1:10:56) your llm so for that first you need to analyze the evaluation results where you'll try to identify the patterns and the model errors and weaknesses and you will prioritize those areas which require Improvement based on your goals okay and based on that you'll say like you know these is the categories I need to improve and you'll say that to the training people and then they'll train accordingly and then there is also data Centric Improvement so what you will do is wherever you find that there is a
(1:11:23) weakness you'll augment those training data in those uh situations and then you'll train it for more number of times there for example in multilingual let's say one language is not doing well you will increase the distribution of the that language so that the model gives more important to that language and then also if you identify some Biers you will filter those data so that those Biers are not there after that when you come to model Centric improvements you will find unit for specific domains or tasks
(1:11:52) uh whenever you see in the future and then also you will uh experiment with uh artical architectural modifications so like I said whenever there is a new architecture advancement comes you can try to plug in play with your llm also hyper parameter optimization so it is about uh finding the optimal number of layers optimal uh embedding Dimension and so on and so forth that comes under optim hyper parameter optimization and you need to find a balance between your computational efficiency and your performance continuous evaluation like I
(1:12:27) said uh if you do all of these as a pipeline it will be an automated evaluation Pipeline and then you'll get the model performance and then you will uh improve it over time okay so that is what is uh your evaluation site now the aim of any llm it is to find to a downstream task right need to adapt it to a scenario even to check how much it has uh adaptability right what people do is they'll just fine-tune the llm for a downstream task okay so here in our case like I said we'll follow this guide and
(1:13:07) then we'll try to pre-rain our own llm okay so what you want we'll do is we'll use the same llm to fine tune llm for a classification task so fine tuning is a stage where you'll adapt a pre-trained llm for a specific task so what is the advantage of using fine tuning over a model which is strained from scratch already is that you'll have a good language understanding already uh SP stask specific data will be less required because it already knows the language and syntax and semantics which
(1:13:38) is involved in the language and since uh these are there there will be a faster convergence and and then there is a potential for better performance okay so here are goal of fine tuning is to determine the sentiment and this can be used in customer feedback anal is social media Etc so what we can do is maybe you know uh try to make a realtime emotion analysis kind of a model okay so for this what we can do is uh we can prepare a sentiment data set uh where what we can what we need to do is we need to collect a label sentiment
(1:14:14) data set and then uh we need to process those and we need to make sure that there is uh imbalance being handled create rain test blits and then we'll find unit so for funing what we need to do is usually we need to add a classification head because initially it will be uh projecting it to the end tokens which are possible next so if we are using it as just a classifier right next token I mean like what classifier it is we need to add a classification head that is usual okay but we won't do that we'll make that classification
(1:14:46) itself a generator one and there is another option which is to freeze some layers so either you can freeze or unfreeze the layers that is completely upon choice and also uh choosing appropriate learning rates uh like I said those are hyper parameter optimization so if you are having uh pre-trained layers you can keep a lower learning rate because it is already trained it will take some time to adapt but if you are having some new layers attached have a higher learning rate for those and then selecting your batch size and number of EO uh that is
(1:15:18) also there keep your number of eox very less okay and when you are training the sentiment classifier your loss function should be across LW uh like I said we are making it generative so we'll use cross and LW but what if you want to just say like you know the class alone so if it is just binary you can use uh BC loss binary cross and loss and if there is a lot of class imbalance you can try to use focal loss to handle that class imbalance I mean as far as the loss function is concerned but you know uh for handling class imbalance like I
(1:15:49) said one is focal loss another one is to adjust the class weights itself so there is a method where you can provide weight for lesser uh number of values being in the class for that uh there is a method you can do that as well so here also we'll monitoring monitor the training process so here we'll not monitor the perplexity kind of metric here we'll monitor the training law validation loss and then we'll see the classification accuracy and relevant metrics like F1 score Precision recall and so on and so
(1:16:19) forth um here we have early stopping medic models early stopping at all where you know since we are retraining the model to avoid war footing we'll have early stopping at all so some of the advanced uh functioning strategies are like I said like you know will train the whole llm okay but if you are freezing right you can gradually unfreeze few of the layers uh in future so that is something which you can do and then also you can do mix pression training knowledge distillation is something uh a very good method we'll see that later okay we have
(1:16:52) a separate day for that um so these are some of the methods I'm not going to focus on a lot on fine tuning because fine tuning we have a separate section for that as well but we are considering that also in the pre-training because that is how you'll be able to uh verify that your model has the capability to adapt to a downstream task okay that is the aim of this fine tuning so the metrics we have are accuracy precision confusion metrics and you can do cross validation as well if you want to do that now let's say we are uh ready with
(1:17:26) the model what are the considerations I have for deployment of this model first I need to see if the model size is good okay if it is not good enough I need to prune to reduce less important weights I I can quantize to reduce the Precision of the weights this all will reduce the size of the model or what I can do is I can already create a smaller model and then train uh by distillation method so that it becomes Deployable in real time inference uh plays a lot of important role uh um so optimizing it for latency uh is very
(1:17:59) important so like you know latency should be very less and thrp put should be very high uh making that to be ensured is very important and also making sure that uh Hardware acceleration is leveraged for that you need to use Library like tensor RT onx R time there is llama CBP and so on and so forth right when you are deploying right in real time there there might be uh existing system so you need to make sure that your API which you have in uh like for your model serving that should be compatible with those systems and also
(1:18:31) uh you should employ uh logging and monitoring to see your model working at the real time and also monitor like you know whenever it fails you can update it uh continuously so you can Implement strategies for that as well now coming on to the challenges challenges and what can be done in the future the challenges in the llm development are that uh you have a computational resource uh and energy consumption as an issue like I said it is a large scale training so computational resource is an issue bias in the training dat there will be there
(1:19:04) how you can handle that is another important issue usually llms are blackbox so making it very explainable and interpretable understanding how the model decides that this is my next token that is a challenge and Hallucination is another challenge so what are the trends and future direction in which the world is moving right now one is continuous learning right so we have Laura where it will try to address the challenge of catastrophic forgetting so that you can continuously make your model learn for the future and then more uh adaptable to
(1:19:40) few shot and zero shot so so that you know like with less samples you can adapt it for new tasks and then uh making the model more ethically responsible finally the conclusion as far as the recap is concerned we have seen from data collection to fine tuning a sentiment classifier everything is covered and the key T if you would have seen we have given more importance to high quality and divers data making sure that you have a careful pre-processing and tokenization because those serve as your foundation data for your model and
(1:20:12) then uh we see the complexity of the architecture uh being balanced with your computation environment and then you are creating a training procedure which is um let's say scalable in nature and then uh we have seen the significance of evaluation and like you know improve of improvement over time like iterative Improvement that significance is there so what are the future of llm uh like know it is going to have a potential impact on every industry which is there and there are lots of ongoing research and challenge challenges and opportunities
(1:20:47) are there we can see uh what we can grab out of those but now if you see you have literally overloaded information of the whole pre-training pipeline of you know like anything and everything which is related to pre-training like you know in the 62 slides you have all the information which is related to training uh process so I hope you all would have liked this video If you like this video please hit the like button share with your friends if you haven't subscribed to the Channel please hit the Subscribe
(1:21:17) button hit the Bell icon I'll see you all in the next video Until Then happy learning
