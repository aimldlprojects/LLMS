(50) Mastering NLP Fundamentals: A 4-hour Hands-on Tutorial - YouTube
https://www.youtube.com/watch?v=aZePVtBHYhM

Transcript:
(00:00) welcome to the day Zero of the llm Mastery course in this video we are going to see about the prerequisites which you should know before going into the course in deep the prerequisites itself are a lot to cover so I made it as three parts beginner intermediate and advanced in this video we are going to see about the beginner prerequisites let's start with the beginner prerequisites what is natural language processing natural language processing is a sub field to artificial intelligence and computer sence which focuses mainly
(00:40) between the interaction of computers and human who use their natural languages but what is the goal of NLP it is to enable missions to understand interpret and generate meaningful content to the humans in essence it is just a sub field you know which will be covering computer science artificial intelligence and human language because it is trying to bridge the gap between human communication and computer understanding which is by allowing missions to read hear and make sense of what know like human speaking by training it on vast amounts of human
(01:26) language data now what are the key components of NLP first syntax as we all speak syntax in programming languages even natural language has its own syntax syntax is often defined or referred to as the rules that governs the structure of the sentence for example the verb should come second noun should come first something like that in NLP syntactical analysis or parsing as they say it is used to whether a sentence conforms to the grammatical rules there are grammatical rules to be followed when you are making
(02:07) a sentence and syntactical Analysis helps you to assess if that is correct or wrong semantics as the name suggest it involves understanding the meaning of words and sentences semantic analysis allows the mission to grasp the content and disambiguate words with multiple meanings what is mean by that let's take an example of bank so for example if I'm saying I went to a bank for getting my loan which is located at the end of a river bank here I'm using bank at two places one is referring to a financial
(02:49) institution and another is sitting like you know referring to uh end of a river so here the computer or a is expected to understand the meaning of the word and sentence by which it will understand the context and say okay the first bank since I'm saying about loan and like you know getting something out of it it is going to refer about a financial institution and since we are saying it is located at a river bank it will say like you know it is the end of a river or something like that okay so semantics is very
(03:28) important third one is morphology so what is morphology morphology is the study of structure of the words know such as root prefixes and suffixes for example Advantage would become disadvantage and play will be like playful something like that which will be prefix and suffix and here the root word will be play play is becoming playing playful playfulness whatever you want adjectives or verbs but the root word is play right morphological analysis breaks down the word to their constituent part to derive meaning you might say about playfulness
(04:12) or whatever it is but it will first understand that okay it is something related to play pragmatics pragmatics deals with how language is used in practice and how it influences meaning it is mostly about missions understanding the indirect language so for example we say like you know uh I am on Cloud9 that I'm launching this course so it you understand that okay I'm very happy that I'm launching this course right so that comes under pragmatics so mostly pragmatics is about the machine understanding the indirect language
(04:49) especially with edms and phrases next phology and speech for a spoken language processing because in NLP it is not going to be just about text it is also going to be about audio or sound so the sound of the speech will also relate you know how the language is spoken for example if I'm shouting a lot it means I'm trying to convey uh Angry emotion out of it right so those are very important when you are processing the audio because there are use cases like audio emotion analysis now that is about the key components of NLP next let's see
(05:30) about the common tasks in NLP so you can say it as tasks or applications of natural language processing and here you can see there are a lot of things smart assistant document analysis predictive text chat boards and so on and so forth language translation automatic summarization but these are you know an end application but at a task level how we Define that text classification there is a text data and we'll be assigning up predefined category to that text data for example if the use case is Spam detection ham or spam if the email is
(06:08) something you know good or is it is like you know spam something like that is Spam detection and with sentiment analysis it depends people try to do it either for positive negative or sometimes people also add it as positive negative neutral but the idea is that it is to express the sentiment or identify the sentiment which is expressed in the sentence all right so that is about text classification and here if you see we are having a set of you know uh categories and we are trying to assign those categories so the
(06:46) perfect definition of text classification would be assigning predefined categories to Text data because even if you see the product reviews right I could have even classified it as type of product right so the TV is not working well I would have classified it as Electronics if my use case was to classify the category or product category right so we are assigning predefined categories and that would be the explanation about text classification so what is named entity recognition as the name suggest it is to recognize the named entities so what are
(07:25) named entities people organization dates location s usually your nouns will be named entities right so recognizing those will be named entity recognition if I say I am making a course on generative AI so I will be a named entity because know it is saying about some person and you might now wonder okay wasn't you said n no here I'm saying about proun also okay let's start with machine translation next a machine translating text from one language to another is machine translation also you can put it as one form to another and
(08:08) why is that I'll explain now popular applications like Google translate have multiple uh language support which is known as multilingual support in their AI systems and you also able to see like you know Google Translate is there that is language translation but when we put it as m in Translation there is some another thing we need to see if you want to convert a text to code it is also one language to another okay so that is why it is said as one language to another but you should understand that code is
(08:40) also a language so if I'm saying text code it is about machine translation itself where the machine is expected to translate a text into a code hope you got that now let's move on to part of speech tagging what is part of speech tagging labeling words in sentences noun verbs pronouns adjectives and so on and so forth so that it will understand the grammatical role of each of those words if I'm saying I'm recording prerequisites for my course okay it will say I it is noun or pronoun here m is
(09:20) verb recording will be another verb where am will be an auxiliary verb and recording will be a primary verb prerequisites uh again it is going to be another noun right so it will understand know what is the grammatical role of each of these words in a sentence so we usually classify it as T text classification but often sentiment analysis by itself is considered as a task because the amount at which every trunk companies try to use sentiment analysis is pretty huge so that also is provided as a separate task
(10:01) here where will try to identify the sentiment all right and then we have question answering as a name suggest Building Systems where once I provide a query the machine is expected to provide a correct answer based on a body of knowledge either it Knows by itself or you know we provide it or something like that okay so this is the foundation of systems like IBM Watson or chb whatever you see it is all about or it is down to question answering speech recognition and generation so it is formerly known as or formally known as speech to text
(10:42) and text to speech okay when you convert a speech to text it is going to be your speech recognition and when you con convert a text to speech it is going to be speech generation next we have text summarization one of a very complex use cases which you can do with AI because it is about automatically generating a shorter version of a long context while you retain the essential meaning underline the this phrase while retaining the essential meaning because when you're going to summarize it is about you understanding the whole
(11:17) context and then rewriting it in your own words so that is why summarization is a pretty complex task in its own way cor reference resolution identifying when two or more expressions in a text refer to a same entity so what is this all about so let's take this itself let's consider I am John okay John is recording prerequisites for a course and he is now moving on to the next topic techniques of NLP here he refers to John right so they are referring to the same entity language modeling language modeling is a task even which is used in
(11:59) Pre R training of LMS which we'll see in detail later where the idea is that the machine is expected to predict the next word which is essential for tasks like autocomplete text generation and so on and so forth and when you say about text generation it will cover everything any kind of text which is generated is text generation so in a in a way or in machine's own way we can say even text summarization is text generation now let's see about some of the techniques in NLP the NLP has evolved significantly
(12:37) or drastically so to say driven by the advancements in machine learning and deep learning and here are some of the most prominent techniques or the models used in NLP nowadays first is Rule based methods you might wonder there are llms right now there is a lot of big models even if not for llms who uses rule based methods you see when you use Transformers or llms or even neural networks there is a sense of you know unpredictability or uncertainity that your model might go wrong right but with r based methods since it is
(13:20) handcrafted right please note the word handcrafted you have crafted the rules to process The Language by yourself so it can never go wrong yes the issue is that there is limited scalability and adaptability unless and until you rule you write a rule for every single change in your use case you won't be go able to go for rule based methods but that is something you should always have in your mind that if I can solve it with rule based methods I'll go for rule based methods right so that is why even in text
(14:02) procing we'll write rules which is most probably reg regx rules to process the data rather than giving it to llm right the reason is not that it is not just computationally intensive it is also on the fact that you can't trust an llm to generate an output and making that as an output to train your own model with that as an input right so that is your rule based methods now the next one is machine learning machine learning is the base for whatever we see nowadays and even that has been used for a lot of use
(14:45) cases from some time before and that is known as modern NLP that is where Modern NLP started so for that it will use supervised and unsupervised learning techniques and algorithms like NB support Vector machines decision trees and so on and so forth has been used for NLP tasks like classification and clustering now you might wonder what machine learning models for NLP doesn't seem you know uh impressive so to say I'll show you at the end of this prerequisite section where we'll be training a neas model for text
(15:25) classification or sentiment analysis to be precise and we'll see how much accuracy it gets right now coming on to deep learning neural networks especially RNs and lstms which is a variant of RNN along with that there is CNN as well is used wiely in NLP to handle tasks that involve sequential data which is like text like I said especially lstm lstm has a capability to capture long range dependencies in text and lstm is abbreviated as long short-term memory right so we'll be seeing about lstm RNN and later but you should know that in D
(16:10) planning there are neural network like RNN lstm and all which is used for use cases with text with RNN and lstm it is not just that you can do with use cases like SYM analysis anything which will involve sequence can be used with long shortterm memory networks so even it can be your stock price prediction you know based on the history of data I want to predict the next day data or next day close price that can be done with the help of deep learning coming on to the most revolutionary architecture after CNN in my opinion which is Transformers
(16:53) the introduction of Transformers basically Revol revolutionized NLP and unlike RNN Transformers can process words in parallel and that will help it to capture better context there are models like bird GPT and tii which are based on this architecture as its core and have achieved state of the art results in many LLP tasks then right but why did I stress on the word then it is because of this thing which has come recently which is pre-trained language models you see language models are also Transformers but the way it has been pre-trained the
(17:34) Large Scale pre-training Models have basically changed the NLP landscape these model are trained on vast amount of data and then are fine- tuned with specific tasks there is bird gpt3 mbot which are all like you know uh let's say the parent models of all the models we are having right now and these models are the ones which have made NLP more accessible for applications like chat BS translation and even creative writing creative writing is a very complex process and Transformers has paved way for that now we are seeing about the
(18:12) introductions to NLP but it is very important that we understand the challenges which are there in NLP and that is what we are going to see now though there are numerous advancements which are there in the field of NLP right now still there are some challenges first is ambiguity language is inherently ambiguous whenever a language is there it is always going to be ambiguous or you can say we say like you know double meaning or triple meaning words right for example the word I said as bank so to understand that it is very important
(18:49) the model understand the context tone cultural factors and so on and so forth yes nowadays model are better on that but still it is a very big challenge next one would be context understanding while Transformers are improved context capture still maintaining long-term context in lengthy conversation or documents is still a challenge right now if you see 496 is something very common as a context length but after that the model can't handle right so that is still a pro problem okay and then MultiLing uality developing nlms to or
(19:32) NLP systems basically that works seamlessly across multiple language is very complex because the grammar is different syntax is different cultural reference is different and more importantly the data which is available for some low resarch language are very less so that you can't ever match you know uh we can say it is highly unlikely because we are all data science and we can speak only in probabilities never we can say that it is impossible so it is highly unlikely that you can train a model which will work close to
(20:12) same for any languages in the world so Universal models are very difficult to create byas present in language models since NLP models are trained on large data sets which are scraped from in in Internet they inherit and learn whatever bias is present in that data as well yes there are efforts made made to make sure that these models are Equitable but still there will be bias right for me the most important Challenge and still people are trying to you know solve this at least these four you know we have made a lot of advancements in it
(20:54) previously we had only 500 now we are moving towards 4096 Conta text length at East ambiguity is something now llms are able to understand multilinguality yes like I said it is highly unlikely that you can create a model which will work for every language as similar as it works for English but still you know it is not like how it was before yes people are making a lot of processing steps and trying to avert that issue but the most important problem that is why I have kept it as the fifth issue which is common sense reasoning reasoning is
(21:32) something what differentiates human from any other species and it will also include AI if I consider AI as a species the only thing which humans have over AI is reasoning because AI is more efficient than human but it will always struggle a lot with common sense reasoning and we can update our knowledge and un less and until an AI can update itself it is not going to you know uh get the Real World Knowledge at any time it wants that human can do right and also reasoning like I said reasoning is something uh which is very challenging
(22:19) with NLP field right now so we have seen about a lot of things now let's see about the applications of of NLP how it is used in various Industries first things would be search engines NLP is the core of search algorithms if you all remember in the techniques of NLP section I have said about rule based methods right search engines previously were rule based right and still it is mostly rule based okay even if you go for Google search the search results are mathematical rules right there is an AI assistant which is there that is fine
(23:04) but the search engine results which Google provides it is only because of rules okay so NLP is at there at the core of search algorithms helping understand and rank relevant user queries results virtual assistants like Amazon Alexa Google's assistant Apple's sir all use NLP to understand spoken command and respond appropriately customer service nowadays chat Bots are used in customer service and those virtual assistants use NLP to understand the inquiries and provide Solutions automatically if they can provide
(23:45) they'll provide themselves or else connected to a technical person so that they can help solve the problem and there is Healthcare where NP helps in medical data processing summarizing patent patient records and so on and so forth in social media there is sentiment analysis Trend detection so that you can capture the trend and probably make it big so these can be done with the help of NLP to inter interpret user interactions and one of the most important applications of NLP domain is legal and finance to process legal
(24:20) documents Financial reports or automate contract analysis NLP has been used very widely in legal and finance so as a summary you can say that natural language Crossing is a dynamic and rapidly evolving field that gives ability for machines to understand interpret and generate human language from rule based systems it has moved to Transformers and no we have come a long way right now with llms but however still like I said there are challenges and it has been used in various field due to its advancements next we are
(24:57) going to see about the NLP pipeline the NLP pipeline consists of several stages which will convert a raw text into a structured data which is suitable for model input and then we'll train the model and then so on and so forth right so it will start with text input or data collection and then we'll process those data will represent those data and what is that it is the stage where you create embeddings and then you'll select some features if you are doing that it is not something which you commonly do right
(25:35) now and then you have your model selection and training where you'll select what model you want like I said there are numerous techniques in NLP it is upon you like what you choose for your model and then you have model deployment and then we'll evaluate at real time and then improve it so this is how an nalp pipeline works but let's see about each of those component at an overview with answering the questions what when how why in this course I would have emphasized more on what when how why because that gives you
(26:15) Crux about any component and it will allow you to apply your knowledge better so let's start with what on data collection what is data collection it is a process of gathering raw text from various data sources so when you'll do that at the first part where you'll collect the data for training according to your use case obviously and then how will you do that there is web scraping apis there are open source databases or close Source databases and generally we can say it as databases or user generated content or a generated content
(26:50) nowadays right so these are the ways you can collect data for training your model and why you should do that it is very necessary because that is going to be your model input for its training data processing it will involve techniques like cleaning tokenization Vector representation and so on and so forth all will come under data processing so what is data processing data processing is a stage where you prepare the raw Text data for analysis and when you will do that it is before you build the model where you
(27:26) have ensured that you know the data is good for the model to be trained on how will you do that it is through the various text pre-processing methods which I will discuss in the next section and then why you need to do that it is to ensure that you have consistency in your text and the data quality is improved because you're collecting it from internet and so on and so forth there will be bias for you to remove there will be a lot of mistakes there will be missing data you need to handle all of those there it will be very
(27:57) unstructured to handle all of this you need to do data processing next we have model building section and what you will do in model building we'll be creating and training NLP models on the processed data so once you process the data you'll create and train your NLP models so this creation of models and training those models is the model building phase all about when you'll do that like I said once your data is created the model also be created along with the training Paradigm because the training pipeline
(28:34) to be written is very important because just model is not going to train by itself right you need to create your own training Paradigm that is a very important step which will be added in model building by default so why you need to do that obviously you need to develop models which is to which has the capability to understand and generate languages and how will you do that you'll use the techniques of NLP which we discussed before we it can be rule based it can be machine learning algorithms deep learning architectures
(29:07) pre language models or so on and so forth inference or testing so what you will do here you have trained your model now you need to make predictions or generate output whatever it is so that is what is inference all about when you will do that as like in the definition itself once you train the model this model is used to get new new results on new data or you can put it as mostly unseen data because we can't like say like 100% sure that this data was not there in pre-training or point uning if you are creating a small model by
(29:44) yourself you can say with some sity but with llms you are using internet data we can't say like you know this data is not something which has seen so I've just put it as new data how will you do that you'll use like new data will be fed into the model model and the train model will interpret and provide the output accordingly and why you need to do that then only you'll complete your use case that is like the basic thing and also while evaluation basically you'll need to infer your model so that is why and
(30:16) deployment deployment is a stage um which is about making the NB model available for real world use cases and it is done after rigorous testing and inferencing on some real world users inferencing with some real world users or we say it as beta testing basically right after that only we'll be doing deployment and how we'll do that there is apis where you can say like you know this is my model hosted and there is cloud services which is almost similar as API and then there is on premises infrastructure
(30:53) where there are some companies who want to keep their model to themselves and then like they'll try to use on promise infrastructure why you need to do that basically to integrate the NLP capabilities to your products and services now you have an idea about all of these specific um components at an overv level we'll go in deep at each of these component at some point of time in the course all right but it is very important that you address this specific component which is data processing because this is not something which we
(31:31) spend a lot of time on but it is a very most important component so let's go on with data pre-processing next data pre-processing or text processing basically here the data is text so text bring can be said as data procing so this is a crucial step which is happening after data collection and before model building it is essential for cleaning and preparing raot Text data so that the tech quality will be improved and will be consistent for model to be trained on so what are the steps usually like you know in old times
(32:06) this is how it will be happening and why I'm saying at all times with llms everything has eased up a bit because llms have tremendous capability to understand even you know some amount of unstructured data or unclean data so to say but even if you give unclean data or clean data both right clean data will give the better performance so it is always good to clean up your data a bit but the extent at which that happens previously with normal neural networks that won't be required nowadays so this is how the
(32:45) text Crossing pipeline basically goes you'll remove the text you'll do stemming or lization you'll tag some POS and based on that you will clean your data and then tokenization will happen this is how the pipeline Works basically but but we are not going to go like that okay we are going to see how basically in real world T processing happens okay we'll go for the methods and we'll answer the question as usual what when why how text normalization so as you can see here it was he visited in London he
(33:16) visited London he visited London all all our caps here caps and small this is probably the most worst data so to speak and once you normalize your text it is expected to have this kind of an out okay converting the text to a standard format is text normalization example is lower case you can have anything for example making it as everything as capital could be your standard format and that is again text normalization right so when you'll do that at the beginning of text procing then why you'll do that it is to
(33:51) basically reduce the variability in the text because see if you have everything as small letters or everything as capital letter there is some kind of a consistency right where it is not something which we see here or here or here or anywhere like you know if I see all of these in my data pipeline let's say all of these four sentences are in my data set for each I would require to write different rules right so that would not be the case once you normalize your text so that is why it is done at the beginning
(34:22) of text processing and how we do that basically you can use string Methods or custom functions so here you can see this is a sample text and if I call text it is going to be this is a sample text but if I call text. Upper it is going to be an upper case Okay so it is upon your choice if you want to make it uppercase it is uppercase if it is want to be a lower case you can keep it as lower case the usual convention is to keep it as lower case that's all P one of the most underrated aspects which is the in NLP
(34:58) but has a tremendous potential so this is a reg expression or by regex by itself because regex is abbreviated as regular Expressions so here capital A to Z says that it is character from A to Z as caps and it is it against smalls so Square back considering this character range okay here we are seeing two with the bracket right so it means that exactly two occurrences of any character from the preceding pattern which means there can be two characters okay it can be Capital small capital capital small small small Capital but there should be
(35:36) only two characters and Then followed by that there should be digits which is from 0 to 9 that is what/ D says and there should be only three occurrences of those basically these are the format of Harvard courses if you have seen it is cs229 cs231 so on and so forth okay so for that you will write a reex like this if you want to process this specific sentence okay so what is that it is a pattern-based text cleaning and extraction procedure and when you'll do that it is in early stages of text Crossing pipel but it is mostly uh done
(36:15) after or with normalization at sometimes why you will do that basically to remove or modify spe specific patterns in text if you don't want some specific patterns in text if you want to remove those then you'll do that for example URLs or special characters or so on and so forth how will you do that you'll use AR module in Python which is default you know to perform regx I'll go in detail about Rex but here is a sample okay right now my thing is that I don't want the link there and if I call this it
(36:51) will just remove that and how will this happen we'll see in detail with in the reg section okay next is probably one of the most important aspects in text browsing which is tokenization tokenization is a process of breaking a text into smaller units which is known as token which can be usually subwords words or even characters when you will do that once you're very sure that you know the text is good or you know normalized why you'll do that it is to prepare the text for further analysis or even to provide the data to the model
(37:27) how you can do that there are lots of M there is nltk there is spy custom functions can be written or even you can use Hing face tokenizers okay so we'll be again seeing about tokenization as a detail section later from the course but just to say here you can see word token what it will do is it will tokenize each words separately so it will be NLP is fascinating all will be separate okay so you can see NLP is fascinating has come as separate right now here you have stop wordss removal this is not something
(38:02) which is done nowadays all right because when you remove stop wordss um it is basically collapsing the structure though it doesn't carry a significant meaning but it was done before okay the process is like I said eliminating common words which doesn't carry any meaning so usually after tokenization if there is a word which doesn't have any meaning those will be removed basically your stop words okay or your auxilary verbs so to speak and like in know your adjectives can be used but this uh and the these kind of words won't be used
(38:40) okay this is to reduce the text and for uh noise in the text and focus more on the meaningful words and how it is done what we'll do is we'll have a set of stop words by ourself and then we'll make sure that these words are not there okay here are some of the common stop words uh I the in F off for at to on with from these all like you know uh carry very less meaning at least from and two have some meaning but you know not everything has some meaning to it so basically these are stop words so in
(39:17) analytic itself there is stop words and if I do this is uh sentence sample sentence with stop words and when I tokenize it it will be this is a sample sentence everything will be words and here we are seeing if those are not in stop wordss so this is uh with all of these are in stop words so those are eliminated and others are printed here so next we have stemming and lemmatization so stemming and lemmatization both are about you know reducing the words to another form where stemming focuses on root form but l
(39:58) focuses on LMA which is the dictionary form when this will happen after stop wordss removal why again to reduce uh vocabulary size and simplify the text so because each token or each word will will be a separate entity in your vocabulary and with increase in vocabulary um the model to normalize itself across the vocabulary it will have a lot of requirement for training and also for while inference the time will also take a lot okay how will you do that you can use rule based algorithms like pH stemmer in stemming
(40:33) and for lization you can use part of switch information and all which is usually uh done with the help of wordnet ler okay so first I'll show you here it is run run ran okay and here it is good uh run and e or better running and eat okay so it will not change a lot of things okay it will capture those information whenever it is required but if I did like for example betterness right it will still give betterness okay so that is why people usually don't prefer LZ because betterness is something would have been
(41:16) there in dictionary okay if there is a word in dictionary it won't do that okay that is about lemmatization so if you want to do it in your root word uh it is better to go with stemming but if you want to make sure that the word captures some meaning because that is not something which is usually the case with stemming you can go for lemmatization now like I said we are going to see about re text procing in detail because that is something which is very important and it plays a crucial role in text processing to standardize and clean
(41:51) the text so what is Rex so it is a sequence of characters which used used to define a search pattern and it is used to do pattern matching and text manipul manipulation basically when you'll do that it will be done during the initial text cleaning phases often right after uh or combined with normalization through the text processing pipeline uh for specific cleaning task you can use um regx because even in post processing you might require okay why you will do that it is to again efficiently remove or modify specific patterns in the text it
(42:31) will standardize more you can extract information and so on and so forth how will you do that using the re module but we are going to see in detail those right now okay common methods are re. sub which will substitute the pattern with a replacement text find all which will find the occurrences all the occurrences based on the pattern we gave and already do search what it will do is it will find the first occurrence okay so here are some of the common reex operations first is removing special characters so if I say hello exclamatory
(43:05) Mark how are you question mark I quotes I'm doing great # happy so it will remove all of those special characters and why is that here we are making sub okay r. sub which is substitute and here the bracket is open right so here A to Z A to Z flashes okay so basically if it is not a character I'll remove this that is what this thing is saying about like you know anything which is not a character that will match this pattern and I'm going to replace those with empty string okay so that is why hello
(43:45) how are you doing I'm uh good doing great happy is coming like this removing extra white space again slash S is space okay anything which is not a character or space will be replaced by text now you might wonder um will that happen for numbers also yes it will happen if I had a number in here it will be removed as well so that will be the case for removing extra white spaces as well if I have you know numerous white spaces I'll replace it with one space okay here you can see there is three to four spaces
(44:21) between each and those are provided as this has Extra Spaces this is how you'll re remove trailing white spaces so to say as a technical name if you want to extract the email right so email passing is something which is done a lot of times and this is how you can do that okay read.
(44:44) find all find allenes of email where the structure Will Be A to Z A to Z numbers or it can also include underscore and so on and so forth okay and here are some patterns as well um I can it is a detail pattern we can go in detail at some point of time but this is how you will extract the emails so how will you remove HTML taxs we have we all know that there is a characteristic for HTML tag there will be an open uh bracket there will be a Clos bracket and then there will be some values inside that okay so we are seeing anything which matches this pattern will be
(45:20) replaced by empty text so that is why it has become this is bold text standardizing phone numbers so here if it is this digits or this kind of digits what we'll do is we'll make make sure like you know everything is just empty string anything other than digits will be empty string okay but like I said regx can be used anywhere you can use it while tokenization you can clean it and then use for stop wordss removal okay everywhere regx can be used and that is why regx is a very important component in text processing next we'll see about
(45:58) an another section which is Vector representation text representation or commonly known as embedding we all know that computers knows only numbers but text is not number right you all know that it is zeros and ones but even converting number to zero and one are easier right so embeddings are what is the in between phas between text and those zeros and ones okay embeddings are formally defined as tense Vector representations of words or phrases that capture semantic meaning okay and significance of embeddings is that uh it
(46:40) will capture semantic relationship between words it will reduce the dimensionality of the text Data it will improve the performance of many NLP tasks okay so that is about embedding so to say it is just a way that you will represent your data which is text which can be words or phrases and this embeddings will capture the semantic relationship between those words basically those will improve your performance now let's see in detail about the embedding methods first let's see about bag of words what is bag of
(47:17) words it is a process of representing a text as a vector based on word counts when it will be used it will be used only with simple task like text classification why it will be used it will be used to because it will be very easy to implement and understand how it can be done basically it can be done by creating a vocabulary and Counting the word occurrences in each document here let's take this example this the child makes the dog happy the has appeared twice dog has appeared one times makes has appeared one times child has
(47:55) appeared one time happy has acquired occurred one time the reverse here also the will have two okay so if you make numbers of those and then you make a like you know number out of this that will be a bag of bir representation okay here you can see this is probably the worst method all right it doesn't capture any meaning see if I say like 1 one one two what you'll be even able to understand out of that there is no difference right so that is why you know bag of words is not something which is you used commonly and then there was an
(48:29) improvement and came tfidf term frequency inverse document frequency here also it occurs based on words okay but it it is all about representing the importance of words in a document relative to a collection so what is this all about if we see about how you'll understand what we'll calculate a term frequency where what it means that is frequency of a word in a document and inverse document frequency is about the number of documents which contain those words and we'll make a logarithm out of it and that will be your inverse
(49:07) document frequency because the idea is that if a term is require is repeated a lot it means you know uh that term is very important but if you see those filler words auxiliary vers or stop words so to say will have the most number of occurrences right so to filter those those out only will have the inverse logi uh doent frequency component okay so once you do that you will have a vector transformation and here you can see there will be some meaning okay here you can see this is having something but the is not having
(49:45) anything okay so yeah so TF IDF is something which can be used for use cases like text classification and all and it is a pretty good uh method for starting okay again here also it is Rule based and it is not always you can do anything with rules and then came an approach which is word to W okay so what is this approach all about you'll create dense Vector representation of words with the help of neural networks why you need to do that you need to provide embeddings that capture context and semantic meaning when you'll do that when you
(50:24) require you know like you capture semantic relationships between words for simple use cases don't those won't be required but for complex use cases yes you need to capture the semantic relationship between the words how will you do that you'll train a shallow Network um on a large text corpora based on the words by using methods like cow okay so for example king and queen will be related and King will be in the same zone of man and queen will be the same zone of woman so to capture this you need to you know make an embedding model
(51:00) okay so if I do this one here you can see go Tock the sentences are cat say meow dog say woof and here I'm calling a word to wick on these specific sentences okay so it will create a vector for all of those instead of that if I say meow still it will create okay so here you can note the number 8 e power minus 3 will keep that and if I keep C it is - 0.
(51:40) 7 okay but if I said dog you can see it is e minus 5 okay the relationship between the words cat say me Doc W so here you can see those are tokens this inherently shows that once you make your tokens to represent the relationship between those words or tokens you will use verto it is not always that you'll have a ver model like right now the performance even I didn't like that so what you will do then you'll create your own custom embedding with the help of neural networks like Ann okay when you'll use that when the pre-train embeddings doesn't capture the specific
(52:15) nuances of your domain it is not always that you know even like for example Financial domain it won't capture the nuances so you will create your own embeddings why you need to do that you need to create your embedding for your specific task because that is when it is tailor made which means it will improve your performance and how you do that basically you will train a neural network with an embedding layer on your specific data set and this is how it will happen so here I'm having a wab size and embedding Dimension wab size is
(52:46) basically a vocabulary the number of tokens which it knows that it is there an emending Dimension which is you know um the vector representation Dimension so for a word I'm representing it with 512 numbers 1,24 numbers so that is about your custom embeding uh Dimension okay basically this Zing is a dictionary but it is a learnable dictionary that's all okay so if I do this right it will create a custom embedding then uh what it will do is it will give for five tokens it will give 50 tensar of each okay each tensar will have or like each
(53:29) number will have a 50 number representation for it okay so it will be 5 cross 50 I haven't trained this but you can also train that that is also possible okay and that will that is how you'll create the embeddings okay now you know how to process your text like data collection is very simple right you'll go for internet sources and so on and so forth you'll collect those you'll process those you'll represent those with embeddings right so now now let's go on to the NLP pipeline which we saw
(53:59) before we have seen about the data collection small right I said like you know you can collect from somewhere like those and all text prepressing is done text representation is done feature selection we we won't do that in NLP basically a lot now it is about model selection and training so in methods already we said say saw about rule based methods with Rex so we'll be starting with NLP with machine learning that is all about our next section now let's see about how you can use machine learning for NLP so this section is all
(54:35) about integrating NLP techniques with machine learning models so for all those who don't know about machine learning machine learning is just a subset of AI which is focused on creating systems that can learn from experience and make decision based on those data okay and why is that required previously it was rule based the hierarchy ofch Ru uh techniques if you remember it is Rule based methods then it was machine learning okay those rule based methods can't adapt themselves and improve themselves but these model can
(55:07) automatically improve based on their performance through experience okay so these have their own set of rules but still it can improve upon their experience the types of ml models uh there is supervised learning models which will learn based on label data to predict new outcomes so it will include use cases like classification and regression which is Spam detection price prediction and so on and so forth unup learning is a specific type of model where you'll try to find the patterns with no labels so it will be customer
(55:43) segmentation dimensionality reduction now customer segmentation is about clustering clustering uh things together which has some similar characteristics semi supervis learning is as the name suggest a combination of supervis and un supervis where you'll do that when you have small amount of labeled data and large amount of unlabelled data okay and reinforcement learning is something when you'll do when you have an environment where you can say like you know if you do this your you have you know a maximized reward reinforcement learning
(56:17) is a kind of semi supervis learning by itself where first you will train with labels okay and then what will happen is the model will reward itself so those reward are considered to be labels indirectly okay so here are the types you can see like I said reinforc learning is a type of semi supervised learning by itself okay but we are not going to go in detail with all of these types or models we are going to focus on Naas okay why n okay I think that is very important see ners is something which is very efficient effcient okay
(56:58) because it is very fast and requires very less training data it can handle High dimensions and you know like though it is said as Nave it is very good for text classification actually okay so that is important reason why we use na B for text classification so how will you prepare the data so let's consider the sentiment analysis so this is just demo but we'll see a training separately so let's say we have four reviews here great product bad quality excellent service table experience positive negative postive negative okay this is
(57:34) our data set vocabulary building so it is about splitting those words and that will be a vocabulary great product bad quality excellent service table experience okay that will be a vocabulary building this is how basically nabas training happens okay so what it will do is it will calculate the prior probabilities okay the prob of being positive out of these four it is like equal okay positive is also 05 negative is also 0.
(58:05) 5 that is prior probabilities but what is the likelihood probability like let's say this word is there what is the likelihood of that word being showing that know like that is positive that is about the likelihood probabilities okay this works based on Bas theorem which is about conditional probabilities okay the prob of a given B is equal to probability of B given a star probability of a by probability of B okay so given a given B is about occurrence of a and occurrence of B so this is all about saying like for example probability of great being good
(58:45) is equal to the probability of good like you know uh the sentence being stated as good given great by the probability into the probability of great occurring by the probability of good okay that is how the formula is with Bas theorem when you apply those these are the values you get for the positive classes and these are the values you get for negative classes right and when you do for text classification let's say you do it for good product and bad product okay good product for positive and good product
(59:20) for negative itself okay we'll see about good occurring in positive and product occurring in positive and good occurring negative and product occurring in negative the probabilities of those okay and if you do the probabilities will be 0.693 for positive and 0.34 for negative so it is set as positive this is how basically it works but should we do this all by ourself in mathematical no it is not required why there is a library known as SK learn which does all of this and we are going going to use that so before going into
(59:59) training uh we are going to see about the limitations where the main limitation is that it is under an assumption that there is a feature Independence which is there because this shows that no like this feature is dependent but those good is a feature which is independent okay but usually that is not the case with Text data and that is why you know there is a limitation with neighbors but still it works very well in real world Okay so that is about the introduction for the neers cortex classification where we are
(1:00:33) seeing about an NP with Mo learning now now let's move on with training a na bias classifier with the help of SK scalar here is a code for training a sentiment analysis model with the help of NAB okay this is how usually it works in real world okay you need to use class because it is very uh formatted and it will be very easy to manage okay and you can add your doc strings for explanation so please try to follow this kind of a procedure this format is known as web 08 okay so here we have the Imports numi
(1:01:10) then tfds which is T oflo data sets from where we'll basically load the data set for our training and then train test split to split the data set for training and testing and then we have vectorization uh which is TF IDF vectorization okay like I said for simple text classification to create those vectors which is your embeddings TF IDF is more than enough okay and then for model we are going to use multinomial neers uh and then we'll use pipeline to combine those vectorization and model all as a single Pipeline and then job
(1:01:48) Loop to save and load models or even data sets stop words to remove those then word tokenize to tokenize the sentences and then we have reg to clean basically whatever it's required this is just a demo so we are not expecting a very good performance and all here um here we are creating a sentiment analyser class and in initialization we have Max features which is 5,000 and test size is 0.
(1:02:19) 2 random stus 42 all are provided in initialization the pipeline is set to none by default and the stop words is the set of stop words uh which are English you know which we got from nltk Corpus okay to understand this what we are going to do is we are going to go through the main function okay so first you're creating an analyzer which will call the init function and we have seen what happens in the unit function so first what we'll do is we'll load the data so here we have the load data function which will load the IMDb data
(1:02:51) set so what happens here first we are calling the tfds load from function basically it will load the T oflo data set about IMDb reviews okay we are setting the with info as true so that it will provide the metadata information and all and as supervised as true so that you'll get the labels as well okay and data set of train and data set of test is obtained from there which is train data set and test data set so what we'll do is we'll concatenate these two data sets and then we'll create nump
(1:03:23) array of those okay that will become your X and Y X is your independent feature and Y is your dependent feature Y is basically your label which depends on your X okay and that is how the data data set is loaded okay now you have the prepare data section okay what it will do it will basically split the data set okay so it will pre-process the text and then it will split it as X and Y as X train X test y train y test so that while testing we'll make sure that you know uh those data is not leaked and this data is basically not test data it
(1:04:05) is validation data okay you'll validate the model validate the performance that how it has trained itself okay now you'll call the train model function where what it will do is it will create the pipeline first so how the pipeline is created the pipeline is created by calling the T of idea vectorization okay and the max feature if you see it is going to constitute about the number of embeddings okay so I'm going to represent it with 5,000 features that is what is Max features all about and once I represent those
(1:04:37) I'll classify with the help of multinomial Na bias classifier okay once you create that pipeline you'll call the fit which is basically the train function for ML models in Psych Lear where how you'll do that you'll do with X train and Y train Okay so given an X like this you'll get a y label like this that is what is uh pipeline all about okay now your pipeline is trained okay what you'll do you will evaluate those okay so how is that so once your pipeline is train you'll call the
(1:05:12) predict function on your test data set and those will be your predictions data predictions and we'll call the classification report which will have the Precision recall and so on and so forth everything will be there in the classification report and will also print the accuracy so once that is done we'll save the model and when required we can load the model and do inferences later and how you'll do that once you get the text you will process the text and then the pipeline will predict the output okay so this is how the pipeline
(1:05:45) is also here now let's see how this works python eore 1core beginner s by d p and now uh it is trying to load the IMDb data set oh it is already there in my system so it is not downloading it for your case it will download it okay just pre-processing the data and as you can see here the data set is already loaded with 50,000 samples in it to process those 50,000 usually it will take some time and it is done now it has split the data into training and testing the training model procedure is happening now and the model
(1:06:22) is trained you know like in minutes and here you can see the accuracy is around 86% that is pretty good right we didn't do a lot like for example this code this right I took only around 10 minutes actually because it wasn't very problematic and to get 86% accuracy it is pretty good if I work some things like you know like text pre bring and all I can push it to 90 92 easily right and here you can see the inferences is at real time right why you go for advanced model for these kind of use cases that is why I showed this in
(1:06:58) our prerequisite section okay here you can see the accuracy is 8536 the model has been saved and loaded again and when I gave in testing okay this movie was excellent the acting was superb it said positive I didn't enjoy this film at all the the plot was confusing and the characters were poly developed it identified that it was negative an average movie it has movements but it overall it is just okay if I had trained it for neutral it would have S neutral but it gives a negative feeling right so it has said negative right and that is
(1:07:32) how you'll train your own na bias classifier for sentiment analysis in the next section we'll be moving on with the intermediate prerequisites where I'll be covering about RNA networks for your advaned use cases then just text classification all right so you can see there are some main networks you to see and we'll also make a story creator with the help of RNN that we also do okay and that will be done in the next section I hope you all like this video guys if you like this video please hit the like
(1:08:08) button share it with your friends if you haven't subscribed to the Channel please hit the Subscribe button hit the Bell icon this is a free course and here you can see for prerequisites itself I'm not even able to make you know a single video after I've made it as beginner intermediate Advanced because that requires that much time you know for example this beginner video itself took around one hour and Hereafter it is going to take even more time because I need to explain the networks at least to
(1:08:36) some extent so that you'll be able to understand why the llm networks are preferred and how it came right so these are all for free all I'm expecting is just some likes and comments of yours so please hit that I'll see you on the next video until then then Happy learning welcome to the day Zero intermediate prerequisites previously we saw about the beginner prerequisites in which we saw a detailed introduction to NLP which covers key components of NLP common tasks in NLP challenges in NLP applications of NLP followed by which we
(1:09:16) saw how the NLP pipeline Works after which we saw a detailed explanation on the methods which are present in text preprocessing where for regx alone we gave some special importance because that is even widely used nowadays even after llm has come into the picture after which we moved on to embeddings which are nothing but the vector representations of the text where this Vector representation basically captures the semantic relationship between those words we saw the types or embedding methods which can be used there is one
(1:09:59) embedding method which I didn't cover here and that embedding method is something which we'll see later because that is Advanced and those Concepts can be discussed only after I discuss about Transformers and then finally we ended up with an introduction to NLP with machine learning where we trained our own nebi sentiment analysis classifier so that was about the beginner prerequisite now in intermediate prerequisits itself it has made as it has been made as two parts so the first part would be about
(1:10:36) deep learning on an overall aspect like what are the aspects which are there while we train a neural network and so on and so forth like that and the part two will be focused on NLP oriented networks which is RNN so we'll be seeing about these networks in detail let in this video with that being said let's jump into the video by starting up with an introduction to deep learning answering the questions of what why when and how what is deep learning deep learning is a subset of machine learning that uses artificial neural networks
(1:11:14) with multiple layers to model and solve complex problems please highlight the word to model and why is that I'll say in a minute so it is inspired by the structure and function of the human brain now what is the difference between machine learning and deep learning is that machine learning algorithms have their own predefined mathematics and what they will do is they'll try to find the best value for a variable which is already defined in the equation which means it is expected that your data has a linear
(1:11:50) relationship in it okay but that is not the case with deep learning how much ever complex relationship is there how much polinomial it involves doesn't matter because these neural networks will be able to find those relationships by themselves so that is what is deep learning why should we use deep learning automatic feature extraction Because deep learning has the capability to automatically learn features from data rather than a lot of feature engineering which will be usually done when we work with Moine learning models scalability
(1:12:29) performance usually improves with larger models and more data so it is highly scalable versatility deep learning networks have a wide range of applications from text to image to speech and so on and so forth even playing games and it provides state of the heartart performance in whatever domains we see it achieves the top results often even it surpasses human level performance so that is why we need to use deep learning when do we need to use deep learning when there is large amounts of data we can use deep learning
(1:13:05) Because deep learning like you know it thrives on Big Data it loves having big amounts of data when you have complex problem like I said if if there is a high nonlinearity in your data or there is high dimensions and you want to find a variable with the help of machine learning that is going to be hard so at that time you can use deep learning when you have unstructured data like I said image audio and text you can use deep learning for time series data you can use a specific variant of uh neural network which is RN and lstm which we'll
(1:13:42) see in detail in the later part and then in autonomous systems also you can use how deep Learning Works the pipeline basically so first you'll collect the data and then you'll make your own neuron work architecture so that is your model building phase and then model training phase what we'll do is we'll do a forward propagation calculate the loss and then we'll update the weights in the backward propagation phase these are just introduction don't worry all of these will be explained each and every
(1:14:13) Concept in detail later in this video evaluation testing the model on unseen data and then once you have evaluated you can just deploy it and that is what is using the rain model for predictions on new data so what are the advantages of deep learning or machine learning Feature Feature learning ml often requires feature engineering but DL can do that by themselves performance with large data machine learning has its Plateau Plateau is something like you know your curve grows like this let's say your learning curve is this okay
(1:14:47) higher the better machine learning will go like this and then it will go like this okay but deep learning will always go like this it always thrives with large data handling unstructured data machine learning can't do that but deep learning can scalability so machine learning models are faster um it is not highly scalable for complex use cases but you can easily SC scale your de learning models for complex use cases by just adding layers transfer learning nowadays if you see llms we are not training the llm we are transfer
(1:15:22) learning we are fing the LM it has some knowledge to we are just adding some knowledge on top of it that is easier with the case of deep learning but you know with a machine learning it is very limited so to say like you know it is close to Impossible and then we have parallel processing which is something you can do with deep learning but not with machine learning however deep learning has some disadvantages as well like I said it requires large amount of data and it is computationally intensive it is not as interpretable as machine
(1:15:55) learning because with machine learning you have equations defined prior but with deep learning it will try to create like you know identify the equation by themsel which means it is going to be a black box and if you don't regularize your network and normalize your network well it will be prone to overfitting so depends on depending on the problem you are facing and data available and computational resources you can either go for Mo learning or deep learning now with that being said how will you to create your deep learning neural network
(1:16:28) architectures Here Comes our Savor pyo py if you see it is an open source machine learning library or deep learning library like I said deep learning is a subset of machine learning it is a library which is been developed by Facebook's AI research lab it was created by Facebook AI but now now Vos is a separate organization it provides a flexible and efficient platform for building and training neuron networks making it a popular choice among researchers and developers in the field of deep learning there is another competitor as well
(1:17:02) which is T of flow but it is more rigid but P to is more flexible why is that first it allows you to use Dynamic computation graph so you know like the model architectures will be very flexible and easier for debugging Dynamic computation graph is nothing but you know your forward and backward computation will be you know a form of graph where the gradients Sol will be stored okay so that is your Dynamic computation graph pythonic interface the most important aspect as far as the developer is concerned it provides a
(1:17:37) natural interface that aligns with Python programming practices if you are a python developer you will be very well versed with using classes and pyos expects you to use classes that is not the case with tsor flow GPU acceleration P provides built-in support for Cuda uh to be used for training your neuron networks autog system which will automatically compute the gradient while you do back propagation and it has a variety of tools for CV NLP and reinforcement learning these are some of the key features and reasons why
(1:18:14) developers and researchers in the field of deep learning prefer pyou over other other Frameworks with tens oflow being the second most popular framework right now so pouch has many functionality and here it has broadly been categorized into some areas one is tensor operations so at the core py tensors are multi-dimensional arrays as like n aray so if you are wondering nump ARR what is it it is just you know normal list but you know uh it is of nump which will allow you to do complex mathematical calculations so this now torch
(1:18:55) tensas will allow you to run it on gpus okay so let's say I have an X tensar which is 1 2 3 and then white tar where I'm calling t. Rand and the size is 3 cross 3 okay so 3 cross 3 random tensor will be generated something like this you will have your data like this okay that is what is 3 cross 3 so how how how you can identify this when you call tensor do shape you'll be able to see the tensor shape okay so I'll also print that print y do shape okay and I also print Z along with that we'll also
(1:19:44) print W which is just you know multiplying uh the same array on itself so here you can see the tensor shape is 3 cross 3 and here is basically a resit where this is how it has been added okay so with 1 2 3 being here and if you print y you'll be able to understand what is the value of y you can see it is 0.
(1:20:16) 94 0.60 and 0.95 with one being added everything will become 1. 1. 1 Point same will be 2. 3 point and so on and so forth okay that is your basic tensor operations and all will be under torch okay and in torch there is something called autograd which will automatically compute your gradients and that is why I said you know the computation graph is very flexible so here all you need to do is just set request grad is equal to true and what will happen is if I do X is equal to toss.
(1:20:52) 1 of 2 cross2 so it will generate a 2 cross2 Matrix and then we'll add two to those variables okay and once that is done we'll multiply y by Y and then we'll multiply it by three okay so if I do this here you can see uh I've got z. mean and if I call backward and then finally if I print the X gradient this is what is my gradient but previously if you see X it has been 1 one one one this is the value of the gradients not you know the value of x all right so that is autograt so aat is basically you know your differentiation
(1:21:36) which will be used for loss optimization which is in back propagation okay so there is a important module which is t. NN module or neural network module when we say it as an AB abbreviated form so the pyos provides high level for building neuron networks through torch.nn module and here is how you can Implement A Simple Touch neuron networks so torch.
(1:22:05) nn as NN you will import it you will create a class or whatever you want and you'll inherit this nn. module first in init do self you need to override the parent class so you'll call Super off simplet Comm self do Dunder inet so here we are taking two fully connected layers ERS so what are those fully connected layers it is nothing but you know like those are Ann basically okay and what it has is a number in represented in dimension 10 will be represented in dimension five but it will be learning out of those okay that is how it happens
(1:22:44) with linear layers so here if I print the model this is how the model is okay and how the forward will happen here your relo activation function will be up apped on Le connector layer one output and then th that output will be sent as an input to fully connected Layer Two which will provide the x output okay so that is your neuron Network modules and with data loading and processing uh it is very important to know about the tools provided for data loading and processing so the first tool is torch data set it will allow you to
(1:23:26) make uh data set of your own and then data loader which will load the data okay so this is how the structure is don't worry we'll see about this in detail but just to say these components uh will work together to provide a comprehensive framework which will help you to develop train deploy your deing models with ease and pyos design has been uh emphasizing on E of use flexibility performance and then it is very powerful for both research and production environment okay so those are the py functionalities key
(1:24:06) functionalities okay but there are some other functionalities as well like for example tor. opum optim which contains the optimizers which we'll see later okay now jumping on to the pyos data set and data loader so what is pyos data set in pyos the data set is an abstract class which which is representing your data set any custom data set you create should inherit this data set class and it will override these two methods one is Dunder learn and then another one is Dunder get item so the dunder learn will
(1:24:38) say like you know the size of the data set is this much and under get item what it will do is once you have an index passed the data will be returned okay that is what is get item so if you have any kind of pre-processing to be done on a raw data those will be done in this get item section okay so for example I want to do something like this it will be done in thundergate item so the custom data set class will inherit the data set class and here in init function it takes two values it is data and labels so the length of the data is
(1:25:18) going to be length of self. data and in get item what we are going to do is we are going to provide the data at an index okay that is what is the purpose of data set a data set or a data value at a specific index in the data set is obtained via get item okay so that is what happens in to data set but one data set is not enough okay it is not of any use what we use is data loader so one of the most important advantages of BOS is that it supports multi Ing and for using multiprocessing you need to use data loader what it will do is it will
(1:26:01) iterate over the data set while it supports automatic batching shuffling multiprocessing all it will support okay these are the three main features of a data loader so if I have a custom data set being called with a and one2 I'll say my bat size is one and I'll set shuffle to True num workers you can set to how much you want I don't have lot of processors left in my system course so I'm setting it to zero so that it'll run in single thread but you can run in multi threads as well so here you
(1:26:34) can see a is printed because that is the first one and then the label is also printed okay so you can see right so why A and B are coming one like you know not in the sequence it was a b a and then another time a and then B so why it happens is because we said Shuffle to true okay so what this data loader will do is this will provide the batch of inputs to the model okay but this will support multi- GPU multi uh processing and all so making your training more efficient so basically the idea is that you will create a data set which will
(1:27:14) represent your data set basically so it is not just a data frame it will be a torch data set and that will be loaded with the help of a data loader into the model so that is what is all about py data set and data loader now let's see about neural networks what are neural networks neural networks are computational models which are inspired by human brain structure and function because researchers wanted to know you know replicate the functionalities of human brain because that would be the real step towards artificial
(1:27:49) intelligence rather than with a pre-computed equation so that is what what is Neuron networks all about it consists of interconnected nodes or else neurons because neurons is the smallest functional unit of a neuron Network and neurons is the most smallest functional unit of a human nervous system okay so like I said you know cliche right they wanted to replicate the human brain so these neuron networks have interconnected neurons which are organized in layers and these are designed to recognize patterns and solve complex problems so
(1:28:30) that is what is Neuron Network so why should you use neuron Network so whatever we said as a reason for using deep learning it will apply to neuron Network because neural network is the reason for which uh deep learning became like you know popular pattern recognition it is excellent at identifying complex patterns in data because it computes the relationship equation by itself and then adaptability it can learn and improve from experience generalization it can make accurate predictions from you know like experience on unseen
(1:29:06) data nonlinearity like I said it can model can model it means like you know it can create that is what is a stylish way to say like you know can model complex nonlinear relationship and parallel processing that is also allowed and how do neuron networks work so once you get an initial data it will be sent to the input layer and it will receive the initial data will be and will be passed to the hidden layers where the real magic happens where all the calculations are done with the help of weighted connections and then the output
(1:29:44) layer will provide the final output here in between we'll use a lot of things like activation functions we we are training it and we'll do backward propagation which is using Optimizer and all okay so that we'll see later but the process is that you'll have a f propagation and how that will happen input will input layer will obtain your input and hidden layer will do the mathematical calculation and say like you know this is this could probably your out be your output and the output layer will say like you know this is my
(1:30:17) final result so that is what is new network now this is how you can create a simple perceptron from scratch with the help of numai so first we are just creating a simple class known as perceptron where we are saying like you know our input size and then the rate at which the model leads to learn learning rate is a control parameter where it will say like you know at what rate you want to learn and don't worry uh we'll see about learning rate later in detail in the backup propagation section so the weights on are going to be np. random.
(1:30:54) Rand like you know we are just randomly initializing weights and biases and the learning rate is already obtained here okay and for Activation function we have a function called activate and predict will be your output gate basically okay so how basically a Perron works this is how it works and now you won't be able to understand this okay I'll say that now itself now you won't be able to understand this when you will be able to understand this once I teach about forward propagation and backward propagation okay let's see that
(1:31:31) now once we see about the different architectures of neuron Network like you know variety of neuron Network architectures which is available first is the most basic type which is artificial neural network which will have one input layer one output layer and one or more hidden layers uh the most suitable use cases would be uh classification regression patent recognition where you know you have a closer to structure data Pros is that it is versatile it can approximate any function basically and the cons is that
(1:32:04) it will structure with spatial or temporal data spatial is nothing but the geographic data and temporal data is a data which involves time as an aspect inside that so CNN are specialized for crossing grid like data such as images okay so the key components here are convolution layers pooling layers and fully connected layers fully connected layers are you know one single perceptron and then you know like you have replicated it as a layer okay that is what is fully connected layer dense layer we say it so the use cases will be
(1:32:40) mostly involving Vision related things computer vision tasks like object reduction image classification and so on and so forth it will be very efficient for special data so like you know when there is geography related like you know images it will be very good and the parameter sharing thing which is there in pooling it will reduce overfitting but the cons is that when there is non-p special data like for example temporal data it will struggle there came RNN to solve the issues in CNN so if you see Ann had issues which was solved by CNN
(1:33:14) and CNN had issues which was solved by RNN but there are things which CNN can do but RNN can't do okay because RNN had focused on one specific issue which was there in CNN okay so RNN are designed to work with sequential data the key feature is that the loop in the network which is all which will be there it will allow you to have information being persisted towards like you know all the time there are variants of this network like lstm Gru and all the use cases is mostly related to sequencial data which
(1:33:50) is like you know natural language processing related task Time series task and so on and so forth Pros is that it can handle variable sequence length data sets maintaining temporal information and cons is that it is very difficult to train due to Vanishing and exploring gradient okay so these are the type of neuron Network before going into P which is creating a p simple Ann let's see in detail about forward propagation and backward propagation and then we'll move on to creating a pyo simple pyn okay so
(1:34:26) for forward and backward propagation I have provided a separate file markdown file because there is a lot of mathematics involved in backward and forward propagation so to explain that it is better to have a separate file that is what I felt and I have provided here so let's start with the forward and backward propagation now in neural networks forward propag and backward propagation are the two most fundamental processes in the training procedure forward propagation is the process of passing the data input data through the
(1:35:05) network layers where you'll calculate the output and you'll compute the loss by comparing it with the actual Target basically what you'll do is there will be an input layer and you'll provide your input there and then all the calculation will happen in the hidden layer and then will be sent to the output layer and then the output will provide an output which is a prediction and that prediction will be compared with the actual Target and that will provide you the loss right so that is the forward
(1:35:41) propagation so now I've have said it in an overview but how this actually works let's see as a step by step with a mathematical example right the input layer what happen happens in the input layer the process starts by feeding the input data to the network that's all okay just providing the input data to the network that is what happens in the input layer let's say we have two features okay two features is like two Dimension okay one is X1 another one is X2 and your target is y and here Y is
(1:36:16) equal to 1 X1 is equal to 1 and X2 is equal to 1 these are the features okay now what what you will do is first you'll calculate the weighted sum or else it is also known as linear transformation each input is multiplied by a corresponding weight and it is summed so if you all know what is linear regression the formula is yal mx + C okay here the linear transformation equation would be Y is equal to W transpose x + B there m will be your slope and c will be your constant right so here it is replaced by weight and
(1:37:00) bias okay and by the way um we'll update those weights and biases that is there okay but these are the parameters which are responsible for neural networks to learn for neural networks to learn we'll be updating the weights and biases so here you can see the each input is multiplied by corresponding weight and Sum along with that we'll add a bias term as well because at times there might be a possibility that you might face zero when you multiply right so at that time now let's take an example that
(1:37:40) we have two neurons all right the two neurons will have weights for each and these are in Hidden layers okay here the weight for the first neuron is W1 and W2 which is 0.1 and 0.6 and weight for the second neuron is 0.5 and 0.7 all right now here you have bias as well which is B1 and B2 and the values are 0.2 and 0.
(1:38:15) 3 okay so why is there two weights for one neuron let's see that here we have a sample neural network which have has two input neurons like I said we have two inputs right one and two as X1 X2 two hidden layers neurons actually there is only one hidden layer as you can see here and there is two neurons in there okay that is why the representation is input layer belongs to a layer which has two neurons hidden layer also belongs to two neurons and output layer has one neuron that is what is our architecture and this is how
(1:38:55) the neural network will look like okay now let me tell how those weights came into the picture here you can see there is a red line there is a tainted blue line and then there is another red line there is another blue line right so these lines contain the weights okay these lines contain the weights and there is a bias as well okay similarly here also if you see there are two weights and there will be one B okay so this is how a neural network will look like so here you can see if I increase the number say it is like this and the
(1:39:38) hidden neurons goes like this I have another layer let's say and here you can see now this is a bigger neural network as you can see the connections are pretty huge and yeah here you can see there are uh a pretty good connections between everything all right so this is how basically a new network looks like and works like in a forward propagation now let's continue with the explanation so we computed the weight we compute the weighted sum how that happens in the neuron one we'll calculate it as X into
(1:40:17) W + X2 into W + V1 like I said the W transpose X Plus plus b right this is how it happens so if I calculate let's say the weights initially is initialized as like you know weights you can't control it you'll just randomly initialize it following some methods like hey uniform um hey normal zavier normal zavier uniform like that okay so the output would be of like the first layer first hidden layer would be 1 into 0.
(1:40:52) 4 + 2 into 0.6 + 0. 2 which will be 0.4 + 0 1.2 + 0.2 which is equal to 1.8 now Z2 will become 1 into 0.5 + 2 into 0.7 plus 0.3 which will become 2.2 okay so this is how the weighted sum is now what happens is in a forward propagation in each neuron first the weighted sum will be calculated and after that that output will be applied with an activation function because that is what is used to capture the nonlinearity okay so the output of each neuron is passed through an activation function example re Sig we'll see these in detail
(1:41:32) later what the idea is that is it is to introduce the nonlinearity the Rel activation function mathematical formula is Max of 0 comma X okay so let's say there is a neuron one the A1 which is the output of neuron one itself let's say like you know there is a neon and the half first half will be weighted Su second half will be activation function output so the activation function output is technically the neuron output so the neuron output of first neuron is zero Max of 0 comma 1.8 which is 1.8 and then
(1:42:12) Max of 0a 2.2 which is 2.2 that is what is the activation function or the output of the Hidden layer one okay now you have an output of 1.8 and 2.2 what happens now is you have an output layer and like I showed in the diagram there will be only two weights following okay so there is two weights here 0.8 and 0.
(1:42:38) 9 along with one bias what happens here is we will multiply with 1.8 and 0.8 together and 1 Point 2.2 and 0.9 together plus 0.1 okay finally your output layer output is 0 3.52 but what was our expected output which is 1.1 okay that is our expected output now you might wonder okay there is input but there is output which is not at all related how we will adapt that that is why we have the process of loss calculation and followed by back propagation where what we'll do is we'll change the values of this weight one and weight two such that
(1:43:20) this output value of 3.52 will become closer to one okay now let's see how those happens loss calculation is the final step in forward propagation and how it happens is you have a true Target which is your output or your expected output and then you also have the model output which we'll say like you know y output common loss functions used are mean squar error cross entropy loss and there is hinge loss binary cross entropy loss and all but right now let's see the most simplest loss function which is
(1:43:56) mean squared error mean squared error is the formula is we'll compute the error and then we'll square it and then we'll divide by two okay so the Y output is 3. 52 y true is 1 so 1x2 of 2.52 the whole Square will be 31752 okay so that is the loss right now now you need to reduce this loss and that's where back propagation which is the second phase or probably the more important phase in deep learning comes into the picture which involves calculating the gradient of the loss function with respect to each weight in
(1:44:39) the network and then updating those weights using an optimization algorithm for example that is gradient descent how step by step it happens first we'll compute the gradients for loss and the formula is partial derivatives we'll use partial derivatives here do loss by do output so it will be Z output minus y true okay do L by do output is z output minus y true and the value would be 2.
(1:45:11) 52 now you have do loss by do output okay now what you will do is you'll calculate the gradient at that output layer okay now you have calculate calculated the gradient for alone now you'll calculate the gradient for the output layer itself so how that happens is with respect to every weight we'll do that okay do L by do weight 5 is equal to if you see the weight 5 it has been constituted with the help of activation function of first neuron's output so we'll take that only okay in back propagation everything happens in
(1:45:49) Reverse so activation Function One into Function One output into to loss by do output which is 1.8 into 2.52 which is equal to 4.53 536 so similarly you'll calculate it for uh do loss by do W6 which is 5544 and then we also calculated it for bias because bias doesn't have uh value already it is just a constant value so here you know for partial derivative we'll just consider the value of do Los by do output itself now we'll calculate the back propagation at the hidden layer now to back propagate the error at the hidden layer
(1:46:33) you should use a chain rule okay the error signal at he each hidden neuron is a product of the derivative of activation function and the error propagated from the output layer so what does that mean these values will be propagated to here as well along with the differentiation of the output of activation function okay so the activation function is reu and if you differentiate that if the asset is greater than zero it will be one and zero otherwise okay you will never face zero a lot of times there are some times
(1:47:13) and we'll see about that later but you know it will be always one mostly here if you see uh everything is one so we need not like you know worry about multiplying it with some one or zero all we need to do is multiply the weights of five and six with this output we obtained okay because this is going to be the value which will impact the gr gradients at the hidden layer so it is going to be 0.
(1:47:50) 8 into 2.52 which is equal to 2.0.1 2.16 2.2 68 okay now these are the error at neuron one and neuron 2 okay that is why we multiply the weights now what is the gradient there so if you want to calculate the error right you will use the weights values of you know like the layer + one okay let's put that way let's say if it's a hidden layer and there is one output layer and then one input layer okay so to calculate the output or like you know the gradient at the hidden layer you will use the error at output layer and then you will calculate the gradients with the help of
(1:48:32) inputs which we we obtained previously okay so the calculation would be do Los by do W1 into do Los by do at 1 which is 2.06 and 4.32 in place of weight 2 and then bias 1 will be 2.06 there also if you see we calculated it for weight five and weight 6 right now we are calculating it for weight one and weight two because that is the first neuron and second neuron is 2.
(1:49:02) 26 eight and 4536 and here since now we have the values of gradients we can update those values so if I want to update a weight one what we will do is we'll have a learning rate like uh I said right previously the rate at which you want the model to learn slower the better okay slow and steady wins the race is the prb and that works for deep learning okay so the formula would be let's say this is weight new okay because you're updating that weight right weight new is equal to weight old minus learning rate into do loss by do W which we calculated
(1:49:42) already okay so it is going to be 0.4 because that was the weight one you can see right weight one is 0.4 into minus 0 01 into 2.06 which is 0.37 now if you see the weight has been reduced from 0.4 to 0.37 similarly from 0.6 it has been reduced to 0.55 similarly it will reduce or increase based on you know like how you want it to be okay this process will be repeated for each training example across multiple box until the network will converge okay so in that way your loss will become minimal this is how basically the Deep learning procedure
(1:50:26) happens in this as a summary for propagation will involve wait at some applying activation calculating output and then loss from that loss you'll start back back propagating the gradients with respect to weights and biases and then you'll update them with the help of gradient descent so here it was said as a simple step that it is update of weights and bias which uses gradient descent okay gradient descent is an iterative optimization algorithm that is used to to minimize a loss by adjusting the model parameter which is
(1:50:56) weights and biases the main idea is to move towards minimum loss function by taking small steps in the opposite direction of the gradient of the loss what is meant by that let me show you here is a sample curve of gradient descent okay if I say here here you can see the main ideas to move towards the minimum uh like you know taking small steps in the opposite direction of of the gradient which is slope of the loss with respect to its parameters you can see let's say initial weight is here okay but you know the loss will be zero
(1:51:34) at here and this place is known as Global Minima okay at real time the curve W be looking like this it will be like you know it will be like this and this your challenge would be you know like there will be a lot of local Minima here like this but there will be a global Minima here you need to attain here okay that will be a challenge we'll see about that later okay now we will have those gradients right we already calculated those those are the gradients which are here now your weights are here you need to take a step by moving from
(1:52:07) here to here and then here to here here to here here to here here to here finally you will achieve the minimum cost at zero like close to zero you will never reach zero um but you'll achieve close to Zero Performance Okay so this is how the green diserve curve works okay now let's see the mathematical explanation because we saw a graphical explanation right now what you'll do first you'll compute the gradients the gradient of the loss function with respect to each parameter which we which we did before here you can see right
(1:52:42) this partial derivative uh we calculated right those will provide the gradient for a v w the gradient will be do L with respect to do w if it is in the output layer yes it will it will be directly impacted by loss but if it is in you know like hidden layer you need to propagate the impact of loss from those output layer to the hidden layer that is what we did here okay first we took the output and then we impacted it at the output layer and then we back propagated it to the hidden layer with the output of these layers right so no law alone
(1:53:21) will be considered but those output also impacted it right so we'll consider those outputs as well so here is a sample okay let's say my weight is 0.5 and then my gradient of do Los byw is 0.2 and the learning rate is 0.01 so 0.5 - 0.01 into 0.2 will be 0.498 now it is a bit adjusted now it will come closer okay that is the idea of gradient Des this is how you'll update the parameters for bias also it is the same bias new will be equal to bias hold minus learning rate into do loss by do B do B is do bias right partial derivative
(1:54:02) calculated with respect to loss for bias this is how gradient descent works and there are algorithms like bat gradient descent stochastic gradient descent mini bat gradient descent and so on and so forth these are the basic optimization algorithm there are lots of optimizers which has come into the picture which we'll see in the optimizer section okay but this is how forward and backward propagation Works yes I explained it very theoretically um and you know like I said some uh explanation with the diagram that is for
(1:54:37) simple so you know like just try to put some time into this document and then you'll be able to get a hang of what is happening okay so as a summary what happens in photo propagation is you'll it is a process of computing the output of a neur network for a given input why you should do that you should do that to make the predictions and compute the loss how you'll do that it is by sequentially applying each layers operation to the input data back propagation is the process of computing the gradients of the loss with respect
(1:55:10) to the network parameters and why you'll do that it is to adjust the model parameters which have the weights and biases to minimize the loss and for this you'll use the chain rule okay chain rule is what we did you know like we did it separate separate and then we multiplied it but you know like at real time what they will show is you know like it will be a single equation by applying the chain rule of calculus uh you'll propagate the gradients backwards through the network okay this is what happens in forward and
(1:55:38) backward propagation now that we know that let's see the theory behind the simple Ann Network and then we'll code that okay a multi-layer network also known as a feed forward neur Network consists of multiple layers of neuron I showed you right there is multiple hidden layers when I showed you that so there will be one input layer and output layer that is constant but hidden layer and uh is like you know number of times you can just replicate it more the number better the output and then back propagation just now we saw right it is
(1:56:14) the process of adjusting the weight now let's see how we can code this okay this will be a sample code okay don't worry we'll train that P provides a high level API for creating neuron networks through talk. NN module so for what for that what you need to do you need to First define a class that inherits the N and.
(1:56:34) module Define all your layers which you'll use in the dunder init method and then implement the forward method which is just you know what we saw as forward propagation taking the output of like you know like you know first calculating the weight at sum and then applying the activation function provide that output to the next layer calculate the weighted sum and if there is an activation function calculate that and so on and so forth till output that is how we did the forward propagation Theory right so
(1:57:03) that's how we'll do it in forward as well here let's take a simple example that we have two layers okay and there is one activation function for the first layer along okay so the dimension here would be 10 cross 20 and 20 cross 2 now you'll wonder 10 cross 20 and 20 cross 2 how is this thing so what happens is actually you'll have three layers to look like okay there will be 10 neurons at first which will be projected to 20 neurons here and then those will be projected to two neurons okay so if I
(1:57:41) had it like 2 2 one this will be the same network which we discussed right now okay so here we have the layers being initialized as linear layers those layers which we said about is linear layer simple linear layer and then the activation function will be also there in NN module itself NN module so we'll call R Rel activation function and here if you see once we get the input we are passing it through the layer one which will provide you Z one okay and then that Z1 will be provided to reu which will be your o1 and o1 is provided to
(1:58:22) your lay to which is to your output and then that will provide your output okay so this is how a neural network is defined and here you can see there are some common layers there is linear layer which is also known as fully connected layers that is what is used Inn and linear layers generally used in any network because fully connected layer is the only layer which has the capability to extract the information which is already there by expanding it and Contracting it you know like the equation it calculates it is very
(1:58:55) versatile to adapt it to any domain there is 2D convolution layer which is used for uh like you know images there is RN lstm Gru for recurrent layers which we'll see later and batch normalization is there and then Dropout is also there Dropout is there because you don't want your model to memorize things right so at that time you know like what we'll do is it is a normalization method where what you will do is you will just drop some of the neurons let's say you have 10 neurons in one layer you have provided the
(1:59:27) probability of Dropout as 0.2 what it will do is it will keep eight neurons and two neurons output will not even be considered considering that that is a Dropout neuron batch normalization is the process of normalizing the values or inputs which are provided to that layer okay that is batch normalization activation function there are common activation function like reu sigo T soft Max and for loss MC cross entropy BC LW an Optimizer stochastic gradient descent which we saw already a variant of gradient descent and then there is Adam
(2:00:00) RMS prop and all okay so a training Loop example will look like this first you will get the loss function you'll initialize your Optimizer so what you will do is you'll go through the number of epochs for each Epoch you'll go through the inputs which are there in the data loader you'll get the model outputs you'll calculate the loss you'll first set the optimizer to zero gradient Optimizer is nothing but the gradient desent algorithm which we saw okay and then you'll call the loss. backward and
(2:00:29) the good thing is the loss do backward will compute all those partial derivatives we spoke about okay so the backward propagation will be done and the model weight would have been updated now you'll call the optimizer step function which will complete that procedure okay backward propagation will calculate those gradients and the updation will be done with the help of Optimizer that step this is how a basic training Loop will happen okay now we saw a simple code explanation now let's train a simple Ann Network itself okay
(2:01:02) here I have imported Torch from torch. neuron Network modules I imported it as neuron network module and then we have imported uh touch. Optimizer as Optimizer from uh data utils we have taken uh data set and data loader from SK learn. model selection train train test split is called called and then NPI is imported as NP you have a random data set class okay like uh the class why I named it as random data set is because I'm just R generating random values for X and Y here the thing is there is only one uh input feature and one output
(2:01:42) feature based on this input feature it should see an output okay that is what is the expectation here so I've just provided as X and Y where Len will be X Len of self self. X and when you call an index it will provide the value of x at that index and value of y at that index okay the neural network we already saw there right that is what is the neural network here and then we have th samples which we need with input sizes two so why it is input Siz as two we need to provide it to a layer which has two neurons as hidden size sorry input size
(2:02:25) okay and then we have the number of classes which is output as two okay that's why we keep it output sizes to Hidden Size Doesn't Matter okay so here it will be th000 cross2 and then here it will be 0 comma 2 comma th000 this is how it will be okay and now here if you see these are the variables or hyper parameters input sizes two 2 hidden size is 16 so it will be 2 cross 16 here and 16 Cross 2 here okay and the learning rate is 0.
(2:03:01) 01 I want to train it for 100 EPO and the bat size is 32 it seems okay so for every 32 uh samples being processed for one step 32 samples will be processed and at that time one back propagation will happen Okay so that is mini badge um SGD okay so generate data you will generate the data you will split it as train test uh with the help of train test split are creating it as a toss data set then making a data loader out of it and then you are creating the model the loss function is cross interrup loss because it is a classification problem when to
(2:03:40) use which LW or yeah when to use which LW we'll see that later because for each of those components already I I I have a dedicated section activation function cross function Optimizer gradient descent for each of those I have a separate you know um sections for us to discuss with okay let's let's not consider that right now you have your crossentropy loss right now which is Criterion your Optimizer is Adam Optimizer where you'll provide the model parameters for it to optimize along with the learning rate OKAY model parameters
(2:04:18) will provide the weights and biases for the optimizer to optim opiz with okay so in the training Loop for Epoch in range of epo so for every Epoch we'll set the model to train at that time let's initialize with the loss being zero and we'll go through the training data loader to get the X and Y value we'll send the X batch so what the DAT loer will do is it will create the X batch and Y batch with 32 values in it okay 32 x values are into the model which will provide 32 outputs and that will be
(2:04:56) computed as loss with 32 true outputs okay now you have your loss make sure that the optimizer doesn't have like you know any gradients in it because if it has the updation will be wrong so first set it to zero gradient start the backward propagation to calculate the gradients of floss and then call the optimizer do strip function to update the weights of model like weights and biases basically model parameters now we are just adding this Sloss so that we can print it and this is how the inference code is okay here if we run
(2:05:30) this oh it is taking some time now you can see it started and the loss ended up at 0.68 and apparently that accuracy is 52 it doesn't matter like this is just for me to show you like you know how it works because this is very random data so we can't expect any performance out of it okay so yeah uh that is how a simple a&n works now what are the components we spoke about like weights and biases are there that's we that's something we already spoke about there is activation function which will be the first thing
(2:06:07) in Pro forward propagation and then you have your loss function which will be calculated and then backward propagation happens and then you have your optimizers which in which gradient descent itself is an Optimizer okay so for each of those component we are going to see what are each of those why should we use those how should we use those and types of each of this component and for those as well we'll see like you know in detail explanation so let's start with the activation function activation function are mathematical equation which
(2:06:41) will determine the output of a neural network key thing highlight this it will determine the output of the neuron Network because that is what is the key for calculating and and capturing the nonlinearity in the data so they are applied to the weighted sum of the inputs at each neuron I told you right calculate the weighted sum and then apply the activation function output activation function on it to get the output so that is what happens here in activation function why you should use activation function it will introduce
(2:07:12) nonlinearity like I said it will normalize the output keeping it in a specific range for example with reu it is 0 to any any positive number because we don't want any negative number that is the idea of reu but there is options like Zig mode which is 0 to one or t which is minus minus one to one okay that will normalize the output and it will enable the back propagation as well because many activation functions are differentiable that will allow the gradients to flow backward through the network okay any function which is
(2:07:44) differentiable alone will allow no back propagation and activation functions are uh you know differentiable these would work they'll take the weighted sum of the input neuron and apply a mathematical operation based on what activation function it is and it will produce an output this output becomes the input for the next layer or final output will be out of the network that also will be coming out of you know activation function now let's come to the type of activation function first most common activation function which is
(2:08:15) sigmoid activation function a sigmoid activation function is a S shap curve that M Maps any input values to a value between 0 and 1 let me show you the curve for each of these type of activation function now this is how the curve of sigmoid will look like okay it is an s shaped curve which will map any input values to a value between 0 to one okay and how it will map that it is by using this formula which is f ofx = 1 by 1 + exponential of - x X which is E power minus X okay why you should use Sigma activation function it is useful
(2:08:59) for the models when you need to predict the probability of an output as soon as I say probability of an output you might ring it as you know like classification maybe yes if you have t it like that then good enough it is for classification okay but since it is 0 to one it is good only when it is binary classification okay it is historically popular but less used in Hidden layers because it can map only between zero and one that is not a case which you will see with the help of uh you know normal linear networks but this network is used
(2:09:38) widely in an very Advanced Network which is lstm okay there it serves a different purpose we'll see that in lstm section okay so there is another activation function which is tan H and the formula is exponential of x - exponential of Min - x by exponential of X Plus exponential of - x okay or if you want to say it conventionally it is E power x minus E power - x by E power x + e power- x this is how the formula of tangent hyperbolic tangent or tan is and here why you should use that see uh this is not a
(2:10:18) zeroc centered curve okay it is 0.5 it is not a zero centered curve but it is zero centered curve here so the derivative will be even better okay and it will often better perform better in than sigmo in Hidden layers because it has a range of minus one to one more the range better will be the value okay with hidden layers and this is how the curve is as a comparison the sigmoid will go like this 0 to 1 but tan will go like this minus 1 to 1 okay and you will initialize it like this okay so if I call it here my activation input will be
(2:10:53) zero and one now and then I'll be converting into float and here output will be 0.5 and 0.7 7311 how this output was calculated it is by following this thing okay this calculation 1 by 1 + E power 0 1 by 1 + E power minus 1 okay something like that it will happen and then it will provide you the sigmoid output so similarly we can do it for man as well so you'll call call the tangent hyperbolic tangent activation function and then if you print the output it is 0. and then 0.
(2:11:33) 7616 okay so that is how it happens with hyperbolic tangent but with linear unit which is rectified linear unit that is what is Rel it returns zero for negative values and input value will be positive for positive values now why you should use this you see exponential uh is a component which is very computationally intensive okay that was the case with sigmo and hyperbolic tangent or tan in short but Rectify linear unit is a Formula which uses Max of 0 comma X you know like max value between two values calculating is very
(2:12:09) efficient okay so that is one thing and then if you continuously differentiate a value between 0 to one or minus one to one this is a very high chance that it will go to zero okay in back propagation I said right you'll propagate the gradient and then you'll do differentiation if you see our weights reduced right the gradient value is reduced at some point of time of continuous reduction it will go to zero okay that is Vanishing gradient okay at that time your model will stop learning and then this also induces sparcity in
(2:12:43) the hidden unit because you know like the values are like that basically how you know there is a sigma curve here if you see it is 0 to one but but with reu it is zero to Y okay like you know it is infinity so if I do this it will be 0.0 and 1.0 you can see 0.0 and 1.0 right but there was a problem with reu reu had a problem of having zero for negative values and that created an issue known as dead neuron okay to solve that they attempted uh new method which is leaky okay d Rel is what they said it as or else that specific issue was known as
(2:13:26) dead neuron okay where the neuron once it gets stuck right like once it goes to negative it is not going to be become like you know positive because we'll always multiply it and it will become zero after that there will be no training okay so what we'll do is we'll multiply it with a small very small uh value for negative input the formula will be that you know it is x if x is greater than Z and if x is less than or equal to Z you'll multiply it with an alpha value which is 0.01 okay it will
(2:13:58) be like this the Leaky Curve will be like this where was like this okay so if I give this here output it will be the same as before but to compare what we are going to do is output let's put it as R out equal to R of um minus one um sorry to. ofus t. of minus one then we'll that okay you can see it is zero okay but if I do leaky of T of minus one it will be something like minus 0.
(2:14:44) 01 yeah it isn't implemented all you need to do is minus 1 now you can see here it is minus 0.01 right it is giving some value which is very less right 0 minus 0.01 is very less but you know like it will make sure that the training continues that is what is the advantage with leyu if you think that you know uh there might be a possibility for having a negative value go for leaky now soft Max uh a very important activation function which you will use it very widely even in Transformers so what it does is it will convert a vector of real numbers to a
(2:15:22) probability distribution okay so it is used in output layer of multiclass classification problem and then it will ensure that all the output values are between 0 to 1 E power x by summation of E power x you know till J so this Curve will also be like sigmoid okay since it is 0 to one and what it will do is basically know if I print the output here you'll understand see here the probability value of 0 is 0.
(2:15:57) 26 89 and 1 is 07311 okay that is what it is trying to say the calculation is happening like this okay basically what it is is let's say I have a multiclass classification problem of 10 classes okay so what it will do is the network will say like you know uh these are the values or like you know logits that's what we say for each class you know now you need to convert it to a probability between 0 to one and for that it will be applied with like you know um soft Max so basically if you see it is 0 + 1 divided by value okay
(2:16:42) that is what happens okay that is what happens here number by summation of all the numbers number by summation of all the numbers that is what it happens here here okay so if I print the activation input you'll be seeing right okay I hope you get that so E power 0 by summation of E power 1 so summation will be like you know um let's do that e z by E1 this is how it would be okay because it is E power 0 + 1 okay I hope you get that so this is how the calculation will happen if it was let's say 1 and two it
(2:17:24) would have become E1 by E3 and then E2 by E3 this is how the calculation will happen okay now like I said the gradient will flow through this activation function so you will differentiate it okay so this is how you know the calculation will happen for most of the activation function which is there already here so here you can see uh there is binary step logistic logistic is nothing but your Zig mod okay there is tan h r tan reu p e soft plus you know like everything is there along with the derivatives so if you plot the curve
(2:18:08) of derivatives this is how it would be okay tan will be 0 to one range Sig will be uh 0 to 0.25 and you know like Z mod of 2.5 that will also will that will be like you know a sharper curve and then T like I said it is 0 to one reu will be like you know zero maximum like you know it will go on it is not just one it will go on like you know uh zero or one there is nothing in between either zero or one because any number differentiated will become one okay that is what is differentiation right and then there is
(2:18:44) soft FL and goian activation function which will be uh like you know derivatives SC will be like this one and then like this okay so when you should choose which activation function for hidden layers re will be a default option or else like you know if you feel like I said there might be negative value in there choose Leu for binary classification sigmoid for multiclass classification softmax for regression AC regression you can use linear activation or you know you can use Rel as well because re if you see it is a kind of
(2:19:19) linear activation function if the values with above zero because it will just provide the same value okay so now we are done with the activation functions all right let's move on to the loss functions what are the loss functions loss functions are the measure which will be calculated by the difference between predicted output and the actual Target this will quantify the model's performance if you are this much deviated from you know like your target then you have some right that is your error so that is what is loss function
(2:19:54) all about why you need to use loss function this will become a guide for you in the learning process by providing a scalar value like you know a Quantified value that will show that like you know you need to be minimizing this value during training and how these loss function work is that they'll compute a score based on the mathematical formula it has by mapping the model prediction and True Value as the values variables like you know to that variables they map this and that score will be used to update the models
(2:20:26) parameters so let's see about the types first thing is mean square error a loss function uh that measures the average square difference between predicted and actual values this is usually good with regression problems and when like you know why this is a very good error is it will penalize the large error more heavily since you are squaring it right the large errors will be uh more heavily penalized so that it will know like you know yes I have long distance to go I need to like you know optimize myself
(2:20:56) faster like that and it is differentiable making it suitable for gradient based optimization every loss function will be mostly uh differentiable okay how this will be calculating first you'll calculate the difference between predicted and actual value you'll Square those and then you'll take the mean okay if for example here there is three right you will first calculate the difference between these so it will be one and then what you will do is you'll calculate the square which is one again and then it will be uh 1 by
(2:21:27) 3 okay so it will be 0.33 something like that let's try and if I print this loss should come as 0.33 if I'm not wrong yeah here you can see it is 0.33 okay that is how mean squ error Works cross inter V loss this is a loss which is very important because in Transformers everything is a classification behind okay so you'll use crossentropy law and soft Max activation function that those two are very important as far as llms and Transformers is concerned so please uh you know have a better concentration
(2:22:05) in these two uh components a loss function that me measures the performance of a classification model whose probability output probability is z between between 0 and one why you should use it it it is very suitable for multiclass classification problem and it will work very well with softmax activation in the output layer how we will use that first you need to apply softmax to the models raw output so that you'll get the probabilities between 0o and one you'll take the negative log likelihood of the predicted probability
(2:22:37) uh to the correct lash and then you'll average that across the sample that is how uh cross interr loss function Works behind Okay so right now what I'm doing is instead of like you know having it as zero here it was 0 one and then zero right so instead of zero I just change it to 0.
(2:22:59) 7 okay so if you see the loss now will be 0.74 61 okay so if it was Zero itself it is 0.55 okay so that is cross entropy and binary cross entropy it is a special kind of cross entropy where it is for binary classification problem okay so since it is a binary classification problem Sigma activation function will be very good at the output layer and how it works it will be the same as cross entropy instead of sock Max apply sigmoid calculate the negative likelihood and then you'll have your loss you can see it is 0.
(2:23:37) 56 65 okay if you see here these two are very close right 0.551 45665 because for both of these values now I provided you know a b uh a binary classification kind of an output Zer and ones okay that is is how the optimization sorry loss functions are now let's move on with the optimizers what are optimizers optimizers are algorithms which are used to adjust the networks parameters if I say networks parameters it is about weights and bies to minimize the loss function why you should use optimizers it will Implement different strategies
(2:24:14) uh to update weights which can lead to faster convergence or better generalization so that the model will update itself to wide range of um you know scenarios how do optimizers Works once you compute the gradients with respect to loss this will update the parameters and like you know with respect to the parameters it will update them accordingly like I said when you call the loss.
(2:24:38) backb all those gradients will be calculated and the updation will be done with the help of optimizers only okay the optimizer types of stoas gradient Adam and RMS prop these are the three main types of Optimizer there are um other types like Adam with weight DK uh add grad and so on and so forth but we are going to uh focus on these three alone okay gradient descent is one of the most basic form of gradient descent which will update the parameters based on a gradient of the current mini batch what is the mini batch for example I had a sample of th000 here right I had
(2:25:16) th000 samples here in which we took 32 as a mini batch for one step will update the value only for uh like you know based on the those 32 right so that is what is Optimizer which is sdd Optimizer you know like mini batch sdd that's what they call it why you should use that it a simple and memory efficient it can you know escape the shallow Minima due to it stochastic nature so basically gradient descent will go like this right 1 2 3 3 3 something like that but stasic gradient moves like this if it is a circle like this it will move like this
(2:25:53) okay so it can escape the local Minima a bit so how this will work you'll compute the gradient and you know you'll update each parameter by subtracting the linear uh learning rate multiplied by the gradient okay that is what happens with SGD Optimizer and if you print that these parameters will be shown okay the learning rate is 0.
(2:26:17) 01 and then there is momentum as well okay in sdd there is momentum as well but if you want you can provide that or else you can just leave it okay we didn't provide weight DK as well so this is the most basic form of Optimizer and this is a revolutionary Optimizer this is the optimizer which is used even nowadays but it was you know like found long back it is an Adaptive learning rate optimization algorithm that computes the individual learning rates for different parameters so what it will do is initially it starts with the learning rate and then it will
(2:26:48) update the learning rate also okay so here uh you know like it will combine the benefits of two Optimizer which is adag and RMS prop RMS prop we'll see about that here and there is uh issue with adagrad which is like you know dimition learning rate that was solved by RMS prop but there were some other benefits with adag which also was include with Adam okay this will uh work very well with uh data where there is no like noise or Spar gradients so basically know like Adam can be used anywhere how it works it will be
(2:27:23) maintaining a moving average over the gradient and the square gradient and those will be used to compute the Adaptive learning rates for each parameter okay so that is adem Optimizer and RMS prop is an Optimizer that adaps the learning rate again uh based on the gradient histry like adom but what it will do is it will just maintain a moving average for the squared gradient gradient and then it will divide the learning rate by the square root of this average that's all okay okay so that is how RMS prop Works uh we don't want to
(2:27:54) go into details a lot of Optimizer that is why I'm just simply saying like know this is how it works finally we have grent descent like you know like that is an Optimizer by itself but you know gradient descent is something we need to spend some time on so that is why I said it it provides a way to find the optimal parameters that minimize the solution how it will do that we know right right we know right now about that we'll compute the gradient of the laws with respect to each parameter and then we'll
(2:28:23) update those those in the opposite direction the types are basr IND which will be doing it for the entire data set mini batch gradient descent is doing it for small uh batch which we saw okay stochastic gradient desent will do it for single sample this is the difference between each mini batch SGD what it will do is it will do the stochastic gradient descent on a mini batch which we provided okay otherwise the formula is the same for each and one of these okay so now you have a initial idea about Ann all the components which are involved in
(2:29:01) a neural network training in the next part we are going to see about RNN networks because these play a major role with natural language processing because RNN were were created for sequential data and NLP is basically about sequential data okay text is sequential right so in the next part we are going to see about that hello everyone welcome to the intermediate prerequisites a Z part two so now we are going to see about the RNN networks in detail in the RNN networks there are four primary networks which we need to
(2:29:47) know about one is the first RNA Network which came into the picture which is RNN by itself and then others were modified out of it which are lstm and then blstm and then Gru all are RN networks and we are going to see about these networks in detail let's start with the recurent neural network so what is recurent neural network recur neural network is a class of artificial neural network all we derived from an and only which are designed to work with sequential data unlike feed forward neural network which
(2:30:26) is yourn RN have connection that forms directed Cycles allowing them to maintain an internal state or memory from previous inputs so what is this definition all about just forget these all right from x0 to xn just consider that xn is your input and there is Ann cell here okay and then there is dense and then output is obtained this is how Ann works but with RNN what they did was if there is a sequence of data let's say your batch has five inputs okay you have um stock price prediction as your use case and you have your close prices of
(2:31:09) past five days and you want to predict the next days okay so first you will send the first day's price in x0 along with the date and then you'll send second days price in X1 where the first um days price information is also provided to the second days okay there will be recurrence inside okay and that is what is shown here if you give a XI the SI which is you know the state of RNN will be recurrently provided to the RNN cell itself and the final output will be obtained with the help of dense Network so I hope you are able to 10
(2:31:51) okay in case of a there will be one one dense Network okay there is no recurrence in inside that and since there is a recurrence of State going on behind that is why it is called as recurrent neural network right now it begs the question how and why the model was created yes there were good models like Ann and CNN but why RNN came into the picture so rnn's were developed in the 1980s to address the limitation of traditional neural networks in processing sequential data if you are processing a sequential data it is very
(2:32:30) important that you remember the past which is your sequence that is not something all the previous models which like Ann CN and your machine learning models any of those models couldn't remember the sequence so they thought okay let's create a network which will do that so for that the motivation was to create a network that could use its in internal memory to process a sequence of inputs which will be making them suitable for tasks like speech recognition language modeling and so on and so forth wherever there is a
(2:33:02) sequence of data involved so basically NLP if you see is all about a sequence I'm talking now and I'm talking in a sequence right there is a word after a word or if we see as the model there is a token after token right so the key motivation was to create a network that could use the models internal memory itself to process those sequence of inputs which makes them suitable for tasks which involves NLP and also time series because time series is also recurrence right now let's see how the model works all right
(2:33:42) what is the basic structure an RNN consists of input hidden and output layers that is very usual the hidden layer has a self Loop connection which allows it to pass information to one from one step to the next step like like I said x0 is one time step X1 is the next time step and x0 output state is provided to X1 similarly it is provided till xn right there is a recurrence so that is what is said as Loop here self Loop connection so forward pass how it happens let's say there is a Time step T the rml will take an input XT
(2:34:20) and there will be a previous hidden State hidden State let's call it as H okay and it since it is previous hidden State we'll call it as T minus one so HT minus one so to compute the current hidden State why we are calling it as current hidden State rather than output is because here we'll compute for RNN cell not the dense Network once it passes through the dense Network that is when we'll get the output usually dense networks are very good with one thing which is to aggregate the inform or
(2:34:50) expand an information which is already there okay so right now we are looking to extract the information across temporal division okay which is time that is why we are calling it as hidden States okay rather than outputs being passed so to compute the current hidden State the formula will go like this if your hidden current hidden state is HT HT will be calculated by tan which is an activation function will be equal to uh sorry HT is equal to tan h of weight of HX okay into XT which is just your like you know WX transpose
(2:35:35) plus b that's what we do right so that is what is here but what we'll also do is we'll also provide the weight of hidden state of previous okay previous time step that is why it is pre w HH star h of T minus one okay so that is what we'll do that is how we'll do every state will be calculated by adding up the information which was there in the previous state now you might wonder what happens if I move 10 States before you see there is an advantage and disadvantage in there the advantage is
(2:36:11) that you will reduce the competition and it would not be required as much because that previous state would have the information of all those previous States and so on and so forth there is a sequence right but it is also very important that um the first information comes to the last as well because every hidden State processes processes it in a different way right that is a problem in recurrence it will happen with the previous time step alone okay so here if you see HX and HH are weight matrices as you all might know now and BH is bias
(2:36:50) this is nothing but tan h of w transpose X plus b okay and W transpose X plus b is the common function for any neural network to work with Okay but here what we are doing is we are adding the previous time step values also that is by wh HH h of T minus one right so that is how the forward pass will happen and if you know the forward like know that's how the model Works in uh indefinitely like like you know it will go on and on so how the output computation will happen like you know if you are doing uh inference or you know
(2:37:32) final output something like that the output YT is computed based on the current hidden State like I said you'll compute the hidden States till that time stamp and output will be computed as wh y y is a weight Matrix of your output into h of t plus b of Y okay this is as similar as this but here the hidden state of current time step which is calculated here is provided here that's all and where this output computation will happen is it will happen in dense Network okay that is why I said we are calculating the RNN cell forward pass
(2:38:12) here and the output computation will happen in the den cell okay so that is your output computation and then how the back proper will happen the back propagation will happen through time okay rnns are trained using uh BPT which is back propagation Through Time network where what it will do is it will unroll the network through time and then it will apply the back propagation because if you see we are going everywhere like you know one state before right hidden State before so it will also do the back propagation like that one state before
(2:38:46) in everywhere like you know for the previous state also back propagation will happen tracking back to the zeroth cell of zeroth State okay that is how the back propagation to time will happen and what happens is after each time step once you compute the gradients it will be summed up together okay so that is how the back propagation Through Time happens and then you have your gradient flow how the gradient flows during training the gradients uh can vanish or explode as they are propagated uh back to through time making it difficult to
(2:39:22) learn long-term dependencies like I said it will flow through time and since it is flowing through time uh if you know back propagation back propagation is kind of using differentiation okay and with differentiation always the value will become lesser so at some point of time there is a chance that you it will either vanish or if it has some weird weights you know exploding is not something which will happen commonly but if there is some weird weight initialization there is a possibility to explode the gradient as well okay so if
(2:39:59) you see as an example for RNN in the Character level language model which is to predict the next character each input XT is a character like I said one character will be XT and the network will predict the next character which is YT and the hidden State HT captures the context of previous characters for example if I say example right let's say our current hidden status or like our current sequence time stamp is y okay so e EXA all will be there in the hidden State y will be my XT which is the current character and the idea is to
(2:40:37) generate P which is the next character okay that is how RNN basically works if you're using a character level RNN when can you use rnns RNN can be generally used when you have use cases like time series prediction natural language processing tasks whatever it is okay again uh these are the State ofth art models then so if we are seeing it now yes we'll feel like okay this is boring like you know the performance is not that good but imagine that these happened in 1980s right at the time these kind of models are pretty huge
(2:41:17) these were revolutionary models then right so those were used for natural language processing tasks then and it can be used for speech recognition music generation because all of these are working on a temporal Dimension now let's come to the advantages and disadvantages of this model because to know the limits of the model it is very important right or else you can't use it for your real use cases so it is very important to know that advantages and disadvantages so what is the advantages of RNN it can process
(2:41:55) sequence of variable length like I said all it cares about is a sequence and the previous time step value will be provided to the current time step that's all right so it can process sequences of variable length it will share parameters across time steps which means it will reduce the number of parameters to learn okay and then we have the capability to capture temporal dependencies in the data temporal is nothing but time right so these are all something we know so what are the disadvantages first like I
(2:42:27) said since you are doing back propagation through time there's a problem of Vanishing or exploding gradient problems which might occur and then there is difficulty in capturing long-term dependencies because let's take ourself as an example and then we'll be able to understand what happens with RNN okay RNN expects on the fact that you should remember all the things which happens previously let's say I put you to the task of remembering what all you learned from your first grade till 12th grade okay you need to remember
(2:43:01) each and everything would you be able to remember that I can't remember maybe you can but still it is very difficult right so that is why it is very difficult to capture the long-term dependencies it is computationally expensive for long sequences all right because because you are calculating each and every time step right if I go for um a th000 token sequence you will remember all the hidden stat of all the Thousand to tokens when you are coming to the th thousandth token okay so it is very computationally expensive because it is
(2:43:38) very additive in nature in sequential rather than parallel okay why I'm stressing on that you will understand when we go to the next section okay now let's see about another model which was more revolutionized in any field which involved time over RNN which is lstm so lstm network is a network which revolutionized a lot of things which when it came because it applied a special form of RNN to capture the long-term dependencies you see if we want to remember the old things we try to forget not important things and then
(2:44:23) we'll remember those things which are very important for us alone right so that is what even lstm does it will selectively forget and remember informations over time okay let me first show you the image in detail so yeah it is a bit blurry because I have scaled it but yeah I hope you know I'll be able to explain it correctly okay so here this is how the RN network is all right and here if you see here you have an X here right this specific place is your forget gate okay and here this specific place is your memory gate
(2:45:11) and this is your output gate so it has three gating mechanisms and why is that you know why is this forget gate why is this memory gate and why is this output gate we'll see about that okay that that is for detail working explanation but this is this is how the architecture is okay so let's start with how and why the models were created so lstm if you see was addressed to in introduced to address the vanishing gradient problem like I said which was there in the traditional RNN because over time the gradients will be managed because you'll
(2:45:50) differentiate the gradients at every step the goal was to create a model that could learn and remember information or sequences which is crucial for any real world applications which is using time steps right so basically how the model Works let's see that okay an lstm cell will consist of a cell State and a three gates okay so what is the cell State this is your cell State C okay the C is your cell state and in the cell State uh along with that you have three Gs like I said which is forget gate input gate and
(2:46:25) output gate forget gate what happens in there sigmoid of weight of f time stamp uh like you know weight wait just forget F okay wait star previous hidden State comma and current state okay current value so this is nothing but you know the thing which happened in RNN okay you'll multiply these and then finally you will add the bias okay but what here happens is we'll make a sigmo okay so before going through that explanation let me show you what happens here okay let's say uh a variable is coming up
(2:47:10) here or the current time step okay and here there will be some previous time step informations okay and this is your memory Channel or cell state here what happens is once a current value appears let's say there is about 10 * step which is stored in this hidden State information which is also there in the cell state that is fine here what we'll do is in that 10 State We'll add the 11th state which is the current time step and then we'll multiply with WX transpose W transpose X plus b in that
(2:47:46) format and then we'll apply a sigmo function if you see the sigun function it's differentiation range activation function range is 0 to 1 okay so if the value is 0 to 1 what happens is we'll round it okay 0o or one if you want to forget it this information will be forgotten because here there is a multiplication it will not be added to the memory Channel which is the cell State I hope you get that there is a cell State and you are multiplying the current values here here okay so we are just basically multiplying and we saying
(2:48:24) like you know okay see uh I want to forget it or I want to remember it okay that happens in here so I've forgotten things which I need to forgot forget sorry now what I'll do is how much I need to remember you see again if I need to remember or not that will happen in the sigmoid function but how much should I remember that is something we need to check right because uh how much I should I give importance and remember uh is something even we do right like if we say like there is a very important question we'll we'll read it again and
(2:49:01) again for five times you know so that we'll remember that for a longer time so that intensity is provided by the tan layer okay so what happens here is the sigmoid will say like you know how should you remember or not that is like as same as the the forget gate but when you do the same process rather than applying sigmoid you will apply tan the tan range activation function range is minus one to one and here if it is minus one to one you will know like you know the intensity should I remember it negatively or should I remember it
(2:49:37) positively if so how long should I remember it or how much I should remember it if it is close to minus one I need to negatively remember it right like you know this is something which is an anomaly okay so let's say there is uh you know stock price there is some price going down I need to negatively remember it okay I need not I should not sell here I need to buy here something like that okay and if it is going upwards I need to remember it positively something like that okay that that is what happens with tan so these value will be
(2:50:09) multiplied so if I want to remember information it will say one and how much should I remember it will be said by tan and those will be multiplied and then will be add added to the cell State okay that is what happens in the memory okay in output gate what we'll do is we'll again see if this is something which is constituent to you know provide output that is by sigmoid and then the value which is there in the cell State because that is what is your let's say your brain memory okay this is your brain
(2:50:40) memory you have all the informations of your previous uh occurrences let's say what you'll do is again you'll apply the tan activation function and then with with this just tan activation function this is W transpose x + B on top of which tan is applied that's why it is in a box and that is orange okay that is a layer but this is just tan activation function basically whatever value is there in this cell state it will be computed with tan and those values will be multiplied okay and finally whatever
(2:51:13) you need to remember and how much you need to remember those will be remembered and that will be up app to this output and your hidden state is obtained this is how lstm works I know this is complex but you know like just remember that you are learning if you want to forget you'll forget it all right you'll just ignore those you are filtering it right like you know I'm not even going to read this so that is your forget gate so if you want to remember it uh let's say you'll have negative memory and positive memory right the
(2:51:45) tough questions you found it hard to remember you will remember negatively okay I I was struggling it struggling hard to learn this question so that kind of memory is provided by memory gate which you'll add to your brain and finally based on those knowledge you'll try to answer a question which is provided at time stamp T right so that is how it works in real time right so those knowledge you are trying to use it and you are providing the current output okay so that is how STM works lstm is was actually designed based on brain
(2:52:23) okay brain how it works on that inspiration lstm was design if you see here actually I didn't even time spend this much time on RNN lstm is very important okay so more than RNN lstms were used a lot of times in those times okay and even till Transformers games lstm were used till let's say 2018 um because 2017 in all need paper came but um it didn't pick up soon okay so once the pre-train models came that is when uh Transformers took over the market so until then lstms were the go-to model for any person who is working with NLB
(2:53:05) or time series so here like I said the forget gate is it is to you know uh you you need to forget it input gate is nothing but your memory gate okay and this will decide what new information you want to store in the cell State and then there is cell State update okay uh like I said there is star and then there is your plus and all right and those updation are cell State update and your output gate like I said it is to apply the Tage along with Sigma sigmoid sorry here you can see sigmoid layer is obtained sorry applied for time stamp T
(2:53:45) and then those values which is O of T is Multiplied with the tan on whatever cell State we have currently so this gate will control what information from the cell state is required for output okay so that is your output gate so example in a sentiment analysis task the lstm can learn to focus on key words or phrases that indicate strong sentiment while forgetting less relevant information so this is a very good example let's say I'm saying the movie was very good I liked it so good that I watched it second time okay
(2:54:19) I like it so much okay let's put it that way if it was RNN it will give equal value to all of those words okay but with lstm what it will do is it will focus on these words like very good like so much which means it is a positive sentiment right that is how even humans do right we'll not focus on all those words we'll focus on few words and lstm does that okay so when you'll use this which is like it is as similar as rnns like where RNN use cases wherever R RNN can be used lstm can be used as well so
(2:54:58) advantages is that it will capture long-term dependencies uh it can mitigate the vanishing gradient problem it has selective memory like in humans through getting mechanisms it will have robust performance across wide range of sequence lens disadvantage is that it is most more complicated than rnns and so it will require computational resour sources okay and then it is very challenging to train so like you know if you don't initialize it correctly it will not remember it and it will not correctly forget things so it is very
(2:55:31) challenged to challenging to find the hyper parameters which are uh correct to train and then it has potential for overfitting especially in smaller data sets because it is very good model it doesn't have a lot of normalization in it okay so that is about lstm so before going into the next model I would like to ask you all to hit the like button share it with your friends if you haven't subscribed to this Channel please hit the Subscribe button hit the Bell icon so that you won't miss any day video which I'm uploading right now and
(2:56:07) also these are just free for you but it means the world for me because the algorithm Works only based on your likes and comments and your shares okay okay so please help me with that and help me reach 10K Subs with that being said let's go to the next model bidirectional lstm bidirectional lstm is an extension of standard lstm that process the input sequences in both forward and backward Direction this allows the network to capture both past and future states which will provide more comprehensive understanding on the sequence so if you
(2:56:46) see in lstm we remember the past right the past sequence but let's take time serious analysis out of the equation and let's put something like sentiment analysis okay uh let's say I have a sequence like this the movie was very good but I didn't like it much okay here the it refers to you know uh the movie okay um let's get let's take another example I didn't like it much okay uh this is something uh not grammatically correct but let's take this as example I didn't like it much but the movie was
(2:57:28) very good okay if I'm doing it in the opposite here the it constituents to the movie constitutes the movie okay but if you don't know what happened in the future you won't be able to know like you know it refers to you know the movie and the movie was very good so will say positive but it is negative right because I didn't like it so it is very important that you remember the future States as well so that is why bym happens okay so by LM what happens is each state which we get will be sent to
(2:58:06) the next state which happens in the forward lstm and in the backward lstm what it happens is so those information let's say there is your first second third and and for cell and here the information is in forward and backward lstm will be in backward so here it will be zero here it will be zero so 0 1 2 3 3 to 4 and 1 to2 so the 1 to2 information is crossed between 3 to four so the information is shared okay so that is what they are trying to say in forward and backward lstm okay so why this model was created like I said there
(2:58:46) are SE uh asks in NLP which required understanding of the full context of sequence basically NLP requires full context sequence understanding okay anywhere it is doesn't matter so what is the working of bstm so BM consists of two separate lstm layer one will process the sequence left to right another one left to uh right to left like we said so forward lstm what it will do is it will be working on forward again it will be based on the time stamp - 1 but since backward lstm happens from right to left it will be working on with t + 1 okay so
(2:59:27) that is what lstm forward and backward is the output will be concatenation of the out uh information which is obtained from the lstm forward and lstm backward and based on that you will get the hidden state right so th those hidden State again will be multiplied with the dense layer which is going to be w y HT Plus by Y which will be the weight Matrix and B Vector multiplying to provide your current output at time stamp T so while training back propagation will happen through time as like in lstms and RNN but will happen simultaneously for both
(3:00:05) okay so it will be right to left for forward lstm and left to right for backward lstm the back propagation will happen like that okay so the use cases again uh any NLP use cases can be said uh as a use case for BM because it captures past and future context it improves the performance in task where there is importance of bidirectional context it reduces the ambiguity in classification tasks and also it can be combined with attention mechanisms for even better performance okay so that is a hint as you all might know of what
(3:00:47) might be coming up in the next video or next next videos so the disadvantage is that it is even more complex than un unidirectional STM because you're just replicating that again and then you are processing it in the opposite direction already lstms are very computationally intensive and just think if you're doing it twice okay and this requires the entire sequence to be available before processing so like that makes it a bit unsuitable for real-time applications if it is not real time let's say it is
(3:01:17) sentiment anal analysis or generation of something you know like that is not real time what is real time is for example you want a real time at real time you want to recognize what you are speaking speech recognition that is not something which you can do with the help of bstm so now uh it all already lstms have the potential for overfitting and you are literally making another LM on top of that which makes it more potential to overfitting and then more it is more complex to implement and fine tune okay now let's see about the next Network
(3:01:53) which is gated recurrent unit now you might see okay this looks like lstm right yes Gru is a type of recur neural network which was made as a simpler alternative to lstm because it uses a gating mechanism to control the information flow like how it happens in lstm but this is more simplified okay and why is that let's see okay so let me say simply how it works okay first uh just remember what I said in lstm right there was a forget gate here okay which will forget whatever information which is needs to be forget forgotten and it
(3:02:34) will added to the cell state right and how much it should remember will be obtained from here and it is provided to you know the cell State and then again you will have your output get here okay so that is how happens in lstm right here the forget gate and output gate is a bit combined okay so what happens is the hidden state is there which is your cell State here the cell state will provide the past information and again what you will do is you will see how much you need to forget here that is what happens here and that will be
(3:03:10) directly provided to your update gate here there is only two gate one is reset gate which is there here and there is update gate why is it here you can see it is into right so if you see like you know the current time step it doesn't use a lot of information from the previous time that is what is obtained from here it will say okay I need to forget it it is combining the memory gate output gate not the forget and output it combines the memory and output okay so here you can see it will say like you know if you want to remember or
(3:03:43) not if you don't want to remember it will reset it if you want to remember it will not reset it okay that is what happens here and if you don't want to remember the past okay that information is already provided from here okay this sigmoid and how much you want to remember that is obtained from this tan like we see in the memory gate and this will be something which we see in the output gate and those are multiplied together and are added together in the cell state which will be providing your output if you see functionally this
(3:04:15) looks like lstm right but there is one g which is reduced which is memory gate okay the forgate gate acts as the reset gate here and there is output gate already okay so that is your update gate so basically your memory gate is alone not there memory gate and output gate are combined together as update gate that is how Gru works okay so here there is a mathematical explanation and you can see that as well so this gate basically decides how much information of past is to pass along the future that is what happens in update gate and the
(3:04:53) reset gate will say like how much information I should forget the candidate hidden state which is nothing but C of T okay which is there here so those will have the memory content okay like you know which you need to update to the hidden State and the hidden State update is nothing but the plus okay so basically in text classification or uh you know any use case is where lstm can be used uh you can use Gru but make sure that it the computational efficiency is the priority not the performance because this is not
(3:05:29) going to be as good as lstms okay it is not going to be as good as lstms but it is going to be very computationally effective than lstm that is what is the advantage simpler architecture than LS because it has few parameters it is efficient uh at capturing medium to long range dependency is not very long long range faster to train and run than lstm comparable performance to lstm on many task it is not better comparable which means it is closer okay so if you want performance go for lstm if not if you want computation efficiency you can try
(3:06:05) Gru okay it is less powerful than lstm so some complex task it is not good it is less studied and understood compared to lstms lstms like I said is used widely even now though you know like language models are there even there are few people who try to use lstms nowadays uh performance uh again can vary based on specific task and data set and it can't do longterm dependency tasks as like as lstm usually AI works very well when you know the works are separated and provided here you know like the memory and output gate are combined so
(3:06:45) two works are done by one gate so it is is you know expected to not work very well as much as lstm so now we have an idea about the most famous kind of networks in RNN there are networks like uh peephole and all which came and went okay but these four are the most primary networks which are used in RNN and we have seen about these four in a brief manner okay now you have an idea basically how this model works maybe uh you can work on these things and then like you know explore on your own way for these but these kind of
(3:07:26) information is more than enough because our focus is to learn more about the language models right so these information are required because that is when you'll understand why language model came into the picture and why it is still good okay and are having more advantages over these kind of network so now that we have a theoretical idea about the RNN let's try to build a story writer with the help of RNA networks here okay so don't fear that there is around 500 lines of code here okay so these are you know just having lot of
(3:08:09) Doc strings and I have made it in that way because know like if you are going to refer the code once again and again you can watch this video again and again right so that's why the code is made in such a way that you do you do not need to you know watch the video again and again okay so I'll explain the code now and then we'll see how we can train because here we'll see how we can write the toss data set how we will write a data loader everything will be covered so let's jump into the code here you can
(3:08:40) see we have some few inputs we have torch uh which is the Deep learning library the most famous deep learning library nowadays which is used to create neural networks and so on and so forth any kind of neural networks and those neural networks are provided in this module and then because torch is a parent library and inside that there is a module which is for neuron networks and there is Optimum which is for Optimizer Optimizer is responsible for your back propagation learning rate updation and all and then you have your
(3:09:15) data uh touch. data in which you will have the all data Utilities in which we'll call the data loader and data set which is used to you know uh provide the data to the model okay so first we are writing a config class so configuration class for the text generation model which will all which will hold the hyper parameters and configuration settings for the text generation model so here there is eming dimension which is the dimension of word embeddings if you remember the previous section which was beginner prerequisits
(3:09:50) in which we said like you know uh we saw that word embeddings with increase Dimension it will have uh more better understanding about the relationship between words so if you are remembering it as TF IDF it is the Max features how much in how much features you are going to represent a word okay so that is your embedding Dimension so higher the dimension better the performance but it will require more computational resources hidden Dimension is the dimension of the Hidden state in rst R RN and lstm or Gru okay and then you
(3:10:28) have your number of layers how many layers do you want so one layer will have the cell state which we discussed in the theory how many cell States like that you want to replicate again everything more the number better the uh result but increase in time increase in size increase in competition that will be there sequence length uh it is opposite here longer the sequence length more memory more difficult to train but you know uh an appropriate training uh length is required bat size in training each batch how much sequences you you
(3:11:05) are expecting to you know uh provide so that is bad size and then you have your EPO which is the number of epo you want to train the model on so like you know how much time you want to train the model it is like you know how much time I'm writing and seeing myself or reading and seeing myself that is your B eox and learning rate at what rate I want to learn that is your learning rate so Optimizer is what will make the model to learn okay so you are sitting like you know at this rate learn and Unk token is
(3:11:42) unknown token so you'll have a vocabulary of words and if the word is not or if the token is not there in the vocabulary it will say unknown okay so here the values are provided embedding Dimension is 256 hidden Dimension is 512 num layers is 4 sequence length is 10 and bat size is 32 epox is 25 learning rate is 0.
(3:12:11) 001 and Unk token is Unk okay so in tokenized text we'll tokenize the input text into words it is to just uh split on space and we'll convert the whole text to lower space and then we'll lower case and then we'll split it okay so here if you see let's say this is the example hello world how are you it will be split as hello world everything is in small how are you okay so that is what happens with text. lower.
(3:12:36) spit now we need to create a touch data set okay so here the touch data set has two things one is length and another one is get item now we are doing a character level uh data so which means like you know every character will be uh provided so if the data has 100 tokens the sequence length is 10 the length will return 90 okay so that will show like you know how much number of sequences are there in the data set so the get item what it will do is let's say uh there is data 1 2 3 4 5 and your sequence length is three what it
(3:13:12) will do is it will split it as 1 2 3 when you call zero and it will do 34 4 5 sorry 2 3 4 if you call it as uh one and then like you know see it is just how much time stamp you want to um cover you want to put that way like you know that will be better so if I say time stamp 10 there will be uh let's say my time stamp is two here okay so if I call zero it will be 1 2 2 3 okay if I call one it will be 2 3 3 4 so that you know like that of remembrance is there and that is how you provide it here and that is why
(3:13:52) we are subtracting the sequence length from the length of data okay this is how you'll construct a touch data set for you know RNN and lstm for a character level okay or not just character level any token level now let's create the model where we'll use these attributes or parameters where we'll get the model type so what kind of model we want lstm RNN BM okay and then we have embeddings and we'll provide the RNN modules as well which will be there and then feed forward so these are there as attributes
(3:14:30) and the arguments which we'll provide is wab size like you know what is the vocabulary size that is what is going to determine your embedding along with the embedding Dimension you know like what happens is let's say my vo size is 50 and my embeding Dimension is 512 each of those vocabulary token will be represented in 512 Dimension vectors that is what will happen in n and embedding and this embedding will learn itself and update those vectors okay and the hidden Dimension like I said like what is the dimension of RNN and the
(3:15:04) number of layers and model type so based on these what we'll do is if the workup size and embedding Dimension is provided those will be instantiated with nn. embedding and if the model type is RNN we'll call RNN lstm Gru by lstm what we'll do is by directional will be set to True okay and if the B directional lstm is set to true the feed forward will become two times the hidden Dimension and then workup size so feet forward is nothing but a dense okay like I said the dense is what will aggregate the information right so
(3:15:41) it will aggregate all those hidden State and then it will multiply based on the wup size to say like you know this is the next Tok okay okay so if it is not B STM and it is like you know normal models the hidden Dimension will be the same one okay so now the forward forward pass what happens is first you'll compute the embeddings and that will be provided to your RNN so that will provide your output and hidden State okay so you'll provide this output alone to the feed forward Network and then you'll get the output but you will
(3:16:12) return the hidden State because that is what will be used in your back propagation and all okay so the initialization of hidden States the hidden States when you want to initialize it you'll initialize it as like know zeros okay so you can initialize that zeros as like it is provided here okay you want to get the model like you know these are auxilary functions helper functions to get the model we'll call the text generation model with those parameters and lot of things are provided in config class and
(3:16:47) some are provided as parameters which are used here okay work SI and model type as provided as parameters those are used here and others are provided as config from config okay when you call the train model uh what we'll do is first we'll import the loss function so for you all who don't know the loss function what is loss function loss function is the one which will say like you know how much you are deviated to the current output it is like the teacher putting you mark uh but you know like lesser the mark
(3:17:17) better the uh performance let's say your teacher only puts negative marking okay lesser the negative marking better your performance and based on those negative marking only you'll improve yourself right so that is your loss function and the optimizer is Adam Optimizer which is the most famous Optimizer so before you know like going through these I'll recommend you all to have an idea on Ann optimizers and loss functions and all okay so the optim measure is obtained and when you call the model. Trin
(3:17:50) function what happens is for Epoch in range of epo like you know number of epo you want to train yourself we'll go through each of those data and then we'll initialize the model and we'll get the hidden State okay of the model like we are initializing it right because you need something to start your time step okay that is what is the initialization of hidden State because in case of RNN or lstm or Gru it is expected that you have a previous hidden State and those previous hidden State at first time step
(3:18:23) is going to be zero okay that is what we are doing here initially the optimizer is set to zero gradient and that is provided to the model OKAY output or your like you know your input and hidden are provided and you get your output which will become your input later because that is what is there even in Hidden State okay and then you have your loss which is calculated and then you'll make your back propagation based on the LW and then you'll call the optimizer step function and then here it is just logging like you know uh the loss at
(3:18:59) time step okay so this is your training function and the inference function uh which is to generate first we'll call the model. eval function to set it to evalue mode and then tokenize text because once you get the text you need to compute it as tokens and you need to to make the input tokens IDs because there might be some tokens which will not be not be there in the vocabulary and those you need to replace by Unk tokens so that is what is happening here and then you'll convert it as pensar and you'll initialize the
(3:19:33) model hidden State and you'll start your generation this method is known as greedy decoding where what we'll do is we'll call the model like you know to generate the next token and we'll see like you know what is those token and then we'll add it to the pre uh all the previous tokens okay that is what happens here the input sequence is there and the next work next word idx is there those will be added and then will be provided to the model again this will be recurrent okay the next best token will
(3:20:04) be calculated again again again okay the next best token is obtained from here so which will say like you know across the 5,000 tokens this is most likely to be the token that is what is provided with this AR Max and that will provide your data uh text okay so while you're creating your data set first we are tokenizing the text okay and once the tokenizing is completed we are making a vocabulary based on calling the set set will basically remove your duplicates and that will create your vocabulary and then what we doing is uh we just
(3:20:40) providing index to word and word to index so that you can map it forward and backward and you know like we are having a function to add Unk token okay because it is not always possible that you'll cover every tokens which you might see in the real world for example capital A and small a are different okay so in this those cases we need a Unk token those are added here finally what we'll do is we'll convert those uh story which we tokenized as tensar okay like um tokenized IDs okay that is what will be
(3:21:18) there as data and those will be provided to the story data set and then like you know data loader will combine uh this data set and make it as a batch and then it will Shuffle it and provide it to the model okay that is what will happen with data loader then you'll call the train and save model which will you know get the model train the model and save it okay so this is basically the function which will integrate all of those other function and then load model to load the model while you're inferencing it
(3:21:48) because if there is a model which you trained already you need not train it right so we'll load the model which is text generation underscore or for example if it is lstm lstm or Gru Gru whatever it is so to inference or to run the inference uh if you see the generate text will have parameters like Model start text from where you want to generate uh word to index and index to word basically your vocabularies M mapping okay so those all are provided and finally our story is generated so what we are doing here is first we are
(3:22:22) preparing the data set the story is already here we are preparing the data set and then um the model type is here lstm we are training and saving the model the model is already there if you want you know like uh you can comment this out and then you know like you can load the model again but yeah we are doing it at single time so I've just commented it so once you're starting the information we can we can start it as once upon a time and then we'll run the inference where I need to generate 50 tokens after
(3:22:54) that okay so let's do that python 2core intermediate RL story. py so here if you remember we are training it for 25 eox okay so it will soon show here here you can see it is you know coming down so yeah here you can see once upon a time uh you know there like it is trying to say something you know like if I train it for even longer time let's say 50 box it'll be even better okay but this is how you'll make a story generator RNN in the next section of prerequisits we'll see about neural machine translation
(3:23:37) with the advanced section where we'll see how to use encoder decoder along with attention mechanism that will be our next section so yeah guys I hope you all like this video If you like this video please hit the like button share it with your friends if you haven't subscribed to the Channel please hit the Subscribe button hit the Bell icon I'll see you all in the next video Until Then happy learning previously we saw about the intermediate prerequisites in which we covered about the introduction to
(3:24:06) deep learning we saw in detail about pyo functionalities how to use pyto data set and data loader what is neural network how to create it from scratch and then we saw a detailed working explanation on forward and backward propagation we created a simple Ann Network followed by that we saw about the types of each components which are involved in deep learning training which includes activation function loss function Optimizer and Then followed by this we also saw about RNN networks in detail which includes RNN LST M Gru bym with
(3:24:50) detailed work expl working explanation pros and cons and then telling about how and why the model was created so it was a pretty detailed video last time and in this video we are going to step up the prerequisite level a bit more and then we are going to start with encoder decod Network what is an encoded decod Network an encoded decod network is a type of neural network work which is commonly used for sequence to sequence task like machine translation text summarization and question answering where there will
(3:25:25) be a requirement for you to use a context it has two main components as the name suggest it is encoder and decoder both components are typically rnns and in this explanation we'll use lstm layers as the cells for encoder and decoder layers okay encoder uh can be considered as a stack of layers and for those stack of layers we are going to use lstm layers okay encod and decod network is not uh neural network it is a neural network architecture okay please highlight the word architecture because it will use any kind
(3:26:09) of layer which you'll use so for example you can use Den layers you can use um RN layers you can use even CNN layers for example if you have a task like uh let's say image captioning then you can use CNN layers okay all kind of these things can be done with the help of encoder decoder Network now we already saw about lstm that it is a type of RNN Network which can handle long range dependencies and it will also mitigate Vanishing gradient problem a bit which was addressed as an issue of RNN now let's
(3:26:47) see as usual what why when how about encoded decod Network let's start with what is encoded decoded Network an encoded decoded network is a neural network that is designed to map an input sequence to Output sequence it consists of two main components your encoder and your decoder the encoder job is to process the input sequence and compress it into something known as cont text Vector it is a very important component or in other words we also call it as hidden state or thought Vector most commonly hidden State okay we call it as
(3:27:28) encod encoder hidden State okay now decoder will take this encode hidden state or context vector and then it will generate an output sequence so this is how it will be like you'll have your Source you'll compute embedding for your Source it will be provided to your recurrent neural network whatever you have in your encoder now what you will do is you have your targets again you'll compute an embedding for that and then this recurrent neural network information will be passed to this recurrent neuronetwork information and final
(3:28:00) output will be provided out of a fully connected layer okay so this is how basically any encoder decoder architecture works okay instead of recurrent if you want to put it as CNN and then let's sayn anything can be done okay okay but like I said in this example we are seeing it with the help of recur and neur networks because usually encoder decoder networks in any one of these components recurent neural networks will be used because if you use image people use image in encoder only when it requires tasks like image
(3:28:36) captioning so in decoder anyway you need to use recurrent neural network all right so in this case we are using lstms for both encoder and decoder that is fine why should you use encoder decoder networks in encod the encoder decoder networks are basically suited for task where the input and output length differ where order and temporal dependencies matter I think this would be the perfect explanation about why you should use an encoder decoder Network you see let's say let's take an example of translation
(3:29:11) okay the input and output length will never be same let's say I'm translating uh sequence from English to German okay can you be sure that know like if it is five words in English it will be five words in German no and the order so if I say like you know I am making a course on llms will that be the same direct word being translated in German no the order will differ because each language has its own syntax and semantics that's why I said you know in first prerequisits video I told you that syntax and semantics play an important
(3:29:55) role so here is a sample of that so the order and and the temporal dependencies also matter okay so that is where you'll use encod decod Network so some of the tasks are machine translation text summarization and then speech recognition why you have speech recognition the input which is audio frames will have different length than the output textt L lstms are used uh because they can efficiently handle sequences especially with those long-term dependencies the gating mechanisms inside lstm will help you to
(3:30:29) retain or forget information over long period which is useful for accurately translating or summarizing sequences which we know right now okay when you should use decod networks by the understanding of why use decod networks you can crack it up a bit with when you when you should use when you have variable length for input and output when the task required you to understand the context and temporal dependencies inside an input sequence and when you want the output to be generated step by step based on a previous output then
(3:31:04) also you can use encoder decoder networks okay so this is when you should use decoder encoded decoder networks now let's see how an encoded decoded Network work so let's say I'm converting it from English to French so let's say the example is they are watching full stop and end of sequence end of sequence means like you know my sequence is completed when I give it to decoder what will happen is first BOS token which is beginning of sequence token will be provided that is what is done here you
(3:31:37) know like the embedding first you'll embed your B token and that will be sent along with this information here you can see this information is passed at every State okay and that is saying like something like ILS now what you'll do is you'll together provide BOS and ILS and that is saying regardant and then uh it is saying full stop okay here itself you can see there is variable length they are watching full stop but it is ILS Regard in Full full stop okay so this itself shows you know why you should use
(3:32:11) an encoder decoder Network for translation now let's see how it works mathem ically and you know like as an intuition way for encoder decoder and all and how you'll train that okay so for an encoder the input will be a sequence of words or tokens in English so for example it is I'm learning deep learning when you are processing it each word word will be embedded each word or token will be embedded into a vector by using Vector word embeddings either you can use your word to Glow kind of pre word embeddings or if you think like you
(3:32:46) know those are not enough then you can use something like a custom nn. embedding layer that is also possible you can use that as well okay so the lstms here keep an internal memory so yeah the sequence of word embeddings again uh I missed that point the sequence of word embeddings is fed into the LST incoder which processes the sequence at one step a time sorry one time step at a time okay one time step so what it does it mean like you know when I provide the word embeddings each of these are going to be considered a
(3:33:21) Time step I am learning deep learning so what lstm encoder will do is it will process each of these one at a time and then it will keep an internal memory of the previous words by using its cell State and hidden State as we all know that it has those forgate input and output gate which will decide whether you know like what information it want to keep and what information it want to uh discard at the last time step the word learning uh the encoder will like you know when you process the encoder uh word learning the encoder outputs a
(3:33:58) context Vector which is a compressed representation of the entire input sequence okay so when you process this you'll have a representation again that is known as context vector or hidden state that is a representation of this whole sequence I am learning deep learning this context Vector contains the lstm hidden State and cell State at the time final time step so if you see if you process the entire sequence it means it has seen the entire sequence and now it has the knowledge of this entire sequence okay so that is why the
(3:34:34) final step what embedding you get out of the lstms at the last time step it will be a compressed representation of the entire output input sequence mathematically how it works is for an input sequence you'll compute the hidden State and cell State at each time step htct where you'll provide XT with the last I mean like previous hidden State and S State the final uh context Vector will be hore T and that will be provided to the decoder okay that is how mathematically it is done in decoder what happens is you'll provide a start
(3:35:09) of sequence token or beginning B token both are fine while processing uh the decoder will use another lstm Network that will generate the output sequence one word at a time now we are considering in this example that you are using Word level tokenization okay so that is why it is said as word word word the decoder is and another LST Network that generates one output uh code at a time so at each time step what you will do is you will take the previous word embedding and the context and that will be provided as input like I said here so
(3:35:46) if I say it is B it is saying is B and is will be provided together and then that will say regard that is what they are trying to say here okay it outputs a predicted word and this word is again fedback for the decoder for the next time step this will continue until the decoder generates a token known as EOS which is end of sequence token here it means that it is end of translation for each time step uh there will be an output here h- T and c-t here y of T minus one is the previous word y t minus1 is the previous
(3:36:22) word let's say if you are starting with C SOS okay and h d h- t minus1 and C- tus1 is the previous hidden State and cell state of the decoder while you train the de uh encoded decoder Network you will train it both together back propagation also will happen together by using a loss know as C entropy loss which we saw previously already the model will learn to predict the correct sequence of output given the input sequence since we use lstm the back propagation will be back back propagation through time if you see an
(3:36:57) example here I'm saying I'm learning deep learning it is Jus in I'm sorry for my French but you know there is a sample here okay in encod what happens is for each of these token there is an embedding being computed all of these are provided one by one to the lstm cells in the encoder which will provide a hidden State and cell state for each word finally context Vector is obtained after Crossing all of these words which will look something like this okay and the decoder time uh like you know step by step what happens is
(3:37:35) first time step s is provided which will predict the word J and then uh in the next time step you'll provide the J along with the context vector which is here already and it is saying s and this process will be repeated until e token is predicted this is how an encoder decoder works as a summary you can say like encoder decoder is a network which is designed for sequence based task where input and output length differ and here we have used LM to maintain long-term dependencies the encoder work is to compress input into a context
(3:38:10) vector and decoders work is to generate an output sequence word by word or token by token based on the context Vector so the main thing is this architecture is highly flexible and can be adapted to various applications like machine translation image captioning spech recognition and so on and so forth now that we know theoretically on how an encoder decoder network works let's create an encoder decoder Network for translation here I have a jupit notebook again we are not going to train it with that we'll train with the script but in
(3:38:43) encod decod network we are going to step this up a bit by adding a simple component but which has tremendous upside to it which is attention we'll see about that okay so first we are going to use sequence to sequence model that we know uh this is a model which will convert an input sequence to an output sequence of variable length and has some temporal dependencies in it so for these we'll use encoder decoder for attention we are going to use a mechanism know as Banu attention which allows the decoder to focus on different
(3:39:20) parts of the input sequence at each time step which will improve the model's ability to handle long sequences and complex relationships okay so what does this mean see I have an input sequence here I'm learning deep learning okay if I don't provide an attention mechanism each word will be provided equal importance okay but when you you add an attention it will process these sequences of different parts in different ways okay so that is what is attention mechanism so for example a lot of importance will be given to words
(3:40:03) like learning and deep learning okay because those are very important here am is not at all important I would have been more important than M like that okay so what are these attention neuron networks attention neural networks are neural networks which incorporate attention mechanisms to dynamically weigh the importance of different input elements while generating each output element for example um if I want to generate an translation right if I want to translate output of let's say deep learning I need to focus more on deep
(3:40:39) learning rather than you know I am and all okay so that will help you a lot that's why we are going to add a component known as Banu attention we know about uh you know the encoder decoder model but here we are adding an attention to it okay so let's see how the model will look like this is how an encoder is um like it sounded very big right uh I'm I'm not sure how it sounded for you it sounded pretty big for me when I was learning it for the first time but it is very simple all you need is an embedding
(3:41:17) layer and then an lstm layer and then a dropout dropout like I said is for regularization so what you will do is first you'll compute the embeddings for the tokens you'll apply Dropout for regularization you'll provide it to your lstm and then get your output hidden Andel State and that is all you do in encoder if you print your encoder um let's say my input Dimension is 10 um embedding Dimension is 512 hidden Dimension is 1024 num layers is pi and then Dropout is 0.0 let's say 0.1 okay so if this is my
(3:42:08) encoder network configuration this is how it look it will look like okay so lstm will have five layers five layers of lstm it will go through all of those okay so this is how my encoder will look like so the explanation is that it uses an embedding layer to convert input tokens into dense vectors and then it will use a multi-layer lstm based on the number of layers we provide and then the forward method Returns the following outputs which will contains the hidden state for each input token which will be useful for attention
(3:42:37) computation the final hidden state of the lstm which is your context Vector along with the cell State okay now Bano attention mechanism Bano attention mechanism is a key Innovation that allows the decoder to focus on different parts of the input sequence at each decoding step how will it look like okay how the architecture will look like when you use an attention to it that we are going to see now without V attention this is how it will look like we you'll provide the source to source to your encoder by Computing the embedding uh it
(3:43:11) will create all the record and neural network and those state will be provided to the Target that is how it works right but here with with B attention what happens is you'll get your recurrent information and that will be provided to something known as attention Okay and those information are still provided to recurrent neuron Network in the Target side also the attention will be uh the information will be added to the attention so this attention will aggregate all those information and say like you know this is where you need to
(3:43:42) focus for your next output so that is why there is a self connection here okay this is how a b attention mechanism works so B attention mechanism is nothing uh a lot of like you know complication and all it is just going to be a linear layer and then you'll use standage function because that will say that that intensity of how much you need to focus if you remember about the memory gate you'll know that tan is very good for these kind of uh situations because it says like you know where you need to like you know how much you need
(3:44:17) to focus like that right so the attention goes on like this the attention layer is nothing but a linear layer here with Dimension being hidden Dimension cross two comma hidden Dimension why the attention will be computed on both hidden States and encoder output hidden state is nothing but your context Vector so along with that you'll use your encoder outputs for your attention because hidden state is a compressed information while encoder output will have like you know the overall information about uh the encoder
(3:44:52) sequence both are almost similar okay so what happens is uh you will first take the hidden State and incoder output and format it in such a way that both of these are having the same shape that is what has happened here and what we'll do here is we'll concatenate those two together okay at Dimension two Dimension two is nothing but your embedding Dimension at embedding Dimension these will be comp uh concatenated together and then you will perform an attention mechanism after which you'll call a tan function
(3:45:28) which will say okay this is where you need to focus okay this is how the attention is and then what you will do is you'll provide it to another linear layer which is self. V which has a dimension of hidden Dimension comma 1 so it will say like no this is the token you need to process okay this is the information you need to process on that is what is hidden Dimension cross one so now what it will do is now you will apply a softx function which will classify and say like you know this is basically uh how you need to focus
(3:46:04) because it is like your flattening which will happen here and then what you will do is uh once you compute the soft Max it will say that probability for each token where you need to like you know how much you need to attend on that is what is the explanation here it calculates an energy score where you will first concatenate decod hidden state with encoder output it is decoder hidden State okay and then uh what you'll do is the concatenate Vector why it is decode hidden State here you can see this is being provided from here
(3:46:41) right it is decod hidden State this concate Vector now will be passed through a linear layer which is self attention and the T activation function that is what is here okay so what this another linear layer will do is that there will be another linear layer it will reduce this to a single score this will be across Dimension but it will reduce to a single score because you have a single sequence right this energy score again will be converted to probabilities using softmax and these weights will determine on which part you
(3:47:13) need to focus on in the input sequence so that you can generate the next output word correctly okay this is how the attention mechanism works all right now let's see about the decoder the decoder will take an embedding an lstm a final linear layer output and then you have a Dropout okay so first you'll get the input you'll compute the embeddings and then you'll apply a Dropout and then what you will do is right now for the first time step alone you'll take the hidden off minus one which is HT minus
(3:47:47) one uh and that information is your beginning of sequence token okay and then uh along with the beginning of sequence token you will provide your encoder outputs which is your context Vector together here you can see the self. attention is attention being provided and that attention is nothing but a Bano attention Okay so once you provide that you'll have your attention related information and that information will be multiplied that is what is bmm batch multip multiplication batch matrix multiplication with the encoder outputs
(3:48:26) okay in the encoder outputs you will multiply the attention uh information and that will say like you know uh this is where you need to give the importance this token is very important because the token information is there in the encar output and where you need to focus on is there in the attention related information so if you multiply both of these you are going to get a weighted information on say like you know this token you need to focus this much so that is what is obtained from here and now this becomes your RNN input and once
(3:48:58) you provide this you will get an output hidden and cell State and that will be provided to your fully connector layer finally providing a token okay so we'll also provide this hidden and still state along with the predicted token so that in the next time step it can be repeated the process can be repeated okay that is what is provided as an explanation here and in see as a whole how the architecture works is you'll first initially have everything as zeros okay as far as outputs is concerned because you need to fill there first
(3:49:36) you'll get the encoder outputs and then what you'll do is you'll go through the length you know max length you have and what you'll do is you'll provide each like you know the output and hidden State being combined to the decoder which will provide an output itself and then what you will do is you'll use a method known as teacher forcing okay teacher forcing is a method where what happens is let's say your token is like you know the task is to convert um English to tumil okay and it
(3:50:13) says like you know I love AI okay and it translates in Tamil as a so when the translation happens while training if it says like I as Nan Nan is also uh word to say about I in Tamil when you are using teacher forcing what you'll do is instead of using Nan you'll replace it with yanak okay that is what happens with teacher forcing it is you know you'll force your model to generate the correct token by providing it with uh information which is there in the ground truth the information which is there in the ground truth becomes your
(3:50:58) teacher that is what is teacher forcing so what we doing is the ratio is 0.5 so whenever there is a possibility of 0.5 what you will do is you'll replace the most recent token with the correct token at that time okay so that is what is teacher forcing and that mechanism is used here so here you can see it is usually used for training stability okay so how the training Loop will be um we'll use the multi 30k data set of uh German to English translation we you we'll first build the vocabulary for both languages and then we'll create
(3:51:37) data loaders for batching and training a model architecture is implemented uh which we already saw Here will Define the training and evaluation Loops we'll use Adam Optimizer and cross entropy laws for this we'll also Implement uh teacher forcing during training and this is a strategy for training recurrent neural networks where you'll use ground truth as an input instead of model output from a prior time step as an input so that it is more stable and learns a lot more for evaluation you'll use a method known as blue score and
(3:52:10) this blue score is very famous for evaluating machine translation task for comparing it to human translation where what it will do is it will chck for word and phrase matches so the range at which this happens is it is between 0 to one with higher being better score okay this provides functions for you to translate individual sequence individual sentences and finally we'll calculate blue score on test set okay so this is how the flow is and now let's see how that happens okay so here we have the training code
(3:52:49) I'm not going to speak about the model architecture and all okay I already trained it because it took some time okay so I already trained it will in for it though okay so here you can see we have set some loggings here seed is set so this is how the data set is constructed we'll get the source uh file and Target file which is nothing but the txt files of English sorry German and English and then what are all the Transformations you need to do on the source sentence and Target sentence which is there here okay so what we'll
(3:53:27) do here first we have an auxilary function known as load data which will load the file based on the file path okay so it will read lines which means you'll have a list of data now this self. Source data will become the length and you know will call the data at that index we transform it and then we'll provide the output okay as source and target for tokenization and all we are going to use Spacey so Spacey is a library which has a lot of NLP functionalities in it one of those is pre-train tokenizers and we are going to
(3:54:05) use that for Spacey D for you know um German tokenization and Spen for English Tok ization which is your Target and the vocabulary is DC news smm which is something you need to install Okay so first for that what you need to do is python hym Spacey download dcore news SM you need to do this along with Encore web SM okay here we have two helper functions which is tokiz D and tokenize where what happens here is with tokenized d first you'll uh normalize your token by Computing it to a lower case and then you'll tokenize it with
(3:54:52) the tokenizer appropriate to the language now here are the training PA I've already provided all of those here with the resource folder you'll have all of these uh files in your resource folder so you need not worry what we doing is we are creating instances for um the multi 30k data set for each dat data training data validation data and test data so what you need to do is just provide the file path along with the tokenization transformation path sorry uh tokenizer okay so basically what it will do is it will tokenize the sentence
(3:55:28) and provide the tokens for source and Target that is what will happen with the data set okay so now we need to create the vocabulary as well here we have the part token start of sequence token end of sequence token and Unk token this is unknown token unknown token is used used when uh there is an occurrence of a token which the model hasn't seen before that is what is unknown token so create vocabulary is used and that is nothing but you know like you'll just take set you know like if you call the set on the
(3:56:01) vocabulary uh what it will happen is uh it will give the unique words that will be a vocabulary okay um that can also be done better like this okay so now what we are going to do is we are going to tokenize all the sentences which are there in the training data so that is done here train D tokenized train and tokenized and then we have computed vocabularies for that now you have Source vocabulary Target vocabulary tokenized training data set here we have defined the model architecture which is already there and what you need to do is
(3:56:40) you have you need to provide these model hyper parameters I have set it for uh 256 embedding Dimension 512 hidden Dimension hidden Dimension is the dimension with which your feed forward network works and I've just used two layers you can use more that is your wish and encoded decoded drop out probabilities I've kept it a pretty high so that it doesn't overfit Okay so this is how the model is created now now I'm creating an Optimizer of Adam by providing the model parameters I have provided a learning rate by default uh
(3:57:17) I'm just leaving it to be default and the default learning rate is 1 E minus 3 okay 0.001 and then uh for LW I'm using cross entropy loss and what we're doing is we we'll ignore the padding index and padding index is the index of pad token okay so colet function is the function which will create the batch okay so the that is what happens in the colit function so what here we will do is we'll uh replace the tokens uh which are to be padded okay so for example my sequence length is 512 and I have only let's say
(3:58:01) 200 so the next 312 sequence uh 12 tokens will basically be padded with the pad token okay so that is what Pat sequence is Pat sequence is a function which should by default do that and that function is from utils okay so that will provide you your source and Target batch which is ready for training and in training Loop what happens is you'll set the model to training your EO Closs will be set at first as zero you'll go through the source and Target values in the data loader I've just provided tqdm so that
(3:58:36) you'll have logs and then you can monitor what is going on you'll first set Optimizer to zero gradient and then uh the model will be provided with the input it will provide an output and then what you will do is um you will take your target which is your label you'll compute loss with that you'll call the backward propagation and that will be ready for backward propagation your clip grad Norm is done so that it doesn't uh fall into exploding gradient okay it doesn't fall into exploding gradient and
(3:59:10) then uh what you will do is you'll call the optimizer step function so that the weights get updated and then we just adding those law so that we can print the LW okay in Nal what you will do is you'll go through the values again and then again this process will be same repeated but the data loader would be eval data loader rather than the train and here you won't use those Optimizer or criterian because those are not required here okay but still loss calculation those are going to be the same this is very important this is
(3:59:41) translate sentence okay translate sentence is the function which which we will use to translate the sequence which is in German to English so tokenize d uh first what it will do is it will tokenize your sentence which you provide and then it will add the start of sequence and end of sequence token okay so you are getting the indexes of those token so what you will do is uh you will create the tens are the source indexes can be said as the input IDs okay these input IDs are provided to the encoder which will provide your encoder output
(4:00:16) hidden State and sell State again as you all might know we will provide those two decoder along with the startup sequence token okay so that is what here it is okay Target wab of start of sequence token we'll take the start of sequence token we'll compute a tensor for that and that will provide the S of sequence token ID as Target tensor to start with and then you're providing a hidden State sell State and encoder output and that will provide an output hidden and sell state now what you will do is you'll see
(4:00:46) which token is the most probable token with the help of argmax and then that token will be appended to the Target indices and again this process will be repeated unless and until you see a token which is e token okay so finally we'll decode uh those tokens by calling the indices on the wab dictionary we created okay we'll ignore the SOS token and use us token and then finally your tokens are provided okay so I have commented the training Loop here I have created the data loader train data loader Val data loader and test data
(4:01:24) loader I've commented uh the training Loop because I just wanted to show you the inference training is done but still yeah you can also do training by just uncommenting these okay um right now uh loading the state di state will load the weights so I have the weights stored as best model.
(4:01:47) PT which is saved with the help of to.ve function okay you can see model. St dict is saved as best model. PT which is already available here and I'm loading that into the model now the weights are loaded which means you're like know it is like if you seen The Matrix movie they'll insert a CD which will load all the information to the brain of the user okay so that is uh that is how like load static basically works okay and now what we we doing is we are taking three samples from the test data and then we are calling the translate sentence function okay let's
(4:02:21) see how the model translates so now if you see it will just try to load this state dict into the model and then it will try to translate the sentence okay so here you can see uh there is some value here and the target is two young white males are outside near the bushes young men outside near the seats some something like that okay so this is not an accurate one again but you know like uh we can train it for a longer time that is there you can adapt the parameters probably that could be your exercise you know uh compute the blue
(4:02:56) score I'll probably make a leaderboard uh let's say we'll take one week from now okay and uh let's see who gets the best model okay let's keep that that as a challenge of neural hack with vers challenge okay so here you can see a group of men uh wearing hard hats something like that so here is men hard hats those are there a little girl climbing into a wooden playhouse a little girl is into a small of wooden playhouse so it is trying to you know like recognize entities correctly but the grammar is not that good it will
(4:03:33) learn that if you increase the layers and Hyper parameters being tuned that will happen Okay so yeah guys now you have reached the end end of the prerequisites from next video we'll be moving into the course starting on with the explanation on the paper attention is all you need I'll see you all in the next video Until Then happy learning if you have any thoughts you want to let me know please let me know in the comment section I'm very active in that please let me know your like or dislike with
(4:04:06) the help of the buttons which I provided if you haven't hit the Subscribe Channel subscribe button uh please hit the Subscribe button and yeah sh with your friends as well I'll see you all in the next video Until Then happy learning
