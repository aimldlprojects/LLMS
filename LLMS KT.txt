hello everyone we are officially jumping into the first day of our course starting with knowing about the most revolutionary architecture in the field of natural language processing in my opinion which is none other than the Transformer architecture which was introduced in the paper attention is all you need in 2017 by Google in day one we are going to get to know about this architecture a bit along with the history of the architecture and why this architecture was introduced and then the most important part of this day's video which is to understand the types of Transformers now let's start with understanding what are Transformers Transformers are a type of a deep learning model architecture like I said it has revolutionized the field of natural language processing and not just that as even revolutionized computer vision with the Advent of a variant of Transformers which is Vision Transformers and then not just that if you are going to consider speech a separate domain then yeah it has revolutionized that as well but we usually consider speech to be a part of natural language procing because it is working with natural language right at the core this revolutionary architecture Transformers was just designed to hand handle sequential data such as text by processing the entire input simultaneously rather than sequentially if you remember the issue with rnns or lstms is that with the longer sequence length it is very computationally intensive because that happen sequentially and when you do things sequentially it is one computationally intensive and two it will allow a problem which is forgetting but here with Transformers they made it possible to process it par with the help of a mechanism known as self attention a self attention is a mechanism which will allow the model to weigh the importance of different parts of the input while producing an output so we saw Banu attention right Banu attention was an attention which still was working sequentially if you remember it was just you know providing some context to each time step in a recurrence state but with Transformers it will see the whole sequence together and then provide information saying that know these parts are important since it is going to be parall that is a key feature here it is parallel processing so since you can process it parall you can handle both short and long range dependency in data very effectively so it is highly scalable and self attention mechanism is a mechanism which will allow you to focus on the different parts of the sequence so you have got your attention inside and then another important component which was introduced in Transformers architecture was positional encoding which will enable the model to understand the order of elements of the inputs in the input sequence let's say um I have a statement like I'm using an Android phone and it is going good right now but Apple seems to be a good competitor and it has let's say security features this it has same context know like same embedding if you are saying about embeddings right same embedding as a meaning right the it here the first it refers to Android mobile and then the second it refers to iOS mobile right how you understand that it is because the way I have positioned the it in the sequence I spoke to you right so that is how even model will understand that you know this it refers to this specific entity this kind of a process is known as core reference resolution which I spoke about briefly in the first prerequisite itself and that is very important you need to Resolute you know the references so that you'll be able to understand the context of your input sequence better that is very much possible with the help of Transformers though it has even more features um these are the four main features one could be that it is very flexible architecture you can add any component anywhere to suit your use case for example if you want to make it a vision Transformer replace this text encoder Vision with a Vision encoder and you're ready with the vision Transformer like that okay so that is again a key feature here now forget that name word and GPT which is there here um I need you to focus on these words encoder and decoder we already know how encoder decoder network works so this is an encoder Network and this is a decoder Network when you give an input to your input embedding person encoding will be computed and then your atten will be calculated and then based on these attention values those outputs will be provided to a feed forward Network and then that information will be shared with the decoder which again will repeat the process here as well but there is some unique components there but if you see as an overhaul of how I sh this flow this is early similar to the flow which we spoke about in the previous encoder decoder video as well if you remember yes this is going to be very close and similar to how any encoded decoder Works what makes this architecture stand out out of all those architectures is these components the multi-ad detention your mass multi-ad attention which is there here and then your personal encoding these three are the three main components which makes the Transformer architecture stand out of all those architectur which were there before Transformers so yeah now that we have a bit of idea about what is Transformers let's see why it came into the picture you see with sequential data there were RNN particularly lstm which people were using it and then they were thinking yeah it's good know like um I'm fine with that it is able to handle time series data it is able to handle text Data basically anything which has a sequence but there were some limitations the first one being sequential processing rnn's whatever Network it is regardless it processes the data sequentially which means it is slower to train on a longer sequence and then like I said you will do back propagation through time so you know like if you go from one state to another with continuous differentiation you will face an issue which is Vanishing gradient and then you you have limited context LS teams were good I mean like better but not enough up to the mark but still they struggle to remember long range context why it is sequential right if you are going to remember everything you learned from your first grade to 12th grade you're not going to remember it right if I tell you to think about it you are not going to think it sequentially you will think it like okay in my first I did this and my second I did this it's not like that you'll just remember first your alphabets and then you'll think about some words you learned and then you'll straight away go to your 12th because that is how your attention is you are not giving equal 
importance to all the sequences which are there in your past right that was a feature which is which was there in Transformers because of the parallel attention but it is not the case with lsts and then there is the issue of paralyzation difficulty you can't uh paralyze with RNN modern gpus can be used to its fullest potential with the help of paralyzation but RNN unfortunately couldn't do that so Transformers was developed to address these limitation offering a more efficient and effective way so that you can process any data which is sequential in nature now we know what is Transformers why it came into the picture but there should be history right like they should have some ancestor architectur from there it would have been derived something like that so the first one being RNN lstm and then came and came a first paper which was using attention which is Bano attention and if you haven't covered that part it is already covered in the previous prerequisite video so please watch that I've explained how B of attention works there so that was the first architecture which was using attention and these guys took an inspiration from there and then made this amazing research paper which is attention is all you need and don't worry I'm giving you an overview in this video and in the next section soon enough we'll be seeing a detailed explanation about each of these components and they're working in day two for that it needs to understand the whole context that is what it is being implied in this example then another task which requires a lot of uh understanding and basically this task is the parent task of whatever we are doing right now which is question answer answering basically asking your AI a question if you see even Char is a QA model because nowadays llms are trained with huge context length and then they are trained for longer time it has a wide range of knowledge so QA is done by decod models as well but when it came one encoders were good enough to process those context and here if you see this is how you can use it okay and if you just put the model outputs right this is how it will look and then if you do an AR Max and then you do to. decode sorry tokenizer decode you'll get your answer but here also the B Direction nature so if you see as an overall right wherever uh there has been a requirement for understanding of the context encoder can be used but decoder is probably a bit opposite if you want to put it that way decod only Transformers consist a stack of decod layers which will have M multi head self attention feed forward Network and then there is another component as well which is final head or you know like if you want to put it as soft Max layer or we generally call it as projection okay because that is what will project of which token it is next if we are seeing it as a component of decoder then yes but usually it is not seen like that okay so now what are the key gal characteristics of GPT family models or generally decod only models first is that it is auto regressive in nature unlike encoders which can see what are the tokens prior and next to a given token it is not possible with the help of decoder because with decoder will mask the upcoming tokens which is the future token so that it will be generative in nature and that is known as Auto regressive and the context it can see it is not bidirectional since we are masking the future it is going to be unidirectional context now you might wonder what is the difference between RNN and uh GPT model if you have wondered it you are in a great space of understanding natural language processing especially these Advanced architectures if you see with RNN with Banu attention which was the last part of our prerequisite we buil a translation model in which the lstm networks were used for encoders and decoders but for decoder we we provided an attention which is B attention this attention also still holds a state like any lstm would be right but this is more focusing on which token to give more importance but that value itself is said based on all the state uh time steps it has come across stored in a state right so it is not parallel in nature it is sequential in nature so with long range it will fail and also it is very computationally intensive when you go with sequential but with Transformers the attention is going to be parallel in nature not sequential which means whatever issues we usually think of with a sequential Network those are almost soled like you know everything are almost solved with the help of this parallel attention which is there in gbt family what are the models which we can see in this GPT family models GPT series which is GPT 1 2 3 and then there is four right now right so all of those along with that there is models like Ctrl which was an old model which is basically nonexistent right now and then there are other models like your llama mistal whatever llm are seeing right now there are almost like you know almost every model are decoder only models but if you remember when I'm talking about encoder itself I said encoder models are pretty good at capturing context and there is a thing we need to address of why decoder only models are preferred which we'll see in the encoder decoder section use cases what are the use cases you can do basically Bally any use case which requires generation of a token regardless of what you are generating it doesn't matter it can be done with the help of decod only model so here is an example we are taking a gpt2 model along with the tokenizer we are tokenizing we are just initializing a tokenizer and a model we are tokenizing a prom where we say like you know in a world where a become sentient basically this is a model which was trained for text completion so what it will do is it will try to complete the sentence from there now the input IDs or your basically your Transformer token inputs are calculated here by encoding this sentence and that is provided to the model and that is generating some text which is decoded with the help of tooner and here is the text in a world where becomes intient it is hard to imagine a better time to be a human we are going to say have to start thinking about how we are doing things says Dr Michael S H and so on and so forth it is trying to generate some coherent data right it is a very simple model but a very powerful and good model in its own way now why should we use decod only model since we are masking the future it is very ideal when you want to generate a text because it can generate coherent text and contextually re relevant text based on all the past sequences it 
has seen by predicting the next token and since it is auto regressive in nature it will allow it to maintain consistency and coherence in longer generated sequences now let's see about the next family of model which is none other than the encoder decoder Transformer which is not which is nothing but the attention is all you need the OG Transformer architecture architecturally as you all might know now it is an encoder decoder Transformer where it is a sequence to sequence Network consisting of both encoder and decoder Stacks the architecture includes an encoder stack a decoder stack and a cross attention mechanism which will provide information from encoder to decoder if you all remember on the RNN with Banu attention for machine translation we were constructing uh encoder decoder Network and then I explained there of how the tokens will be generated first the encoder will see the whole context and that will be used as an input along with the previous token as a context to an RNN State here the only difference is that the whole input are still provided bya a mechanism which is known as cross attention mechanism and then similar to a normal decoder only model all tokens are seen with the help of a mased uh attention you know like only previous to a current token that's all okay so it is parall what are the key characteristics it is suitable when you want to convert one sequence into another and it can handle input and output uh sequences being of different length and it combines benefits of both encoder and decoder architecture the use cases are you can use uh like you can do machine translation there so here is an example I trans ated English to German the house is wonderful and it became the house isar sorry if my German is not that good and then there is another use case where you want to summarize it here you have it okay when I gave a passage it was able to summarize it and here if you see uh the encoded decoded Transformers are well suited for summarization task because the encoder can capture the important information from a long input text and then the decoder will summarize it by generating the next word okay similar case was with uh translation first the encoder uh will capture the meaning of the input sentence and decoder will just try to say that in German okay it is like how we will uh you know translate for example uh you want to speak in let's say Tamil but you don't know Tamil a lot okay so first what you will do is you'll think a sentence in English or your mother tongue whatever it is H the bangali uh Malayalam whatever it is doesn't matter you'll think in that and then you'll try to put that sentence in Tamil or whatever language you want to put it in so basically you understand the Crux and then you construct it right that is how encoded decoded Transformer works now you might think yes now this is able to understand the context better and it can generate as well so it is the best of uh combining the best of both of these so theoretically these should have been used for llms right llm should have been built on encoder decoder Transformer right this is what this was a question even I thought and this is the question I said we'll discuss at the end once we discuss about the other architectures as well first let me conclude about these three encod only architectures are very good at understanding text and decoder only is very powerful for text generation and encoder decoder like I said will combine but there's a problem with encoder decoder which is inherently the problem of encoder okay encoder expects you to have a fixed input length you know like not fixed input length let's say a fixed Max input length so let's say I have 1024 if you if you go to 1025 encoder won't work well okay if not not work at all but with decoder it is very flexible in nature and it is highly scalable and there have been lot of research seches to improve the attention there is sliding window attention and so on and so forth which has provided a context window of around 128k tokens which is never possible with encoder because to do that kind of attention uh you can't do it with encoder okay you can move a window with encoders that is not something which is done in practice right now and that is why uh llms are done usually with decoder only models and there is only very less gap between a decoder model and an encoder decoder model with all those advancements still if you want to know like you know which is better at understanding context if you theoretically pre-train with the same amount of data and find unit with the same amount of data encoder decoder theoretically is the superior architecture but the decoder only models are very flexible in nature and it is very highly scalable in nature as well and it is very versatile because it is open-ended right you can generate any number of tokens out of it even though it is not trained for let's say 496 token you can generate 4,096 tokens out of it and that is not something which you can do with the help of encoder decoder so yeah now that is an introduction for llm this is why llms are decod of only architecture and this is not something which we usually think about you know uh why encoded decoder architectures are superior but still decod only models are used as llms this is how is the answer in my opinion but if you have any contradictions let me know your thoughts in the comment section so yeah guys that's it for this video I I hope you like this video If you like this video please hit the like button share it with your friends if you haven't subscribed to the Channel please hit the Subscribe button hit the Bell icon I hope you all uh would help me reach 10,000 subscriber but by the end of this course and yeah I'll see you on the next video Until Then happy learning welcome to day to In the Journey of mastering llms in 30 days in an NLP pipeline the first step is to process the text as an input which is suitable to the model with llms in the picture we don't do a lot of text cleaning as we discussed in the prerequisite video itself but the most important step which we do is tokenization today's agenda is to know about tokenization in detail where we'll be seeing what are the types of the tokenizers which are there and then we'll go in detail about one tokenizer which is BP tokenizer let's start with a detail introduction to the tokenization asking the questions what when why and how what is 
tokenization tokenization is the process of breaking down a text into a smaller unit which is known as token so these texts like it can be a word it can be character it can be subword or else whatever entity at you want to split a text that also can be considered as a token let's say you want to split it as two to three characters together so if you want to call it as characters you can call it that if you're calling it as subword you can call it that as well right so yeah um it is depending on what type of tokenization method you are using tokenization is a very fundamental step like I said it is a very important step in NLP and it is crucial for any NLP task so that is tokenization so if you are considering Word level tokenization right if you see all of these words in the sentence if you see as a separate entity together like for example tokenization is the process of breaking down that tokenization as simple as that now you might wonder yes this sounds very simple so why are we making an extensive day like you know a whole day for this specific concept you'll know that by the end of this video when should we use tokenization tokenization is usually done once you feel that your dra Text data is clean and then you are ready for providing a model input which will be tokenization process because the model expects numbers and you'll convert these text to numbers in the stage Vector creation which is embedding stage okay so you'll do tokenization after text cleaning but before vectorization nowadays vectorization are part of the model itself because model itself have embedding layers but you should do it once you feel like you know your data is processed in a common format this text cleaning can also be seen as text formatting or processing because it is not always possible that you will get your data in your expected format if you format that then your data is ready for training and then you'll provide it to your tokenizer why should you do tokenization first NLP models require discrete inputs which is just numbers so that it can convert into embeddings and then train itself so for that you need it Grand Rarity level that is very important because the type of tokenization controls the granularity level in which the model works for example we made a character level uh story generation RNN possibly that could have been better if we trained it at Word level right so yeah that is provided in granularity control vocabulary management vocabulary management it is like the model's knowledge of words you know the collection of words that the model knows that is vocabulary this tokenizer will help you to create and manage vocabulary for language models let's say if I ask you a question of how many words you know you will list down a number of words and I'm sure the number of words you'll list down will be more than what an AI can list down because you know it as direct words but if you increase that memory of your model it will face an issue like you for example if I ask you about you know like talking in a language you will think of you know like what is the next word what is the next word like is this the appropriate word right so what like this vocabulary helps you with is it will provide a limited vocabulary for you to begin with and then based on that vocabulary it will have the capability to construct a sentence in such a way that that vocabulary is not that big for example uh if you know 50,000 words you will know only 50,000 words but llms if it knows 50,000 words it can construct 5 lakh words out of it something like that so that is vocabulary management and we'll be creating those vocabularies with the help of tokenizers and next is feature extraction like I said llms take tokens as input and those tokens are what is used to compute as embeddings and then provide it as input so that processes feature extraction which will take tokens as input dimensionality reduction by taking uh breaking down text into tokens you can just uh reduce the dimensionality of your input now how will you do to ionization first make sure that your text is normalized normalized as in not just you know like simple normalization like this text to lower case removing punctuation handling special characters these are old okay but these are the convention but nowadays you'll train it for some custom use cases make sure that your text is normalized for that use case okay so normalized here also means that it follows a common format boundary detection you want to identify where your token starts and ends simple things can be like uh simplifying it as an example could be here am I splitting at you know Space level white space the quick brown fox that is white Space level you can do it at character level okay this at character level here and then what you can do it as like you know for example there is punctuation you can split it as punctuation or you can write your own common reix rules as well which will be very complex in nature but it is upon you okay so make sure that you define your boundary detection next is to extract those token where what you will do is you'll separate that tokens from the original text okay just take those token which matches your rules and then you'll process those token uh for example uh subw tokenization is a tokenization which what it will do is it will be additive kind of tokenization okay so here is an example the quick brown uh Fox jump space Sorry hash s it means there is something which needs to be joined together with so it is jump and then yes together okay jumps over the lazy dog like I said for you jump and jumps are two words right in your vocabulary but for llm it is just one word along with a suffix okay that is why I said llms are efficient than us in managing our vocabularies so why do you need this tokenization we already see that but as a significance it will provide language understanding it will manage your vocabulary like I said multilingual applications for that you can use tokenization methods like BP because that will ensure that there is no unknown token in it you'll handle rare words you know like if there is even a rare word for example the longest word in the world super Cali frile something like that there is a word like that and then there is a disease name uh which is ironically uh lung related disease if I'm not wrong and then that word is pretty huge okay if you want to represent that word as well with the help of LMS you can do that because llm will work at subword level okay this subword level will break down that word in uh subw units and then it will work from there okay it will uh like proper tokenization will give better performance uh it will make sure that uh the text is consistent the tokens is consistent at a consistent level basically it is very efficient and then it will help you in feature engineering okay these are the wise like you know in a different aspect as a significance now that we have an idea about tokenization let's see about what are the token iers which are available first character tokenizer a character tokenizer as the name suggest will break down character into sorry text into 






individual character so these characters will also include spaces punctation Mark everything will be a separate token so here you can see Hello World h e l l o comma space world and then exclamatory Mark the simple advantages is it is very simple you will never face out of vocabulary out of vocabul is something like you know it is an unknown word for the LM but unlike you know humans it can't learn on the go right so it will always be unknown for it so if you are not facing that issue it is a very big plus point in your tokenization method um these are useful for tasks which involves character level analysis but the disadvantage is that first you will you lose the word level semantics each word will have its own advantage and disant right considering hello as h e l l o and as hello there is a difference right so that word level semantics is lost here and then uh now if I split it as hello world the sequence has two tokens but right now it has 5 six 7 well 13 tokens which means it is going to be computationally expensive and yeah so since it is like this it will not capture the language structure and all okay but still if you want to do this all you need to do is just call the list of function and it will be done okay so that is character level tokenizer that my kernel connect unless until then uh yeah here you can see now what is Word level if it is splited as words then it is Word level and here what we'll do is we usually uh split it as spaces or or else if you think like for example I want sentence level or like you know at some punctuations you can also do that but most common is uh White Space level so here you can see the quick brown everything is separated here together so you preserve Word level semantics it is easier to interpret now see if I give this right would you prefer this kind of a uh text input or this kind of a text input I think with longer sequence everyone will say like this right so it is very easier to interpret and it will work very well for more many NLP task but the main problem comes into the picture which is large vocabulary size because each word should be there if it is not there you will have out of vocabulary words so if you are going to manage that you will increase your vocabulary so if you increase your vocabulary then it is going to be computationally expensive and your model won't be able to uh predict the next word very well so if there is a compound word or unconventional spellings like like I said the world's largest uh token right then your llm will struggle there even if it is there in the vocabulary right because that is a very big token if you want to do one like you know Word level tokenization just call word tokenize from NLT key tokenization and then you'll get your output okay so this is Word level tokenization like I said Word level can be anything but whites space is just splitting it at Whit space okay and here if you see the punctuations are not considered a separate token but here punctuation are considered a separate token or separate word here you can see dot is separate right so that is a very important difference between Word level tokenization common word level tokenization and um your Whit space tokenization so this Whit space tokenization won't handle punctuation well because it is together maintained and then here it is simple fast again whatever advantages are set for Word level it will apply to whites space and whatever disadvantages I said to work level all of those will be like you know applied for white space um so yeah that is white space tokenization now coming on to the important type of tokenization which is performed nowadays and in that important type we'll be seeing about one type in very detail which is byad encoding okay this falls under a to to iation method which is known as subw tokenization sub tokenization is a method as the name suggest to break words also into meaningful units so that you'll be balancing the vocabulary size also the semantic representation it is like getting the best out of both of character level and Word level it is like between that character level and Word level where if a word is it in in its root form right it will obviously there be be there in the vocabulary it should also be a very common word okay but for example if it is playing play playing plays played if you are considering all of these to be a uh separate word that is a problem for example uh let's take walk walks walking walked let's take these three and then a plays played playing did you see uh common pattern here it is play as a common word which has play as SB suffix played as suffix PD playing ing suffix this is common in English right sub word works on that what it will do is it will try to split the word itself into smaller words making sure that the semantic representation and vocabulary size both are balanced together sub organization uh in this approach you will handle work out of vocabulary words mostly you will never face out of vocabulary words when you use methods like bite pair encoding or uni in word piece uh there is a chance you might go into um Unk tokens okay but we'll see about these four tokenization methods right now the first one being bite par encoding so the bite PA encoding is a method which starts with a vocabulary of initial characters which is individual characters and what we'll do is we'll go in an additive approach where we will merge the most frequent adjacent pair of character characters um don't worry about the sentence if it is very complicated for you I'll explain it very simply let's say you have an initially vo initial vocabulary of individual characters okay what you will do is you'll count the frequency of character pairs in the Corpus for example here it is hug 10 times pug five times pun 12 times uh one four times hugs five times okay so these are uh the words and the base vocabulary is b g h n PSU after one merge how it will happen is if you see ug is together ug here it's together a lot of times U is also together a lot of times and apparently if you see hug itself is 10 times and then there is a suffix here so even if I'm considering at first level right UG and un will be a token which is combined together okay here you can see that UG and un instead of separating as H ug U as three tokens you'll split it as H ug two tokens what you will do is you will repeat these two steps counting the frequency of character which we 
did now and then merging the most uh frequent pair and then adding to vocabulary again and again until your vocabulary size is reached desired vocabulary size whatever issues we have addressed as main issues of tokenizer everything will be solved here balance between vocabulary size and token uh semantics and then uh handling rad words out of vocabulary words subw semantics is captured here works very well with multilingual models but there are disadvantages as well sometimes the split wouldn't make any sense for some words so it requires a training for corpers but you know like the training would not be uh something like you know you'll expect that long and all it is a very small training and then uh at some point of time uh it will not capture the morphological structures effectively so there is a variant of this tokenizer which is bite level BP where instead of working at these token level it will be working on its bite level okay so this is how that works actually so you can see this is a separate token here is a separate token uh sentence was this is a sample sentence okay here you can see this is a three all of these three are you know common words okay like you can say it as stop words and all but sample if you see is separated as s a m for some reason uh is there in the vocabulary for some reason I'm not sure and sent and ns are together NS is a suffix a very common suffix so that is there and then full stop is also there here you can see like you know it is not splitted at character level it is not splitted as Word level right so yeah that is BP tokenizer for you all and B level BP right now okay next we have word piece word piece is similar to VP but instead of choosing the most frequent pair it will try to identify you know like which token has the maximum likelihood to be there in the training data okay so this will be as similar as VP uh but you know it is very computationally expensive because like you know it should see the training data a lot and then like you know it will say like this has a very high chance for occurring but with this there is a possibility for you to get Unk tokens okay so the advantages and disadvantages are here again the process is going to be similar instead of seeing like you know uh the frequency in the vocabulary sorry yeah in your vocabulary initial vocabulary from there you'll see it as like you know how likely it is there to be there in the training data here is a code to create a word piece tokenizer which is similar to the code which is there for B P encoding but I'll explain this a bit okay why I'm not explaining BP tokenizer because we are going to create that tokenizer from scratch and also based on hugging face at the later end so here you'll first call the tokenizer saying that the type of tokenizer is word piece with my Unk token being unk unk is unknown token token okay we are creating a trainer because this needs to be trained on a corpus initial Corpus and for that you need to provide your workup size and the work is 2,000 here special tokens are Unk unknown token CLS classification token separation token P token M token and all what is the pre- tokenization level like I said you want to split it at some level and then work from there right so for that we are going to work at whes Space level now once you call create the tokenizer you'll train it by calling the train function and tokenize it uh by just encoding and then providing the tokens okay let's see how word piece does the same task you can see this is a this is what I like in word piece if you ask me personally I'm a very big fan of uh word piece tokenization more than b l BP but the only thing is the likelihood of um an unknown token being occurring is higher with word piece tokenization but still it can be handled by uh providing all the character possibility character set when it comes to multilingual languages and when it comes to multilingual languages it will increase the computation if you are fine with that with English I think uh I'll always go forward piece tokenization any day over uh BP that is my personal thing because here you can see this is uh that is fine yes it is saying a is a part of the previous word MP is a part of the previous word L is part of the previous word which means L will be M now and then M will become M ample let's say put it a sample and then ample also says Hash Hash it will become sample okay then again the same thing persists sent and ends right so this is why I like word piece tokenization a lot but it can be computationally very expensive than BP that is an issue with word piece but yes uh I like word piece a lot what piece is used in uh B IFI and even I think in some few llms which are coming right now I think word piece is used but most common llms which we see use by PA encoding and that to bite level B PA encoding not normal bite pa okay so as far as the unigram tokenization it will start with a large vocabulary and then it will remove tokens to reach the vocabulary till now it was additive right so you'll try to merge but here it is you'll Define a bigger vocabulary and then you'll remove it so first you'll create a large vocabulary you'll have a loss function but I mean we are not going to Define it hugging space would have done that and you will run this tokenizer on your current vocabulary which is constructed over a corpus you provided and the trainer will be there from the hugging pH now what you will do is for each sybol you will calculate a loss and if it is like you know whichever like for example each token is there a symbol okay for each symbol you'll calculate a loss if it is removed okay so like you know how much loss it is increased we'll remove the percentage of symbol to attain the exact vocabulary size you want finding the lowest loss increase so here it will open up uh lot of possibilities when it comes to multiple tokenization levels because this is a format right so it will be uh very robust in nature it is very optimal uh to find the vocabulary because you are working down okay from above you're coming below which means your vocabulary is good but you are removing the tokens which has very less possibility to be there in the real world okay and the disadvantage is that it is very complex to implement compared to BP and word piece and also training will be harder but for us nothing okay same code nothing is changed okay we are providing the same coppers here and if I provide it it is going to say t as a separate one H has a seate One S separate one is like you know this is doing it it in its own way but yeah unigram is also a very good method okay and then finally 
sentence Fe sentence Fe is not a tokenization algorithm by itself it is a framework where you can use BP or unigram so what it will do is it will have input as a RW stream where space is also considered to be a part of the token set treat uh the input text as a raw stream of characters characters means one one one okay including spaces you'll apply BP or uni uh unigram there and then what you will do is you will learn the vocabulary that includes a space separated token so here you can see hello and then space like you know and then world it is two things when you do it in this kind of a coding method right this is logic you can reverse it easily and then it will be very consistent um you are like you know setting up boundaries so it will be that also will be very well but at times it will not be as good as the other methods because you are doing a higher level on top of something and that is always not good okay that is why you know sentence p is not used a lot okay so now we saw about all these methods and if you're going to use for a general purpose model GPD is good if you want to work it with encoder models basically when morphology is the most important thing right like you need to understand better encoding is very important then you go for word piece and other two methods like you know it is basically if you want multilingual go for sentence speech and unigram is working on top of anything like you know it is just to handle ambiguity but we are going to focus as the example I've already provided here it is to focus on BP so I have created a document here for BP let's see about how that works first what is bite encoding originally first bad encoding was a data compression method where the most frequent bytes will be replaced with a single B okay in NLP this has been adapted to merge the most frequent characters uh here instead of bites it will merge the characters now you will know that like you know how B level BP happens actually pre tokenization what it will do is it will split the word by spaces and once it is done it will add a special symbol underscore to say like you know this is the boundary of the word and here the boundary is space for example uh low low low lower lower like no newest newest widest widest here it is becoming low underscore lower underscore newest underscore widest code because these four four are the unique tokens which are there in this text and the occurrence time is given here okay now you have the list of words and the frequencies and of which that words happens or else you want to call it as tokens that is also pretty good so the number of tokens are already uh number of times the token has appeared is also there then you are ready to implement BP first you'll create a base vocabulary this is how you'll construct a vocabulary uh with tokenizer because tokenizer is focused on creating a vocabulary because while inference what you will do is you'll try to see the word which is there in the vocabulary and then it will you'll split from there okay that is how you'll do so vocabulary construction is all about uh what a tokenizer does so first you will uh take the base vocabulary which is character level l o w e r n e s wi okay so all of these are there and then underscore now what we'll do is we'll represent these words in base vocabulary lore wcor l w eore and so on and so forth the third step is merging the most frequence PA first is creating a best vocabulary which is say characters and then unique characters all the unique characters and then representing these words as characters and then merging the most frequent pair which occurs in these characters for example you can see newest widest here it will happen at like you know level NR level if you want to see two characters repeating together it is byr level if it is three level it is trigram level okay so yeah like you know first if you start it as B Bagram level it is going to be es s together and it will be combined because it occurs six times in newest here you can see newest is six time widest is three times so es has occurred nine times now these words will become n e w sore T if you see es and T itself it's occurring nine times so it will become n w EST EST here EST and then underscore are there together you can also do it as tore but tore is not something we see commonly is what is the Assumption here okay this is just an example working of how BP basically Works behind while it is trying to construct the vocabulary okay so it is becoming n w eStore y w ID eStore here in low lower what will happen is if you see yellow will be merging at as low and then if you see W is coming up together here here our assumption is that the vocabulary is this much only okay that's why very it is very important that you have a wide range of vocabulary while you're training um your tokenizer okay so low will be a separate token already and uh and then lower also okay but you will not merge low and underscore here because it has occurred only one time and in lower it is not there so what you'll do is you'll stop this merging process this is repetition process okay you will stop that when you feel that the vocabulary size you want is achieved okay here are the merge rules and here now this will now this is your vocabulary okay this is your vocabulary and if I I give a new text what you'll do is first put it as words add an underscore and then break it into characters which is there already here for example the words are newest binded and lowest okay so before going here I would like you all to just work it out like you know you have seen the rules here already based on those rules try to toonize this I hope you would have done that yes now we'll see how it happens here first we'll combine this es in newest so that will be uh es and T like you know es first we'll convert e and s to es es and T to EST and then eStore tore so it will be eStore NE W eStore that is our final token when it comes to bind it we don't have any rules which is applicable here so it will be b i n d e d but when it comes to lawyers we already have a rule l o being low and then L and W being L here you can see everything is Byram level okay so L ERS so if any token which is not there in the VAB it will be replaced with Unk token and that is why what people do is when they work with B level BP or BP they'll just provide the character set of any language they work with Okay so so that at least at character level uh that will be tokenized and there will not be any Unk there okay so the tokenization result is already here you can see uh find it as Unk because none of these values are 
already there in our vocabulary Yes except n so it will become Unk okay because that whole word should be represented somehow but there are some things for example I is not there b is not there um but yeah n and d d is there D I think D is also not there yeah D is also not there n and E only is there so you can't represent that token so it will become a newk token okay so this is how basically bite bar encoding works now how will bite level BP work instead of working at character or Word level it will operate at bite level what it will do is it will convert the words or tokens to Raw bites as like a numerical representation of characters in the computer similar to your asky coding and this allows to handle all the unic code characters like if you see Char and all right it can generate emojis very easily because these work at Uni code character level okay it works at Uni code character level only the B level VP ensures that any character can be encoded regardless of what kind of encoding it is like utf8 so so this will be very useful if you use for multilingual languages especially like which has unique characters like Tamil Chinese Arabic Hindi and so on and so forth where you see a possibility for lot of unseen symbols coming up your way with lot of H cases there bite level BP will be very useful why does GPT use bite level BP GPT models will work with vast languages which will have emojis like you know World level people are using it you won't know what kind of symbol or lot what kind of token is going to be coming your way but when you work it at bite level that will allow you to um you know have the tokenization process almost being lossless which means like you knowk token is not there and you are tokenizing it correctly which means it is lossless because you are not in losing any information how this bite level BP works first you'll convert the text into a bite sequence uh in it utf8 encoding this is under the impression that like I said you can use any encoding this is under the uh idea that BP occurs at UTF encoding utf8 encoding okay a is 97 p is 112 P again 112 L 108 so e is 101 so it will be like this okay so you will apply the same processes uh like you applied for bite level for this B sorry for normal BP to white Lev BP as well and here you can see it is pcore PP where it is saying like you know it is occurred to times here so that is why the merge is there 970 PP 10811 okay if you do it as tokenization what it will do is uh it will be saying like you know app and then L because the occurrence of a together with PP is very high and it will also handle uh characters very much because instead of char uh like Emoji you'll be providing this utf8 number so that is very easy for the model right so the final tokenization will be uh bite values will then be like you know again decoded first what you'll do is you'll break down the text into bite sequences and then you will merge the frequent bytes to generate the token that is how B level BPA work just you know uh consider that instead of um working at character or token level you'll work at at B level that's all is b b Lev BP but the benefits is like I said like you know Universal coverage efficient representation like you know you will you can represent any number of words with that and you will not lose any information okay this is how uh GPD model works for example G PT high and 3 okay let's consider that and then along with the Emoji here you can see uh everything is encoded here so what it will do is after you know number of merging together GPT and amazing will become a single token okay and then rocket emoji will be another token East will be another token okay that is how this work if you remember or if you don't know like I'm just saying GPT models use an encoder which is Tik token encoder and Tik token encoder is what uh like you know Tik token encoder use this kind of tokenization method as its base behind okay so yeah that is how B level BP works I hope you understood how B level BP works because if you using a tokenizer it is very important that you understand it so that you'll know your models flaws okay models important flaws can be understood with the help of tokenization if you tokenize it well you will get very good results with your llms okay now let's see how you can create that kind of a tokenizer from scratch BP token from scratch I'll just walk you through these codes okay uh I have provided it with detailed doc string so you'll be able to understand how things works by yourself as well okay so first U from BP tokenizer scratch I'm creating the BP tokenizer and then OS is there I'm providing my Corpus file path and then tokenizer like you know how I want to save it like you know where do I want to save it so the training token what it will do is it will take the Corpus it will initialize the instance of tokenizer it will train it and save it at run time you'll create an instance of the tokenizer you'll load your tokenization file you'll encode it okay this is all there in this uh like you know train file the important file is this one creating the tokenizer okay so for that first you need to see what what is the base tokenizer okay so here these are functions which we will override right now it will be not implemented okay train encode decode and all but you can build a vocabulary okay the bite level tokens it is 0 to 255 okay so what you will do is you'll just provide 1 is to 2 is to 3 is to that is a dictionary of integers which are mapped to each bite okay that will be a vocabulary which is being constructed here for each bite you will create a single bite token and for each merge token those will be together as one token if you're working as like you know Bagram it will be too much token we are working at Bagram here and that two token will have one number in the vocabulary okay so that is building a vocabulary function and how that will do is it is assume that you have merges ready okay self. merges it will be an empty dictionary you need to merge that right once your merge is done it is just creating empty dictionary from and to you can map it your map your bytes basically okay so how will you save it uh you can just open and save your dictionary as anything like you want okay you can save it as a normal file or you can just add an extension on as wab and then save all of these but make sure that you decode your bytes because if you are providing it as bites no one is able to understand that right no one is going to be understanding that so just write it as bites okay so these are all just you know like writing a format in 
which you can see it and understand it okay so this is to load your vocabulary okay so what you will do is you'll go through each of those lines and you'll split it as like you know uh Space level because you can see it is splitted as Space level ID and token okay so that is how you work here you'll read your merges and and then based on those Mees you will build your vocabulary okay I'm just giving you a walkth through I'm not going in detail because we already saw in detail working previously these are just you know basic level things in BP tokenizer scratch what we are going to do is in code decode first we are going to see about the train function train function okay which takes three four three four three arguments yes text where it is the input text you want to train on basically your Corpus VAB size like what number of vocabulary size you want veros uh do you want to see debugging and all okay uh written this will written nothing instead what it will do is it will change the merges and VAB attributes self do merges and self. VAB okay so how it will do it will convert the input string into bite level integers and then it will iteratively merge together the consecutive bite pair until the workup size is reached and each time a pair is merged a new token is created and then it will be assigned the next available ID because your 0 to 256 already is done okay from 257 you'll be add adding it that's why I said it is additive okay you need to merge in such a way that for example if it is 300 it is going to be 44 44 uh new tokens are to be created so that is what we are doing here we are making sure sure that our vocabulary is good greater than 256 okay and the number of merges you need to do is just subtracting that workup size from uh 256 okay first what we'll do is we'll get the byes of text and then we'll consider it like you know bite sequence is first obtained here and that bite sequence is separated at like you know character level with the help of list function okay so what you will do now is for the number of level of merges first we'll use use get stats function uh these are all there in the utils okay so let me open the utils file here you can see get stats what it will do is for example you have IDs 1 2 312 it will generate like you know pairs like 1 2 2 3 3 1 1 2 and then it will find the occurrence like you know 1 2 has occurred twice here and two three occurs once three one occurs once okay that kind of thing for example your collection. counter you have seen in uh python right that kind of function is what get stats does and that stats you if you get a Max right which is the most frequent pair that is what is the pair here that will be added to the merges and then added to the vocabulary as well okay you'll get the ID of those and then it will be added okay so if it is veros we'll print the merge as well and now the self. merges is updated and this process will be repeated for the number of merges okay given a new sentence let's say your tokenizer restrain how you will compute the tokens at inference timers you'll again calculate the bytes uh do it at a list as a list and what you will do is uh you'll make sure that it is greater than two because for you to perform even one merge you need two tokens because we working at Bagram level right you'll get the stats and then you'll see like you know uh the best P to merge which has the lowest merge index okay that is how you can go additive like you know uh first to for example EST right first you need to find if there is es together and then es and T that is why uh it is first Min and then combination okay so if there is no valid merges you'll just break the loop and then say like you know these are the characters and character level you are going to work but if it is there you'll just merge and then return return the IDS decoding is just you know like referring the vocabulary okay so this is how BP tokenizer is created I know I've just run through it um but it is very simple if you remember all of those Concepts I said you can understand this and I have an exercise for you all because what I need you all to do is and you all to try to understand this code and and also try to understand the concepts which I have discussed today and then see if you can create a vocabulary by yourself with the help of VP tokenizer which you created by yourself okay let's see if someone is able to do that you can refer this but uh there are some flaws in this code which I know and I kept it uh in on purpose I want you all to Cas multiple scenarios I I I'll will just give some conditions okay there should not be An Occurrence for unknown token okay uh let's say I will work with five languages okay um let's say I'll take English German French Hindi Tamil okay let's say these are my five languages and this is a challenge okay you need to create a BP tokenizer B lbb tokenizer where any token I give in these five languages you should be able to do that um I'll soon create a Google form for submission which will be provided in the link here in the description the link will be provide in the description you can you can make a submission there and whoever does it best yes uh we'll see like you know uh we'll probably have some price for them probably a book for them okay that is something which I can do so yeah um but if you are like you know uh going to do this efficiently you are not going to train by yourself okay rather what you are going to do is you're going to use hugging face okay you'll call The pre- tokenizer Bite level okay and then the tokenizer is also BP first you will take your training Corpus load data set will load a data set from hugging pH we are just writing it as line by level line level with like you know we'll go through each row write it as uh write it in the file and then add a slashing token to add the next line in the next uh like you know next row in the next line basically we are taking just the Thousand samples here and then we are trying to construct uh tokenizer of VAB size 25,000 and by the way the VAB size I want you all to work with is 10,000 okay so here the special token is end of text okay that is something which is very common it is to say like you know your sequence is ended here that is eos token so first what we'll do is we we'll initialize the BP trainer here with the wab size and the special tokens and they will train the tokenizer which is already initialized here with the trainer provided already on Wiki text which we constructed here you can see the wiki text okay it is a pretty big Corpus but I don't want you all to like you know just follow this alone probably you can build it on something else okay probably we can make it as a two-part challenge I'll let you know the two-part challenge at the end of this video okay so now uh we'll train it from the iterator as as well uh like it is just you know using a data loader okay so you'll save the tokenizer by calling the save function you'll provide a save path for example it is BP tokenizer GPT here which is already saved inside BP tokenizer HF I manually moved it there okay if you want to load just provide that path as load path and then if you call the tokenizer class from with the tokenizer file you'll be able to load it okay so here you can see this is how the tokenizer file will be these are my tokens okay if you see I provided 25,000 there and there will be 25,000 25,000 tokens here as well 24,999 right so when you want to encode text just call encode and when you want to decode text just call decode with encode it will be text and decoded will be token IDs and if you want to save it as hugging face tokenizer right just Norm not normal tokenizer just as Hing Hing face tokenizer just say uh beginning of sequence and end of sequence maybe you can say like you know end of text as beginning of text as well okay it is upon your wish just provide B token and EOS token EOS token providing a sent of text is is itself more than enough just wrap your tokenizer with that and then save as prain tokenizer that is what will create this one okay here have B token and Usos token are your special tokens this is your tokenization config here you can see uh your B token us token cleanup tokenization spaces is true I didn't set up a model Maxin so it is like Infinity basically and then uh the tokenizer class is pre Train tokenizer F okay and that is the this is the vocabulary you can see it will be under the name wab see here wab right so and also here if you see this is the character set I was saying about first you will compute the character set and then you will move on to other things so right now for example this is about German and all and then you'll combine it okay so this G is about like you know saying there is something previous to it now if you want to run this class you need to call the Run pipeline okay what it will do is it will first prepare the training copper train the tokenizer save it load it encode this text and then decode it and then you'll save it with Transformers okay let's run both of those we'll train our tokenizer from scratch as well as uh we'll do it with hugging fish I hope my terminal is visible for you all first I'll do it with hugging phase okay yeah so now it is running it is starting the tokenization pipeline okay here it is starting the tokenization pipeline now it will prepare the training Corpus and then do on all of those here it is tokenizing the words it is counting the pairs it has computed the merges like you know it is doing it again and again and here when I said let's test this tokenizer it became L colon or apostrophe yes apostrophe yes test this PO and iser okay this is to say there was a space before that that is why G is there before T alone okay so yeah if I had said while inference that clean up tokenization spaces is true it would have said like you know uh let's this would have been mapped like this but yeah it is working okay so here if you see BP toer HF is there and then jbt is uh save separately okay so now we'll try to train it from scratch as well VP tokenizer scratch train.py um I think it is going to be like this yeah you can see it happened at bite level and yeah you can see the VAB all right so there is something here which we are not able to see but if you want to see the merges here you can see this is how it became merged 28 and 99 became 296 token okay first you initialize till 256 255 0 to 25 and then 32 and 116 token merge to became 26 like that okay this is how it works at the behind the scenes this is just for your understanding but the better way would be to use hugging face so yeah guys this is a very extensive video I made on tokenization that to BP I hope you all like this video I hope you all find this informator I would just like to request you all to just hit the like button share it with your friends if you haven't just subscribed to the Channel please hit the Subscribe button we are on an llm Learning Journey I'll see you all in the next video Until then let me know your thoughts in the comment section as well happy learning welcome to day three in your 30 days llm Mastery course in day one we saw an introduction to the Transformer architecture and today we are going to go in depth and understand the mathematics behind each of those components which are present in this revolutionary architecture attention is all un uh which is specified in attention is all un need paper and the architecture is none other than the Transformer architecture as you can see this resource consist of everything it consists of the mathematics it consists of diagrams which explain the working it consists of illustrative examples and many more so if you're looking to learn about Transformers you are in the right place let's start understanding with the architecture on an overall level where this is a encoder decoder architecture as we all know the encoder family models are known as bird family models and decoder models are known as GPT family models but this model Transformer architecture is on an overall level known as an encoded decod model and that is usually known as a t or Bart family model and this model is the parent model for all the llms and small slms or any language model we are seeing right now for everything this architecture is the parent architecture so you can say that in this 30- days course today is the most important day why is that whatever llm you're seeing right now almost every llm is using this side of the architecture alone which means it is very important then for first you understand this architecture so that you'll be able to understand every other thing which is there in the market right now so first this Transformer architecture is an encoded decoder architecture as you all know right now encoder work is to process and input sequence continue uh it till the end of the sequence and then at the end of the sequence you will generate an embedding which will be provided to the decoder and decoder will use that embedding to generate an output sequence this is how an encoded decoder works so why you should allow a decoder and encoder together the encoder decoder structure will allow you to handle sequence to sequence task when there is a task that involves an input to be a sequence of tokens or words and then there is an also there is also an output with a sequence of words but on both sides you don't know how much you know like length they would be they are of variable length right so in that case you'll be using encoder decoder and here since you're using encoder it will retain the context information send it to the decoder and the decoder will generate the output sequences accordingly based on what task it is because translation and summarization are some of the examples of sequence to sequence task and both of those require you to understand the context better also whatever task you are seeing right now in the llm world even if it is a simple classification or the most complex forms like Vision language model tasks audio tasks or even like in llm you have chatbot real time chatbot everything is going to be sequence to sequence because you're giving a sequence of input and the model is generating a sequence of output right so whatever task which is there in this world can be done with encoder decoder but we prefer to use decoder because the issues already said in the previous videos but in short enodo has this problem of not able to be like you know extensible with its context length and nowadays large context length is very important and that is why we are moving towards EOD only model and then trying to get better performance even without an encoder because architecturally encoded decoder is superior to decoder one because it has this multi-head attention since it has a multi-head self attention that has doesn't have a mask to it that is why it is able to see the whole context attend the whole context together and then make an representation which can be provided to the decoder as information so here in encoder the encoder is not just a single layer like you see here this is one layer as you can see here it has written NX right so it is the number of stack and if it is N1 it is the first stack the encoder consist of a stack of six layers which has two main components multi-head self attention and a Feit forward block these two are the main components which are available in this encoder and then there is a auxilary component which is add a n along with Dropout to maintain a high level performance and training stability here on the right if you're confused with this kind of a structure no problem will be seeing this in detail because this is how a multi-head self attention works this multihead self attention is shown in detail here the decoder also has six layers But it includes an additional uh multi-ad cross ATT ention sub layer so that it can incorporate the encoder's output so this is your cross attention the encoder information is given from here right so that is your cross attention and there is another difference as well here it is multi-head attention and here it is Mas multi attention don't worry about these key wordss you will understand those by the end of the day but similar to encoder it also has six layer stack with those changes which we specified since we have a good idea about the architecture on an overall level let's kick it up a notch and then go through each of those components separately understand what is that component why should we use that component how is the component working we'll see it mathematically and then we'll also try to see it um you know with some illustrations if possible also with some examples so that you get the best understanding about each component and the first component to start with is token embeddings already we have seen about tokenization so if you haven't seen that video please check it out tokenization is the process of converting uh input sequence which is a number of words into tokens and that is the input for the Transformers right so still a Transformer can't process raw text so the tokens must must be computed as an embedding in a continuous Vector space continuous Vector space is also known as latent space or embedding space however you want to call it it is a space where an entity is represented with some Vector okay if you you can put it in a general way here it is tokens represented in vectors if it is image it will be image patches represented in vectors it depends on the use case right so still why do you you want to do this because this will provide you a better semantic representation it will give you meaning if you remember the embedding discussion that embeddings shown how king and queen are close to each other like boy and girl right because contextually it is you know closer to each other but what is a token embedding a token embedding is an embedding where each input is rep each input token is represented as a dense Vector of fixed size what does this even mean let's say the token is input right for this input I'll have a vector which is Vector is nothing but an array of numbers and the length of that array we call it as Dimension okay and that Dimension here is fixed you know like once I say it is 512 or 1024 it is like that and here it is 512 all right but here the one thing is these tokens have these representations right these representations will get updated while training all right that is why we train embedding layers so that better representations are obtained for each token for example now my sentence for me to embed is I love NLP right so if I embed it it will become 0.21 minus 0.34 these are random numbers all right 0.56 and similarly it will go for 512 Dimensions so each of the tokens which are there in the sequence now will have a 512 Dimension Vector right and these will be passed to the model now the bat size will become one okay because there is one sentence here comma three because there are three tokens here and for each of those tokens you have a 512 Dimension array right so it will become one cross 3 cross 51 okay basically it is like make this an array and instead of each of those token have this array of numbers right that is how an embedding is computed and this will be passed to the next layer which is attention but there is a catch if you see here these are all discrete tokens right like it one is not related to another for example I doesn't know like love and love doesn't know NLP like know there is no context sharing between these so to share you know the position of each word to the model we have an embeding which is known as portion embedding there are various types of portion embeddings which are there right now but we are going to focus only on cidal because that is the question and coding which is used in the atom ch un need paper but there is a separate day for discussion of portion embedding in which I'll explain about almost any portion embedding method which is there in the market and why know that type of method is followed what are the pros cons we'll see everything but right now let's see about cidal portion encoding and before going into cidal first you need to know why portion encoding right so let me give an example I ate an Apple at Apple Incorporation but I didn't like the experience there right so here I ate an apple that apple is constituting to a fruit and then I'm saying at Apple and here at states that apple is a location or a company right how would you deduce that apple is an incorporation or a company or a location because of the position of the word being near to a preposition at right so if it not had been like near at you wouldn't have produced it to be an organization and also I said I didn't like the experience there right so here there constitutes to the second apple right if I said it would have said about the first Apple but still you know you need to remember the positions right because if I said it right the Apple both of the Apple uh basically has the same context know like if I compute as embeddings both will give me the same embeddings but here it means only the fruit apple and not the second Apple so to give positional awareness to the model we have po encoding and that is something which was lacked in the previous architecture which is RNN why do you need this potion coding I think uh we all would know right now because self attention will compute uh relationships without any regard for the sequence order here let's say it is position encodings so even if the encodings was here and position was here the attention will compute uh in the same way so while we are providing itself if we provide some positional information right it will help the model to understand where a word is positioned in the because with attention it is just multiplication of each token against another token don't worry we'll go about attention in detail later now how will you calculate this sinusoidal portional encoding okay so s and coine functions are used with different wavelength wavelength um is determined by your embedding dimension for example I have a sequence here which is I love NLP with a length of three tokens and let's assume here that are D Model D model is nothing but the embedding Dimension let's say it's four right so if I plot a wave for this specific sequence let's say there is a DOT here as well okay let's say the length is 4 this is how it would look right and here if you notice I equal to 0 IAL 2 I equal 4 is there where is 1A 3 right those are available in cosine Okay so so here for cidal you'll sorry for even you'll use a sign function and for odd you'll use cosine function but that is not important what is important is here if you see the location of each position is different at each embedding Index this is your embedding index and this is your position okay and here you can see the position of each are different right here you can see the length is three tokens here let's say NLP Dot and NLP dot is a single token okay that is our assumption here and here you can see right at zero at zero embedding Dimension right if you see it at embedding Dimension level like how the models is Zer is here one is here two is here right so it has some different meaning to it because the numbers are going to be different but at two the numbers are having a different uh kind of structure right even if I plot of structure it is like this and here it is like this and here it is like this right so it will see different possibility of tokens okay each token will have different representation with respect to other tokens at different embedding Dimension right so that the model will understand what is the position how much each token influences the other token that is how AOS encoding works I hope you'll be able to understand it so now let's say uh you know let's go for the formula for even Dimensions let's keep it that here the person encoding will will be computed for a given position for 2 I right why it is 2 I because whatever embedding Dimension you have regardless it is 5224 you're Computing numbers for even and odd and for odd you are going to use cos uh cosine and for sign sorry even you're using sign right so the number of positions is splitted into two which means you can't compute number of I right so you'll split it and that is why there is 2 I okay so you'll start from zero and then you'll go to two and then you'll go to four and then you'll go to six right that is how it works and if that is the case with um you know odd it will go from one and then it will go to 3 5 let's say it is 512 it will end up at 52 and 511 in the in here right that is how it works imagine I had 512 already right so it will be uh 0 2 and then finally it will end up with 20,24 and we don't have 1024 position embedding right so that is why we have it as 2 I all right and you can't use I obviously because even numbers are not allowed here that is why we have it as 2 I so the formula is sign of your position number position is nothing but where is the position of your token for example love is one NLP is two okay something like that divided by 10,000 for your frequency like you know 10,000 is your frequency term for 2 I by D model 2 I is nothing but for example let's say it is five sorry 6 so it will become 6 by 512 some number will be calculated and then that will be the power of 10,000 and that will be divided for six and then you'll have the position value six and then sorry position value two I guess right and then here for six sorry if I'm just confusing okay let's give a simple example rather for my first token so the position becomes zero I need to calculate the value of the portion embedding at index zero right so it is going to be 0 comma 0 so it will become 0 by 10,000 0 by 1 so it will become 0 by 1 which becomes 0 right so yeah so what if I want to calculate across embedding Dimension one and then my potion is zero so it will become 0 sorry 1 by 10,000 power zero so become sign of 1 by one and there will be a value to it so similarly you'll calculate it for coine as well okay and here is a sample table for that so yeah the table uh values are a bit wrong okay um and why I said that to be wrong is because okay if you see it is approximated all right that is very much important here because let's say your dimension is 512 okay imagine dividing 1 by 512 it will be something like 0. something like that right and that is going to be very close to zero so it is like almost I'm dividing it like you know by zero so it is going to be 2 I by D model will be almost close to zero always with initial dimensions that's what I'm trying to say right with initial Dimensions if if you do it for 52 Dimensions yes like you know that will be very uh bigger but here it is wrong because know we consider to be four so you can calculate this also to be four but right now I'm assuming that you know there is 512 Dimension but I'm considering it to be four something like that okay that is why this is wrong but this is how it is work okay so it will be like 0 2 I which means like for example that it's 2 okay it's like 2 into 2 by 52 4X 512 it is like 1 by 100 and something like that so it is going to be 0. some number right so if you do all those calculation and if you do it right for example 1 by 10,000 power Z it is going to be 0.841 5 similarly you'll get numbers for all okay these numbers will be different not the same because here everything is approximated to zero right but you won't get like that so for each token you'll have different value use okay that's why I said wrong the calculation is not accurate because that's why I provided an approximate symbol here right so please have that in mind let's consider you know like this is our positional encoding right now what will happen if you see in this architecture this is personal encoding and this input embedding is your token embedding there is an addition here what is that and why is that you see the token embedding has all the semantic information do it but to provide a positional context we'll add this embedding with the token embedding so let's say our token embedding right now for the zero token is 0.2 0.1 0.3 0.4 now we'll add it with this number 0 1 0 1 so it will become 0.2 1.1 0.3 2.4 similarly you will do it for token 2 token 3 and then you will do it for in tokens you have right so finally the combined embedding will have both semantic context and portional context to it which you will provide as input to your model that is portion embedding okay now let's go into the Crux the real Crux of the paper which is attention here there are various levels of attentions being used with this scale do product attention being the core for all of those so before going into that first let's attention understand what is attention right so here you you see how much is fruit and fruit related it is exact right so the most attention will be given there fruits are how much it is related fruits delicious how much it is related delicious and fruit will have a same like this is a confusion Matrix that is what we call them right sorry heat map this is a heat map and this heat map shows how much these are you know um attending to each other how much each word impact the other okay this is how it works but how this works mathematically we'll see that so basically this says r word right how much is R important for fruits how much is R important for R how much is R important for delicious okay how much is fruits important for R everything like you know every token will see how much it is important for the other tokens that is why I said in a previous itself it it doesn't matter what order you give it in for attention because it is just going to compute attention for everything so that is why we are giving a personal context as well so what is the scale. product attention it is a core mechanism where attention weights are computed by taking the dot product between queries and keys okay so why do you need that it will allows the model to focus on relevant parts of the input by Computing similarity between the tokens this is self-explanatory right but not this one you see first why should you use that let's say you have 1,24 token sequence right you can't remember all the24 sequence you'll when you even read right you'll remember some parts of what you read or let's say for exam you will remember some parts of what you read and based on that you will put your own language and words to it and then you'll try to fill it up from there but you remember the key wordss because you give more attention to it similarly the model also will give attention to the most important words and for that it will have vectors like queries ke and all okay so here the query key values there are three vectors which will see in detail on how it works later once we know about scale. product attention or rather let's first see what is s key value because this is going to be the Crux of the scale do product attention when I say it scale do product attention is the Crux of Transformer qkv are the Crux of scale. product attention all right now let's see in detail about that so what are the query key value like I said these are the fundamental components of attention mechanism and that will allow you and your model sorry your model rather to focus on relevant parts of the input where query is a vector which will represent what you're looking for and carry key will represent like is it available if it is available where and then value will represent you know like the content itself which you want to retrieve how this will work together for while at run time right when you are Computing attention score what you'll do is you'll multiply the query with key then it will say like you know if it is available or not if available where and then you will multiply let's say you get the location that location uh will be given more importance and by that you'll multiply it with the value so that you'll get the value with of the location in which your relevant information is there that will get the highest value right when you multiply with this kind of a number here because that is going to create it a sum right so once that is done you will have you know like uh your t Target which is which is the most attentive thing which is the answer you want right so that is how it is done so what you will do is first you'll create it so given an input what you will do is you'll create three matrices for you to learn which is 2 KV and what you'll do is your input embeddings which you get you'll multiply those and that weight matrices of qkv are your learnable matrices right so that will become your qk actually and then you'll multiply it with the attention formula which we'll see later okay but theoretically we can compare this query key value with a library catalog and an attention mechanism let's say you're implementing a library catalog system right how you'll do a user will submit a query for books so what the system you create will do is in this query it will see for book categories like what are the category of book you're looking for that will become your keys and then it will try to find the book which are closely matched to those SK for example if I'm seeing about sports right so anything which falls under Sports can be seen or vice versa if I'm interested in a single sport I might look into other sports as well so finding the appropriate key and then finding the books that are closest to those query right even in sports let's say in sports I want to see about Cricket right so I want the books in category that are closer to my query that is what this does okay so let's say uh I'm asking a book of cricket analytics right so what it will do is it will go to sports category and from there it will try to see which book is closest so it will say like you know is there a book which is related to Cricket and Cricket analytics here I'll try to give more importance to that so it will try to find the location of the book I'm looking for and what it will do is it will once it finds the book it will retrieve all the information of the book including the book itself and that is presented to the user because the user found what they want right that is how an attention also work the model will process the input data as query vectors what you want that is represented in query that is compared against key now attention is computed like to say like which is the relevant one for you between the query and key and that importance uh will be your weight attention weight and that will be multiplied with value which will focus on the most relevant information this is the best way you can understand query key value as well as attention right so these are calculations which we are going to see but qkv M are matrices which contain the vectors like I said and here DK uh if you see is the dimension of the key Vector okay usually this DK d V uh will be the same Dimension as the embedding Dimension okay and also this mechanism will allow your model to basically dynamically focus on different parts of the input because that is aim of attention right now that you know what is quy key value will go to attention Okay when you give an input right will multiply it with three weight matrices weight Q weight K and weight V and that will become your qkv okay and that is multiply in a matrix multiplication and then you're doing some scaling here and then uh you are doing masking if required because there will be padding you can't compute for padding token so if there is a mask you can do masking if not just leave it and then you will do a soft Max and then there is another matrix multiplication of query key with value right this is the most self-explanation image which can be given for this equation here you can see first query is Multiplied with key key transpose okay and that will give you a vector again and then you are uh like you know scaling it basically with the root of DK DK is nothing but the dimensionality of key or else you can also say it as the dimensionality of embedding okay both can be said once you scale that you will apply soft Max and then you'll multiply it with v okay this is how it happens in the model itself okay so let's say our example because now uh we are like you know having some kind of a idea about qkv and all uh and just to say uh here the scaling is to stabilize the gradient to keep it in a smaller Dimension or smaller number and the softmax is to normalize these weights so that it will focus on relevant token because the numbers won't be like you know small numbers it would be like five 100 200 and all so the calculation will be harder so softmax what it will do it will bring everything to zero to one okay that is the work of softmax activation function now let's consider this example our query is 0.1 0.4 key is 0.2 0.5 value is 1.2 0.9 if I do um like you know let's say this is 2x2 matrix okay so it is 0.3 0.7 these are all there first I need to do qkv qkt right query key transpose if I do this I'm getting a value of 0.28 0.22 0.54 0.42 okay now I need to re scale it right now my uh root of DK is going to be root of two right because I'm having only two Dimension 1 two okay because it is a 2x2 matrix right so it is going to be root of two on this okay root of two is 1. 732 if I'm not wrong uh sorry 1.414 I think 1. 732 is for root of three um so it is like I'm dividing all of these numbers with 1.73 to and there is an arbitary number which is obtained and now what you'll do is you'll apply softmax on this and here you can see it has become 0.537 0.521 0.479 and 0.463 right so again uh these are all some arbitrary numbers okay the calculation might be wrong or might be right right this is just to show how it works okay um and here if you see we are now multiplying this is your attention weight okay this is how your attention is the most important token right now is in the first okay so now what you'll do is you'll multiply these with your value Vector 1.2 0.9 0.8 0.1 which is giving a value of 1.27 1.016 1.1 0.993 okay this is how it works and why do you need this for example if you are translating I love cats to another language the model ATT uh loves and cats and when generating the word corresponds to love okay so first it will see about love cats and all but while generating it will uh generate a word which is corresponding to love we already saw about these and all but this attention will focus on one entity right so it's like we are focusing on one way to solve a problem you need to focus on different way right so what they brought is multi head self attention instead of using a single attention mechanism you'll split the attention into multiple attention heads where you are focusing on different aspects at different level okay so that is multi-ad attention basically one attention is uh split into multiple attention heads we call it as heads okay so what it will do it will allow to learn multiple relationships because each attention will work differently for example if I'm saying to understand an image one might understand about the background one might understand about the entities which are present in the image one might understand about the colors which are there in the Mage and so on and so forth right so that is the idea of using multihead self attention that is why you need to atten use multi-ad self attention because it will capture richer interactions between token and learn multiple represent uh relationships so basically what you do is once you compute your um query key value right you'll have it as splitted as you know um let's say it is eight here okay you'll have eight par uh attention heads for query for key for value and then eight parall attention will be there for all of those values being concatenated together and sent to a linear layer this linear layer is of known as projection layer and why do you need that it is because this projection layer will concatenate all the information you learn together for example this focused on one thing this focused on one thing this focused on one thing like attention it each attention might have focused on one thing but you need to concate and comprise all of this information together right it's like you know let's take myself as an example I might have learned from different sources right I might have learned from uh videos from courses from blogs their research paper there are different sources which I gave attention to okay but finally I'm concatenating all those information together and here I am the linear layer because I'm delivering an output based on all of those Source information that is the best way you can understand MTI detention okay guys the best way in my opinion to understand Transformers or any llm is to put yourself in the shoes of llm because it is actually working in your shoes Okay so for each input sentence you'll have multiple attention heads okay you'll compute all of those you'll concatenate and then you'll multiply it with weight zero okay it will linearly transform and concate or comprise all of this information together but in decoder I said there is a Twist right there is multi-ad cross attention because you need to get the relevant information from the encoder now let me give you a question I want you all to answer in uh answer you know try to answer it just pause the video here which is giving you the context okay that is the hint right now I'm going to alter this qkv in such a way that information is obtained from the encoder and what you want all right that Target is obtained from decoder so what are the matrices I'm going to use as information from encoder and what are the matrices I'm going to use from decoder just try to find it out I'll just give you a minute pause the video and then do it if you're done awesome let's check if you right here we are going to use encoder skis and values and decoder query because if you see the question for example even if I'm doing question answering right all the context are there in encoder the best way to understand cross attention is to compare it with QA okay because it is almost like QA right even qkb is like a question answering mechanism so it is best to compare that for example I've given a context like artificial intelligence was found in this year and so on and so forth nowadays there are lots of AI which is going good but all of those started with ch being a revolutionary model but to all of those the parent Revolution was from attention all you need now if I ask a question what is the parent Revolution architecture right so you'll go token wise that is there and now you have a context stating like you know you want to find what is the parent architecture where is the context you already encoded those context which I said with the help of encoder right so the location of the cont text and the context itself is available in encoder so what you will do is for the question you computed you'll have the encoder context as your key and value okay so now why do we need this um basically it will help the decoder to focus on the relevant Parts which are there in input tokens which will improve any task basically right now we are using translation so it will improve the translation quality okay how basically it is different from self attention because the query comes from decoder and key and value from the encoder let's see and let's see a step-by-step example okay uh we are working with two HS okay qu comes from the decoder with the shape of 2 comma 3 this is one head and this is the other head okay here the sequence length is two and the hidden size is three okay this is not head this is your sequence length Okay this is your sequence length this this is token one and this is token two and these are the three embedding this is the three embed Dimension Vector for query okay and then there is for key and value and this is obtained from the encoder okay these two are obtained from the encoder and this is from decoder right so the first step is like I said you'll multiply it with the weight query weight key and weight value Matrix right what you need to do is to compute the query you'll compute it with query cross you'll transpose this okay basically you'll transpose this and you'll get a value similarly you'll get key vector and value Vector now the weight vectors of query key values are multiplied with the embeddings and now you have query key value vectors okay now we need to compute qk qk transpose and then you'll do uh by root of two and then there's a soft Max and then uh you'll multiply it with the attention weights okay with V1 finally giving you a value okay this is from your one attention head right I already said you we are working with two attention heads okay this is from one attention head at head two let's say you repeat the same process and you get an output like this 0.7 to 1.01 0.65 1.12 what you will do is you'll concatenate it one after another so it will become 0.80 0.14 sorry 1.04 0.79 1.09 after that there is 0.72 1.01 uh 0.65 and then there is 1.12 okay now what we'll do is we'll have a linear layer which has a weight Matrix in it because that is how it works right so once you pass this through a linear layer it will you know combine those information because here if you see the mention again it's 2 cross2 it became 4 cross2 but it became 2 cross2 again right now this has the information across heads each head might have focused on different things and all of those are stored in this okay that is how a multi-head cross attention works and for multi-ad self attention here again the heads might have produced different numbers and still the conation will be the same the only difference between multi head cell and multi cross is that key and value here comes from encoder query from uh decoder but there everything is from encoder okay now if you see this architecture we are covered with multi-ad attention multi-ad cross attention and then we saw about embeddings and all but here there is another attention right M multi-ad attention why is that you see decoder is generator it needs to generate the next token right which means if it it sees the next token it is cheating basically so what we'll do is we'll mask the next token at time stamp T for example at time St T the time stamp t plus one token is must while even while training okay so that the future information is not available and it will use only the past information this is what is known as a caal mechanism because there is a cost in the past you know like the past tokens are the only ones which are influencing the next token and this is also known as Auto regressive nature okay so in the decoder uh to ensure that the model attends only to the previous position we'll use MK multi redention by masking the next token why should we use that to prevent cheating right so how will you do that you will will apply a mask to the attention weights where the corresponding to the Future tokens whatever tokens are there it will be set to minus infinity because when you apply softmax to minus infinity those tokens will receive zero attention because soft Max of minus infinity is going to be zero soft Max is like you know let's say there are 10 values okay different values to the max it will be like one okay to the minimum it will be zero that is how it is what is the probability of a value being the next value you know like a value a token being the next token that is where we are using softmax right it is like scaling everything between Z and one 1 minus infinity is the least value right if you see it so it will become zero okay so now we are considering a sequence length of four and embedding dimension of four as example so it will become 4 cross 4 the decoder query is uh 4 cross 4 and there is a vector here similarly there is K and V Let's consider right now that K is equal to Q equal to V okay so we'll also assume you know the projection weights are same once you multiply the weight vector right let's assume all of those are same and this is the value okay now you need to compute since the values are same again you are going to get the same uh vectors for all of these those vectors are here okay when we are Computing do product attention you you're going to multiply this Vector with the transpose of this Vector okay basically this would have been like this um should have been like this my mistake have been like this okay um and here you are getting a value right so now what we'll do is since our Dimension is four we'll divide it by root of 4 which means we are dividing it by root of two okay we'll divide all of those that is there okay it will be 0.5 0.4 0.35 0.2 0.25 but here we'll apply the Cal mask right here you can see this is a lower triangle mask where the upper triangle is minus infinity now okay so once you apply the mask now the attention score is going to be like 0.4 5 0.4 0.3 like no there are some random values okay but you know these are like I said these are just to explain right so let's consider this to be our attention score right now and if I compute softmax for this is going to give me a weight something like this still if you see these are zero which means the model doesn't have any knowledge of these tokens okay and we'll multiply these with the value okay for example how we will do is one will be multiplied with b0 0 will be multiplied with B Das of 1 0 will be multiplied with v- of two something like that and that is giving me a vector like this okay so similarly I'll do it for token one and token two token 3 finally giving me an output like this okay again we'll do it for multiple head and each head will give an output we'll concate all of those and then we'll multiply it by zero but the important difference here is this masking and since I'm masking it I'll be giving values only for this because softmax of this became zero right so now I hope you all understood about the attention mechanisms which are there in the Transformers is all you need architecture let's go with the next component which is feed forward so what is a feed forward block is it very complex no it is very simple actually right so it is just a two layer fully connected neural network fully connected neural networks are nothing but Anns which are applied independently to each position in the sequence okay so what it will do it will help the model to basically understand the nonlinear uh relationships which are there and then model will be able to concatenate all the information which were learned from the attention right this is a block so why this feed forward can be used you know to learn information why is that first I'll say okay uh maybe you can guess guess it right now but I'll say Okay previously here you concatenated it okay regardless of what attention it is it will always follow uh spe forward block why is that you see you have concatenated with a linear layer which means from a big level of information it is concatenated and comprised in a small information okay it is like you know I'm learning a research paper but just the summary of it okay but to understand I need to learn logs or something like that right I need to put some effort there there is a large effort required that is where P forward comes into the picture because there is a small amount of information here and that is expanded in a larger amount so for example usually this will be four times the size okay so if it is 5 well it will be 248 the number once you Dimension increases the number number of information or the information representation will be better and bigger there and then what you will do is in that larger Dimension you will learn all the information you require to learn okay let's say I'm uh let's take a human learning curve right first what the human does is the human or let's say take me okay I'm learning through videos okay but videos are very limited right so what I'm doing is that information is there that is my basic information after that I'm projecting that information into a larger information with the help of added resources you know like blogs research papers um whatever articles I find it over internet and so on and so forth all of those information are now obtained you know I have a big amount of information which is relevant to what I want and I'm comprising all of this information into this one video again right so this video was there I have an initial content okay but I'm expressing those content in a bigger way with the help of you know some other means here it is a linear layer okay so one linear layer will try to expand the information which was already there so that it can learn from those patterns and the other linear one layer once it it learns all the information it will comprise it to the original Dimension itself that is how a feed forward block works and that is why you need Fe forward block okay so it consists of two linear transformation with a Rel activation function being there here the formula is W transpose X plus b so you need two weight vectors and then two for two inputs 0.5 - 0.3 has these weight vectors and 0- 0.2 0.8 has 0.1 0.7 - 0.4 0.9 as weight vectors okay bias Vector we are just uh considering something simple okay so this is how basically an input would go okay if we had it at three dimension right so how it would be that is how this process is constructed okay so here let's assume our weight Vector here is 0.5 0.4 0.3 0.2 minus 0.1 0.7 okay instead of this just ignore that okay let's consider this to be our weight Vector now and the bias is 0.1 0.1 and here our input is 0.5 - 0.3 0.1 minus 0.2 0.8 and 0.3 this is our input okay if you multiply this with this and then add it with 0.1 0.1 you get a value like this okay now we'll apply reu to it as you all know reu has a function like 0 comma Max and that gives you this output once you get this output you'll multiply that output with the weight two matx right now it is a value like this it should have been uh two okay uh for some reason here it's three here okay so now you'll multiply it with two and then you get an another value finally you have no feed forward just simple W transpose X plus b okay that is how it works in layer normalization what is layer normalization and why it is required that to a reduce residual Network because we are not just normalizing it we are adding something so this skip layers was first introduced in rest net okay residual Network what it does is what was the reason it came was there was this issue where the training goes on and the loss increased in case of resonate okay because with training either your performance should go down or else you should be the same level but it was not happening there but there is also another issue of Vanishing gradient okay there might be an issue of Vanishing gradient because you are having that much layer that much differentiation will be there so to avoid that to normalize it we have ADD and layer NM and some layers might have in know negative impact that is why we have ADD okay so let's say you have a sub layer here multi multi tension that output what you will do is you will transform it like this X Plus sub layer of X what it will do is basically let's go to the architecture first you'll have your input here right you have a multi- attention output here what this add and nor will do is it will add the input which was there already with the attention calculated output so that at least the same level performance is there and also the normalization will allow you to you know avoid Vanishing gradient and exploding gradient issue and all that is the idea of ADD and layer n and it'll also improve the convergence because you know you're not going in a negative way while you're training it all right so the residual connection uh how it works is once you provide the input to your sublayer that input is kept aside the output of the sub layer is also kept at aside both of those are added once that is done you will apply layer normalization to it layer normalization um there are different ways okay and one of the most common ways batch Norm okay in those in that batch you'll do normalization across the batch okay Z is your input mu is the mean of the input of the batch and standard deviation of the batch and then there is a scaling factor and beta okay basically this is a standardization formula okay basically it will um provide a distribution of - 3 to 3 that is what layer normalization does okay so if I give it as 0.2 + 0 2 4 1 3 it will become 37 and if I do mean it will be giving me 3 + 7 which is equal to 5 and variance calculation is 3 - 5 because x - mu the whole s uh by two that is what it is and then we are doing for 7 as well uh 7 - 5 by 2 the whole Square so it will be four here and variance root is standard deviation right because we see we need standard deviation mean both of those are calculated now okay now to normalize the input 37 minus 5 okay basically we minus 5 from this Matrix which is which is going to become Min - 22 by 2 so it is like 1 by 2 of - 2 2 it is -1 1 okay so let's say a scaling Vector is there like you know I don't want to scale at all scale and shift is not there it is not an issue if there is something like that it is like 0.1 so it will become minus 0.11 let's say here it is uh 1 so it will become minus 1.1 1.1 something like that okay so from 37 here it became one1 right and the input was 24 so that is how add and lay n works okay now you have an idea about all the components which was there um I would also like to apologize for some of the values being wrong here um I'm extremely sorry for that but yes here this was to show you like you know how it works mathematically hopefully you found it useful if you all found it useful um I'm happy with that if not I'm sorry I'll also try to improve myself over time but now you have an idea about all the components which are there so let's see how training happens basically your input tokens first will be embedded and then your potion encoding also will be done so potion encoded uh input along with embedding will be combined together and then you'll pass it to self attention which will be add and layer normed and then provided to the next layer um like it will go through the stack right and then you'll pass through the feed forward and your encoder or output will be ready here in decoder the same process will start where the output of past along with this encod output will be uh attended and then you will generate token sequentially right this is just like a summary how it works uh the training objective is to minimize the difference between the predicted token and uh true output for this we'll be using cross entropy loss okay so similarly it works in inference process but here we already have the label right so in label also we'll uh we'll not do anything we'll just mask it and we'll try to teacher Force at some point of time but it inference you know like it will generate one by one but here we'll provide a mask and it will happen parall with decoder as well that is a difference but here there is one thing you need to know which is decoding strategy like I said know decoder will generate one by one token right how it is doing that it is doing it that with the help of decoding strategy a decoder strategy is a method where a sequence generation model like a language model will be using it to generate output tokens based on the probability distribution it predicted for tokens at each time step okay so let's say at time stamp T I have uh n tokens n is nothing but your vocabulary size which is my next token all right so for that I have a probity distribution which the softmax would have given me that is nothing but the LM head and based on that there are different ways to calculate the next token with uh the most popular Being Greedy decoding where it will see like you know what is the most probable next token with beam search what it will see is it will see multiple possible sequences okay so it is not that you know apple fruit must not be you know like the next word it could have been incorporation right to see that multiple possibilities you can use beam search but it is very exhaustive but still it will give the better performance always uh rather than the gritty decoding okay topk sampling uh it is like you know we'll first select top K tokens and out of those we'll take the next token okay rather than going through all the tokens we'll sample and then uh select the highest probability one out of those So based on probability also we'll cut it that is what is sampling sampling will basically reduce your time okay um but in the paper uh they would have used grey decoding and we are going to focus only on that where the idea is that you will see the next uh most probable token at each time step until the sequence is completed how the sequence completion will be uh noticed it will be with the help of the end of sequence token so that is a simple explanation to gy decoding now we all know it uh takes at each time step so which means it doesn't uh consider the long term consequences of that choice let's say uh the Apple if I say as fruit right always it is going to talk something related to food but if it had seen a possibility that there is a possibility for incorporation it would have also seen a tech possibility okay so that is a problem with greedy decoding but if you are willing to take that yes greedy decoding is very good because it is fast it is it is a deterministic approach so you know like it will always see uh the next most probable token which means it will always generate the same output which is not the case with the others right so why do you need to need to use gry decoding first speed it is very computationally efficient because you are just seeing the next token alone it is very simple because you're not taking multiple Parts like in beam search means it is going to be very simple um but like I said optimal sequence you will not find it because know the example like I said for Apple right that should show so uh it is not always possible to get an optimal sequence here is a sample you know how this gry decoding basically Works let's say practice you know let's say the next word right the uh the previous word could have been the right the practice key seems to be the next most probable token but there are also possib possibilities like influence and then great which has probabilities of 0.2 0.2 here the vocabulary size apparently uh they are trying to see the top three tokens okay and he has the most probability in which influence during uh had are the three tokens which are most probability here with had being the most probability so the practice influence time could have been another option but it doesn't consider that but this is how it will work it will go here so for example had again will see the uh top three and then from there it will see the probability this is how basically the gry decoding works and here is a like you know step by-step example if you want to see okay so let's take uh this as an example the cat sat on and then uh you know we'll try to start the sequence okay here what we'll do is uh we'll compute a probability for the first tokens and all like you know for all of these tokens you have the probability and the next token will have the probability you know uh the Apparently has the most probability here here you can see 0.8 0.05 okay and then what we will do is We'll add that to the input and then again we'll try to generate uh at time stamp T here uh cat is generated basically we are trying to generate this okay that is the task here the model is tasked to generate the cat sat on and and with like end something like that this is how it will go the cat will be again combined together and then the most probably one was sat and then on and then the most probable one was end this is how it works but the mathematical representation if you want to say it is like AR Max of YT is uh given a probability of Y 1 is to T minus one with the input okay just to say like you know which is the next token given the previous token inputs okay that is what is given here advantages and disadvantages are already said right and here is a summary okay this is how apparently it calculated and found the value and as a summary table you know of whole Transformer first you'll have the token embedding where it will convert it will be converted into dense vector and then you'll compute the potion embedding which will be added together and then you'll multiply it with qkv which will provide you an attention weighted Matrix and then your feet forward will will transform the features and then there is uh add layer Norm which has a residual connection and the layer normalization which will normalize the feature feed forward has a linear layer sent to an activation function and then L layer and uh multi attention will split uh multiple attention hits compute self attention and then you'll do a weighted averaging which will give you attention weighted Matrix and that attention weight Matrix is a context s attention Vector okay so yeah that is about the Transformer architecture I hope you all uh got something out of today and the next day we'll try to code this architecture for translation I'll see you all in the next video Until then if you all like this video please hit the like button share it with your friends if you haven't subscribed to the Channel please hit the Subscribe button hit the Bell icon happy learning Welcome to the day four of 30 days llm Mastery course in day three we all saw on everything which is related to the theory of a transformer architecture we saw how it works with mathematical examples and today we are going to implement each one of these mathematics with the help of pyo let's jump into the video right now our aim is to build a translator where what we are going to do is we are going to make a translator from German to English with the help of Transformer what are the components it has first we need to create a positional encoding right let's do some basic inputs first so that we don't get any errors in the future let's import torch let's import torch do NN as NN optim as optim we need to add input statement there and then we have some other inputs which we need to do we will need math down the line inut math we need time for logging whenever we need and then we need Spacey we need for tokenizer we need logging so that we can print logs whenever we want and then from torch. utils data if you have think thought you know like this is data set and data loader then yes very good we are importing data set and data loader data loader and then also we'll use something known as Pat sequence okay because in a batch it is not always uh common that you'll have a same sequence length so we'll have a p sequence t.n dos. RNN which means like you know there is an RN util which they have written that is p sequence okay and then finally when we are doing a training Loop we need to see the progress right so for that from tqdm import tqdm that's all these are our inputs okay so now what are the things we need to do first set up logging we need to set up our loging okay set up seats random seats for reproducibility we need to do that as well we need to Define data set data set definition then we have load spacy tokenization models for tokenization we need to tokenize right so need we need speci models for that and then path for data sets we need to Define it let's put that as well right now we are just doing what are the things we need to do so that we'll have a clear plan plan okay and then we need to load those data sets with the data set class will return right okay and then we need to Define our special tokens the pad token OS tokens start off sequence token Unk token all of those should be defined and then we have tokenization of sentences we need to tokenize all the sentences so we have tokenized all sentences and then create vocabulary we need to create vocabulary right let's put it as vocabulary creation vocabulary creation that is also there um and then based on those vocabulary creation we need to um you know like that is a function for vocabulary ation and then we'll add the function with special tokens Okay add special tokens or let's put it this way create vocabul with special token okay and then what we can do is we can start with our Transformer components okay and why I return all of these because we'll do this later okay first we'll focus on creating a Transformer components that is the first thing which we are going to do and that is why I said like you know let's do this first and for this we are going to start with positional encoding because that is the first thing here right here if you see the first thing which is there is positional encoding and that is what we are going to see uh right now first let's create that let me put it here positional encoding let's go to the mathematics so that we can be able to implement it right so what is the formula here position encoding of a given position and the embedding index it should be sign of position number by a number which is you know like very common 10,000 part 2 I by D model right so how can we implement this that is what we are going to see okay first we need to create a personal encoding class here I'm going to inherit the nn. modul so that it has all the characteristics like forward and all okay here we're going to initialize it and what are the parameters I want to give here I want to give my D model which is my embedding Dimension and my max length which is my Max sequence length okay let's put it as let's say th okay don't want to specify these types and all now first I need to create a placeholder you know which will be able to store the position values and all so first I'm creating an empty uh penser of zeros to. zeros and what will be its Dimension if you see for each token I'll have a uh you know 512 or whatever dimension of D model we have that is what is here right for each position and have number of uh values so it is going to be max length comma B mod Okay so that is my position encoding storing placeholder this will be basically my position encoding output okay and then now I need the positions you know what are the positions which are there Position will be 0 to max length right so that is what we need do. arrange so there is a function known as arrange which will be generating number from start to end okay so we are going to get that zero till max length and here we are setting up the device to be float because we are going to do some computations here right so for that we are setting it to uh float and then we are setting up UNS squeeze of one why is that because to add the batch as a parameter okay that is why now we are going to calculate the division term so what is the division term this is the division term okay we are going to compute that so if you see the division term right 2 I by D model if you remember for position encoding I said we'll split it up into two right odds and evens 2 I + 1 and 2 I for both of those if you see uh 2 I by D model is same all right so the division term will be common for both of those and how we'll calculate is torch. exponential because we need to do exponential of 10,000 right so what we'll do is for each position torch. a range of 0 comma D model we'll do a step of two because we need to do for 0o two four like that okay do float so here we are going to multiply it with for each value we'll be multiplying it with negative of math do log why negative of math. log because the 10,000 is in div uh division right so 10,000 uh s negative of math. log and why this came know tor. because there is a position at the top okay that is why we have negative because all we'll do is we'll just multiply this value to position that is what we'll do next so that is why we have math negative math. log and what is the frequency term here 10,000 right 100,000 10,000 and then add a float value okay which is zero and then we'll divide it with D model as it is written there okay so now our division term is ready we'll compute a position encoding where I'll go through all of those value in a batch okay here you can see right in batch for Max so each token will go and we are going to fill the even values alone okay so for even values alone we'll compute t. sign off we already have the 10,000 part y by D model all we need to do is multiply it with position okay so we have 1 by 10,000 uh 2i model which is already computed here all we need to do is multiply it with position so we are going to do that position dra D and why this will work already if you see the position is calculated um with the division term being you know 2 I okay so which means our division term is ready and the position we are splitting right we are splitting it here that is why it will work okay question encoding of is two which means we'll go through all the batch one is is to2 will be if you guess it t. cost then yes you are right t. cost of position multiplied by division term okay now what we'll do is we'll do an UNS squeeze because here if you see for each token we are going through here that is the batch that I saying about for one batch we have all the tokens but what we have is you know bat size comma sequence length comma um you know D model that is how it works so we'll go through that e do UNS squeeze of zero we are adding a dimension and then we are transposing cling it 0a 1 okay finally the most important step which is register buffer and what we are doing here is this model will be training okay this module will be training because it is an n. module and we have a forward here but the param should not be updated okay so that's why we have P set up as a buffer okay now we'll Implement forward if you see oh I don't need that you see forward right what happens here it is just adding up it is having its own encoding it is adding with the potion en input embeding right that is what we are going to do we take the input embeddings and what we are going to return is return input embedding plus self. position encoding of is to X do size of 0 from zero you know from zero index till the max length we are Computing the question and coding okay that is what it says and then we are having uh you know the dimension left out the second dimension which is M Dimension okay we'll add that and we'll be fine okay now the first major important component is done okay next what we are going to do is we are going to implement attention first we are starting with encoder okay then we'll move with decoder what do we need here we need multi-ad attention right class multi head attention and here also it is going to be nn. module here it is attention and here it is position encoding okay and here I'm going to do an init here again F init and what are all the things I need here D model and then number of hits what is my number of attention hits we need both of those first what you need to do is we need to verify if your D model which is your embedding Dimension is divisible by number of XS because that is when you can do equal split right how we can do that we can add an assertion statement by doing D model percentage which will calculate the remainer of num hits equal to equal to Z this is just to check e model must be divisible because if you know it is not equal to zero this statement will be printed okay that is why we have it here now we'll add these values to the self okay self do D model equal to D model self do num H equal to num hit so what will be the head Dimension okay the head Dimension or else we usually state it as Dore K here okay that is what we stated in here in multi detention okay so that here you can see Dore K right so we are going to compute that as well self do Dore K is equal to it is the head Dimension which means you will divide by head you'll divide the embedding Dimension by the number of H okay now how does the attention work you know first I said right we need to uh calculate the weight vectors multiplication with the input right so first we'll initialize those weight vectors weight Vector of Q will be equal to NN do linear of D model by D model this will be the same for all okay B and then finally a projection if you all remember that is the one which will concatenate all the information okay so those are the functions we have here now let's Implement forward self I get the query value key value sorry key value value and then a mask by default it will be be none because this mask might be of the padding mask or else you know uh decoder mask both of course right first we'll get the bat size batch size equal to if you call query do size of zero that itself will give you the bad size because bat size will be the first Dimension bat size sequence length embedding Dimension that is how usually the structure will be now we are replacing the query with the weight Vector which is like the query Dash okay that is what we doing X star WQ right that is what we are calculating here here you can see this one this is the line which we are trying to do here cell do weight of query we have a weight Vector already and then we are you know sending a query in there but we are have opting a view view will you know uh reshape the tensar basically and how we reshaping it we need the bat size and then the final Dimension which is the embeding dimension and then what we're doing is we are splitting it we splitting it up for each hge okay num hedge and then c. Dore K and then what we are doing is transpose of 1 comma 2 why this should be replaced here right the number of H should come first and then only your sequence L this is a sequence L actually okay and this is come here right so that is why so your query similarly will happen for key if you all remember right key value send it for key send it for Value here it is going to be K it is going to be for B again a then B right which means your query key value is ready we need to compute the output which is attention output okay attention output will be equal to self do scaled do product attention and we'll be passing this very key value and then Mass to it which will provide our attention output now we'll create this function scale um why to do this we just just copy and paste this scale do product attemption s qu key value and then mask will be equal to none right so this is the function where first let's see here here it is we need to calculate qu key transpose and then divided by root of here it is H here it is root of uh DK right square root of DK that is what we are going to do now attention scce equal to do do matrix multiplication of what are we need to multiply we need to multiply quy and then key we need to transpose it right quy sorry and then key dot transpose of final dimension transposed okay that is when you know like it will become uh good enough for multiplying it query okay and then now we are going to do math do square root of s dot Dore K okay so that is how um the attention score will be calculated once theen s code is calculated if you all remember if there is a mask we need to apply this mask okay visually usually we saw it with um decoder but it also applies to the encoder because there is a chance that you'll have padding in your batch and if you are padding it there should not be an attention for it so how we'll do that is attention scores and by default there is a function known as mask f for uh torch itself where what you need to do is we need to provide the mask tensor where and all mask is equal to equal to Z I need to replace it with -1 E9 this is almost negative Infinity okay and why we provide that if you all remember if you provide negative Infinity it will become soft Max will be uh close to zero uh please before watching this video make sure that you are uh very confident with what I have explained in the previous day which is day three because that is when you'll be able to understand what I say okay now where is the formula we already calculated query key transpose by root of DK okay now what I'm going to do is I'm going to apply that to softmax so the attention uh probabilities now Dome probabilities not just SP torch dot soft Max off attention scores and then my Dimension is minus one because I need to happen it in embedding Dimension okay so the dimension is minus one now the attention output will be because here if you see my soft Max is ready right torch um M what I need to do I need to multiply my attention uh probabilties with value right and then I need to return return this output which means my attention is ready now okay my attention is ready is my mathematical example ah here it is till now we have done till this okay but if you all remember it is not just that we need to do uh projection okay that is also there so we need to prepare that for the projection right now um it is num hits first uh like you know num hits and DKR together we need to replace this with D model okay so output will be output do transpose 1 comma 2 why are we doing that because we already did a transpose here we are trying to get that again um we are applying continuous because uh we need to ensure that when we are calling view it should be continuous okay bat size and minus one like we created for uh query and key but again we are comp uh combining um num heads and DK as D model self do Dore model and if you all remember this is the concatenation okay here you can see right the concatenated output this is nothing but your concatenation and then once you concatenated you'll multiplied with wo right and that is what we are going to do now return self do w uncore o of output okay now your attention is ready okay next what do we have what is the component feed forward let's Implement that then E4 CL position wise power NN module what and all do we need here we D model and dff okay there is two three so given a value will project it from D mod to dff and then dfff to D module so how this will work let's see that def Thunder init we super in it so that we can overwrite things right and then we have our D model and dff here now what I'm going to do is we have cell do fully connected layer one will be NN do linear of T model to DF right and then a fully connected Layer Two actually this is where the layer will happen Okay this is not the layer there is not three layers there is two layers this is will be one layer and this will be another layer and that is what we are trying to do fc2 fully connected layer 2 will be NN linear a linear layer which will compute from dff to D model we have expanded the information to a dimension which is bigger than D model and then we are Contracting it again which is usually D dff is usually you know four times uh D model dff comma D model and then if you all remember we need to uh do re activation function in between right so self. R will be n and reu so reu is called so how the forward will happen now left forward L comma X return so what we knew will do okay let's do it from this itself self. fully connected layer one of X for which we'll call self. relu okay now the fully connected layer one is applied on top of which reu activation function is applied and now it is sent to cell. fully connected Layer Two okay this is how the position feed forward works now if you see this has been very simple till now right but our encoder is ready already okay now it's time for us to create one encoder layer after which we'll do for decoder layer as well okay first we'll create the encoder layer because all the components here if you see multi detention feed forward uh all are ready and then uh for layer Nom that is not an issue there is a layer n function by default aail able in torch itself we'll use that okay first we'll create the encoder layer class encoder layer nn. module ther in it sell comma what are the things we need here we need Dem model Dimension we need uh number of hits for attention we need dff for feed forward already we have D model ready and then a drop out because if you all remember I said you know layer NM and Dropout are there for you know regularization and all so those we need here first we need to call super super do Dunder init will create all those right now we need uh self. attention why do we uh why didn't we do uh you know embedding of person encoding and uh input embedding we didn't do that here because encoder layer is here that is separate this is separate okay encoder layer is one which will replicate for the number of Stack right now you know like there is six stack right so we'll compute this portion coding only outside so the first component will be self. attention let's put it as self attention because cross attention is coming into the picture right self do self attention is equal to multi-head attention of D model and then DFS next what all do we need we need uh position way feed forward right which is feed forward so self dot feed forward and that is going to be position wise feed forward being called with D model and dff right and then we need a normalization here right there is two normalization one after attention one after Fe forward we'll call that as well self. Nom 1 equal to NN do layer nom for layer n you need to provide the shape okay and here we are working with D model right we need to normalize it at embedding Dimension and here also we are going to do a normal normalization two and then we'll call a Dropout which is equal to n drop out of your Dropout probability which you called right now okay and here uh I've called it wrong it is number of hits right um now our initialization of all the layers is done we need to do our forward right so our forward is going to be cell X and then attention mask okay so the first thing which we are doing is multiplication of attention uh like you know providing attention that two with three values right all you need to do is to calculate attention output sell. self attention we just providing a copy of the input three times that is what we do for multiplication along with that we are providing the mask okay our attention is ready now add a layer n so here x is going to be cell do n one of add a layer n uh let me show you the formula is layer n of X Plus sub layer of X okay so the layer is called here and is going to be X Plus self. Dropout we already calculated uh attention output right so we are just adding a Dropout to that and then calling attention output so now FF output which is feet forward output we need to calculate that and it is going to be self do feed forward so we'll provide the input here again we are going to call the same okay we going to call this here we are going to write it as FF output and then we are going to change it it as self. Nom to okay because that is how the architecture is right here see the architecture it is there like that now this is my encoder output which I'll return return it as X let me put for decoder now class first and say it as decoder layer SL decoder layer and here again we are going to call it with nn. module f in it of self where we need to provide the same values D model DF uh number of heads we'll go in the same order right number of heads dff and then drop out right so yeah um right now our initialization with parameters are so right now uh we need to First do a super. in it right so super dot Dunder init and what I'm going to do is actually you know just copy all of these and paste it here and why we are doing this you'll let you you will know you know first if you see self attention for self attention the logic is already written right the master multi detention logic is already done and that is already ready right because we already added mask in attention multi attention which means that is ready okay so all we need to do is just provideed with the mask correctly so that is ready we need a cross attention so that is going to be self. cross attention and the cross attention if you remember my explanation guys if you haven't watched uh the Transformer architecture uh detailed explanation video please watch it because without that you will not understand why I am doing things like this okay so for that we just going to provide the values of encoder as key and value but still the logic Remains the Same right it is going to be still scale. product attention so the initialization Remains the Same uh and also here there are three layer normalization right so we'll add a layer normalization here self. Nom 3 which will be NN do layer Nom of Dropout right so here this is going to be changing okay we don't have just X we need X which is X is from decoder okay for given token X will be uh coming inside but along with that we need the encoder output right so encoder output will be there and a mask for the encoder output and a mask for decoder output so encoder output mask uh let's put it as Source mask and decoder output mask will be Target mask right right now self attention is working on the encoder uh right so the mask will be Source mask sorry Target mask okay and now I'm just going to copy this paste again okay here the attention output is going to be calculated for cross attention and like I said this is going to be query key value right key value remain uh change it with encod output that's all okay your cross attention is ready just change it with LOM two and then here change it to Nom 3 that's all decoder is ready okay now your encoder is ready decoder layer layers are ready not you know like still we don't have uh embedding to be added and the classification head uh final head LM head is not ready um stack of layers are not replicated all of those will be done in Transformers which we are going to see right now Transformer so in this layer we need a lot of inputs or you know like for initialization class Transformer we are initializing it from nn. module and here we are using under init watch and for parameters I'm going to provide Source whatab size because if you remember um the token embedding right where is it ah here it is each of these are tokens and like I said each input token is represented as a dense Vector okay so this is like a dictionary that is how n embedding is and for each token it will have a 512 or 1024 whatever embedding Dimension we have for that it will have a mapping okay so we need the workup size for both not just Source but also for Target Source workup size Target workup size and then uh it is added with D model and then number of heads we need uh number of layers you know like right now here if you see it is written NX right number of layer like this is N1 which means in that stack it is the first layer like that we need for n layers right so we'll replicate that for that only it is there dff and Max sequence L along with rockout right these are my imports and here first I'll create an instance for encoder embedding which will be NN do mding off Source recab Si from a b model for each token in the source recab just generate a whatever Dimension Vector we provided for D model so similarly we'll do it for Target as well for which we'll provide Target work SI and that is going to be my decoder EMB okay and now we need to initialize position encoding because if you remember the positional encoding is not added in uh either coder or not decoder or decoder sorry so it is going to be NN do positional embedding encoding rather not in sorry positional encoding we already return it right so it is going to be D model along with that we need to provide the max sequence length if you remember right so those are done now we need to create the stack of encoder layers okay so cell do encoder layers will be for for this we'll use list comprehension okay there is a thing known as module list which will have you know like list being uh an iterable one so what we are going to do is we are going to call the encoder layer with all the parameters D model num HS dff and drop out for for underscore in range of num layers now what do we do for decoder yeah you're right just put this again that's all instead of this just replace it with decoder in here I'll replace it with decoder because the parameters are going to remain the same okay now here if you see I have a final output right softmax is just going to be applied that is not an issue you know like even if you don't apply it uh that is not going to be an issue a lot um but feed forward output is required fully connected output so we'll initialize that as well okay so it is going to be cell do fully connected out and that will be YN do linear of we'll have D model being projected to what token it is you know like we are seeing which token it is that is why it is written like this and then we have a Dropout as usual for normalization NN do dropout with Dropout probability which is already there in init and then we can uh also provide a scaling for embed you know if required so because at times embedding can be pretty huge okay so we can also provide a scaling this is optional okay this is not as per uh the implementation of you know [Music] um Transformer paper so it is not like that I'm just uh you know making sure that there is no problem data death forward self and then we'll provide source and Target which is nothing but source and Target tokens okay so for this we need to First calculate mask so Source MK and Target mask will be equal to self do generate mask which we haven't created yet for which we'll provide the source and Target okay now we'll Implement that function if you remember for Source it is just going to be uh you know padding mask but with for Target you need padding mask as as well as that auto regressive mask C mask okay we'll do that now source and Target here right now the source mass will be equal to Source not equal Al to we'll have a source vocabulary okay right now we haven't implemented yet of P token wherever there is not a pad token right then we'll consider source and then we'll just UNS squeeze it one do two copy this paste it here here I'm going to replace it with Target vocabulary it is going to be one three here Target replaced here and Target mask right so now I'm just going to provide a function which I'm I've written okay I'll explain what happened in that the sequence length right now is Target of shape of one like you know that is the sequence length because we already added UNS squeeze. one and then UNS squeeze. 3 you know at third dimension only we are adding a dimension so that is going to be shape of one we are having a cal mask which is known as no peak mask so that it won't Peak the Future How we'll do is 1 minus to. tryu Tru what it will do is it will create an upper triangular mask like this there it is here you can see 0 0 0 and then in the upper triangle there is a value right we are doing a 1 minus so that it will do at lower okay so one for sequence length comma sequence length we'll generate a no peak mask for Target mask alone uh along with the padding mask we are multiplying it with no peak mask and then that is return returned okay we are will now calculate Source embedded value Source embedded will be equal to First We'll add a Dropout okay we'll call the self. Post encoding of self do uh encoder embedding of source here is where I'm trying to scale okay self dot scale okay so similarly I'll do for Target as well ERG that is my target embed it so this is going to be my decoder embedding there it is decoder embedding for which I'll use Target okay which means my source and Target are ready my encoder output for initial first step right it is going to be source embeded that is when it will work you know uh we're just having it as just dummy value to start with for ENC layer in self oh yeah it is like that what is it encoder layers right yeah s for. encoder layers we calculate the encoder output which will be equal to ENC layer and encoder output why are we providing it you see uh one layer output should be provided to the other layer like you know in the stack itself so that is why we are starting with with Source embedded and then that will be provided next next okay encoder output along with Source mask right next what I'm going to do is copy this P it here and then I'll put it as DC output put that as well here and here the order is decoder output encoder output Source mask and Target mask oh source mask is not used here guys in decoder I just left this but please remember that decoder uh since in Cross attention we'll work with uh [Music] um what here this is wrong yeah right here here it is um so things are right right now so the for cross attention we use Source mask because we are working with Source tokens right so here we need to replace with Target embedded and we'll use decoder layers is going to be DC layer and we'll change the decoder output because from one uh layer in the stack to other layer when it goes it needs to change so we'll provide decoder output along with that encoder output along with that Source mask and then Target mask okay finally your decoder output is obtained on which we'll calculate the final output which will be self. FC out which is the fully connected out the layer final linear layer out of which we'll send the decoder output and that is our Transformer layer being ready okay finally we have implemented the Transformer but like I said there are other things to implement for example uh there are things like these to implement model hyper parameters model initialization initialize the model with appro uh like you know appropriate values and then here whenever we need we need to uh provide the logging as well that is also there so here we need to provide the optimum ER and loss and then collate function and then finally a train function inference function all of those needs to be return right so now I'll um you know do all of those but you know I'll not sure right now I'll just walk through for those okay so right now I have written uh the code and then I'm just pasting it okay so here you can see the Transformer components are already there like we wrote it but here if you see the logging is added we are logging at info level seed is set so that you know we'll be you will be able to reproduce what I'm doing and then for data set we are using multi 30k data set uh don't worry the resources will be provided correctly and here it is going to be source file and Target file where uh source file will consider the source data set and Target file will consider the target data set and then Source transform will have the source tokenizer and Target transform will have the target tokenizer okay so those are loaded here so to open you know uh the file it is a gz file that is how it is already there so we are loading those data set we are calling it here okay if you all remember the data set definition you need to provide the uh data set length since it is a translation the length of load uh source and Target are going to be same right now this w't be required so source and Target data uh length are obtained from here and then we have uh self. Source transform and self. Target transform so if you see in day four uh all the resources are there now G files okay don't worry uh already people were asking me resources are not updated all will be updated correctly okay so I'm getting uh the source sentence at an index we are stripping it we are tokenizing it and that becomes my source token similarly Target tokens are uptain for tokenization uh like I said we are doing it from uh German to English right so first for Source we are loading a model which is trained on uh German context or German Corpus which is De core new s and similarly for Target we are using enan core if you haven't uh loaded this model so first you need to download it with these two commands Okay make sure that you execute these two commands and then then you'll be good so for tokenization what we doing is we are uh calling the tokenizer of this model for Source tokenization where we are going through each text we are tokenizing it and then we are Computing the text to become lower okay similarly for English as well and then here we have provided the path and then here it is to load the data set and this function is to sorry this specific section is to uh Define the special tokens what are the special tokens we have P token start of sequence token end of sequence token and Unk token what how we will create is for each of those token we go and enumerate and that will provide a number for those token that's all okay so token let's say V will be one a and will be two something like that so for that only we are doing this and that is what this function will do now we'll uh calculate the tokenization like in toiz values output for the training data set for both source and Target and we'll also create the vocabulary for source and Target okay and here we are just printing the loging and then our Transformer which we have written here already and then next we have the hyper parameters what is my vocabulary size vocabulary size we wouldn't even know because you know we are calculating it dynamically based on uh this vocabulary created and similarly go for Target by vocabulary size my embedding Dimension is 512 I need eight attention heads three layers just three layers we are not even going to six or eight or 12 and all just three layers and then we have uh feet forward Dimension has 2048 and Max sequence length 100 just 100 tokens more than enough okay finally uh a drop out probability of 0.1 we are initializing the Transformer with all those value here if you see the order also will be same which means your model is ready and now we need to define a tokenizer here I'm adding a login which will show you the number of trainable parameters how many number of parameters are there which will be trainable in your model why I'm saying it as trainable parameters is because if you have question encoding that is not a trainable parameter okay um here we have Optimizer we using Adam Optimizer with a learning rate of 1-4 and Epsilon of 1 eus 9 and uh we have set a padding index uh because for Ping index law should not be computed okay so that is why uh we are using Crosser below that is the standard loss and here we are ignoring the index which is padding index Now colate function is the function which is responsible uh for doing all the functions which is related to creation of input for the model okay so given a batch the batch will be fetched by the data loader so the source and Target will be there for each sample in the batch what we'll do is we'll do padding um and adding Unk token if the token is not there in the vocabulary all those things will be done here you know and then we'll pad it here okay that is what is there first we'll see if there is a uh start token so we'll add the start token and then end token and also once that is done if there is not a token We'll add a Unk token okay and if there is a requirement for padding we'll do pad sequence as well because it is not always that you'll have a 100 token length finally we are providing it with a transpose 0 comma 1 and here our training Loop starts okay so for training loop I hope you'll all have an idea of how it works okay so I'm not going to go in detail I've already explained it in the prerequisits and all so initially when you start you should start with Optimizer 0 gr okay you will get the model output and then uh you will send it to your loss where no like we are changing uh the target Dimension such a way that Target and output which is uh predicted and ground rout are of the same Dimension so that cross entery loss can work once you compute the loss you'll start the backward propagation uh and then Optimizer step will change the parameters and then we are just adding the loss okay clip grad Norm is used so that you know the normalization is not too huge it will uh you know flip it with some values zero to some value okay so similarly I have an evaluate function as well and then there is a translate function translate function is basically the inference function where given a sentence right we'll tokenize it because tokenized D is the function which is responsible for you know German tokenization we are adding source Target okay and if there is token which is not there in uh vocabulary we are just adding Unk token now we are making this a tenser we are generating mask for it we are getting the encoder embeddings and then we are getting uh the output of the encoder layer similarly we are uh calculating it for decoder as well but why we are doing like this because we need to implement the G decoding which means that should happen token level right so that is why and if you do it at model level you won't be able to uh you know work it as taking the encoder output and providing it to decoder that and all can't be repeated all process will be repeated but right now we only need to do a loop for decoder right so this is where we are implementing the decoding strategy here you can see the output is obtain okay for in the output we are going through the eming Dimension and there we are obtaining AR Max okay which is the next most probable token we are getting that we are appending it to the Target indexes where it is start with the start of sequence token and then the context is already there from encoder output so it will provide the next token similarly we will go unless and until it generates a Evo token we will not stop the loop okay and the target token is uh finally decoded and then we are providing the target token of 1 is to minus one because zero token will be your start of sequence token and minus one token will be your end of sequence token without that we are providing okay now here I'm going to train it for 10 Loops clip graad Norm is range is one and the B is 32 now I'm creating a data loader okay and training is called inference is called finally uh the model will be saved here okay best valid loss so that you know like the loss with the best validation loss sorry you know the model training with the best validation loss will be saved here perplexity is also calculated okay perplexity is a metric which will show you know how much your model is confused when it says sees a new data okay less the value better it is and then uh we are loading the model State dict once we save it here you can see it is saved we are loading it and then in the test data we just taking the three and then we are predicting it okay so actually I have already trained it so all I need to do is just inference it okay so I'll comment these alone we'll inference it now okay control shift function till CD day four you see I already have translation model okay I got a very good uh loss okay all you need to is just do python one onecore atten Sol py if you don't comment the training Lo code it will train okay but I have already done that so if I do inference I'll get some output so here you can see um you know white fence White Fence SL green grass green uh you know like it is not very accurate but you know we can work with it we are starting with the good one right all the translations are uh good to start with we can put that way okay all you can do is just change some hyper parameters add some layers trrain group add it for longer values you know make the embedding Dimension bigger make more attention hits more layers then yes um you can do it for a bigger data itself and also maybe as an assignment what you can try to do is do it for English to French okay maybe do a translator for that and let me know if you can able to do that so yeah guys I hope you like this video If you like this video please hit the like button share it with your friends if you haven't subscribed to the Channel please hit the Subscribe button hit the Bell icon I'll see you on the next video Until Then happy learning hello everyone welcome to the dayi in your llm Mastery course in 30 days in this video we are going to see about the different types of portion encoding including absolute portion encoding the types of absolute portion encoding uh relative potion encoding what are the methods in which you can do the relative potion encoding rope and then we'll also see uh mixed mixture of potion encoding how you can add two kinds of potion encoding for your own custom use cases and then we'll also see one of the latest methods which is Alibi and in each type we'll see anything and everything which you need to know about the each type for example we'll see about what is a type all about the mathematical formulation of it why should you use that type how it works a detail code implementation for the type and also we'll see a plot on how this portion in encoding will impact the model's attention we'll see uh those as well and then we'll see the pros and cons and how it is compared or like you know how it is piring against all the other methods so you can see that this is going to be a very comprehensive video so stay till the end and have a good learning ahead in this video let's start with absolute por encoding what is absolute po encoding it is a method in which you'll incorporate position information to the Transformer model by assigning a very unique Vector it will uh for example position one will have a unique vector and position two will have a unique Vector right but this position in uh encoding right this will not change it will be always the same if I run it even after 10 times it is going to be the same okay always the number 10 will have let's say uh a vector starting with the number 0.5 it will be always 0.5 Okay so this will allow the model to distinguish between the tokens based on the absolute position within the input because we all know why potion encoding is important it will try to provide the potion information uh to the Token because of one of the examples which I have discussed in the Transformers video itself but I'll again provide you with that I love eating Apple at Apple Incorporation right here incorporation uh the position of incorporation is what uh providing context to to Apple that you know that is a company and eating is what is influencing Apple uh as a fruit right so for that portion encoding is important and we can do that even with absolute portion encoding what are the types in which you can do absolute portion encoding first the portion encoding method which was there in Transformer paper which is sinusoidal encoding where you'll use a sign and cosine wave for even position you'll use S Wave sign of position uh by 10,2 I by D model I I here says about the I embedding Dimension and if it is odd you'll use uh cosine okay what are the pros for these methods it is deterministic which means uh like you know it is not going to change like I said it is a mathematical formula so it is never going to change so you need not learn for this and also to some extent it can extra it to unse sequence unseen sequence lens for example uh you have trained it only for 1,24 okay since it is just a mathematical equation right if I give 1025 there is going to be still some value to it but you can't expect good results okay that is there and the values are within a bounded range it is between minus one to 1 so that is also an advantage cons is that uh like in offense you have a fixed pattern uh there might be uh data which will have flexible pattern being required so in that case this might fail so since it is not flexible it cannot capture you know complex portion relationships and all so that is sinusoidal encoding and in learned portion embeddings what we'll do is uh this is a method which will involve using a unique embedding for each position like uh you know the most famous method of absolute portion encoding is this learn portion embedding because this method is even used nowadays okay if you see in py torch there will be a layer known as n and embedding layer if you use that that is learn potion embedding okay for each position there will be an a vector which will be calculated okay and that Vector will remain the same when we start okay but when you do for training right it can uh you know uh train itself and adapt itself for uh data specific big things but how it is going to be absolute here you see like I said for each position there is always going to be a value in a table right so it becomes absolute here there is nothing you know relative to one uh token so if there is an impact of one token onto another then it becomes relative right the position becomes relative here but here regardless of what uh you know like how many numbers are there it doesn't matter this token has its own position value that's all okay so given a position I'll uh see my lookup table of embedding and that is a learned embedding table uh while I'm training it will be trained and there I'll get get a value here we'll overcome one of the issues which was in the cons here which is it can adapt to data specific patterns because you're training it and it is simpler to implement here Al and here at least you need to code those okay but with uh learn portion embedding you can just call n. embedding and that will be more than enough but since you call n embedding right imagine that as a table I have a table of values from 1 to 1024 what if I have a value known as 1025 can I have an embedding for that no right you don't have some values in the lookup table which means there is no uh way you can extend the maximum sequence length if you train it for 1024 it can only work for 1024 that is an issue with learned potion embeddings and and uh it also requires additional parameters to learn because in embedding layers are learnable layers so it increases the parameter in which you want to learn but learn portion embeddings are a very good method when you want to encode uh things like for example your VI models right in Vision Transformers you'll have patches image patches to encode the position of those image patches in which order the images was there at that time uh you'll require learn portion embeddings okay these methods are very useful and there are two unpopular methods uh like you know not good in any way probably one is algebraic encoding where you know you will not even use sinusoidal uh but it is going to be same where what you'll do is position by what is your max sequence length and we'll subtract one out of it and then power of 1 by D model minus one that is the equation here it uses an algebraic formation but the thing which you need to not is you can also make something like this what if you know like you want to make uh something like Max sequence length by two also that is also something which you can try to do and if you do that you need to divide the position values also by two okay that is also there but you know that is a beauty of math you can customize it however you want so here the pros are that it is deterministic uh and since it is again a mathematical formula it can handle arbitary sequence length but uh the cons Remains the Same as in the cidal encoding it can't capture complex personal relationship and it is not adaptable at all probably the worst method is bit uh where you know like each position will be represented with binary representation for E bit there will be binary representation of the position okay so it will be Compact deterministic and you know like you just going to give a binary value for each position so it can handle long sequences but cons is that it is very discontinuous in nature and binary encoding usually uh takes some processing so that is also uh a concern but these are the types but you need to more know about learn portion embedding and sinusoidal encoding sinusoidal encoding we have discussed in detail already learn portion embedding like I said it is just NN embedding that's all okay so why should you use this absolute portion embedding like you know any of these type first it is unique for each uh position it will give a unique value it is fixed uh which which means you know like the encoding is Dake and it doesn't require any learning unless until you use learn position embedding okay and then there is boundedness um always the values can be scaled because you know you are controlling this it is all mathematical except uh n and eming all are mathematical which means you can uh bound it between whatever you want whatever value you want and some can generalize uh to sequences longer than the those what it has seen during training and if you have embedding layers you can pre-compute it and store it as you know PT file or something like that so that you can for have a quick look up so it is very efficient in nature as well so those are the advantages of absolute portion embedding so how it works first what you will do is you will uh generate unique Vector for each portion using any functions for example cidal function and then you will add the portion encoding to token embedding and then you will send it to uh the model and the model which is attention basically you will use this information while training this is how obsolute encoding will work while Transformer training and this is the code uh we'll go through the code later because you know code I have written it in a better way as well this is to keep this uh specific document cleaner I have added doc strings and comments everything which is required uh all of those are done and those are in a separate file we'll see that later okay but on a general level if you see we'll compute the potion encoding and then we'll use it in attention this will be common for every type that is how we'll uh you know understand how potion encoding basically Works what are the pros and like you know Pros are mostly the reasons why you should use personal coding simplistic uh deterministic extrapolation efficiency stability all of those are there and cons are that it is fixed it is limited it can't capture comp complex relationships uh no relative information so one position doesn't affect the other but you know like for example the Apple at position one and Apple at position uh like you know near incorporation should given should be given different values with respect to incorporation right or good so that relative information is missing here and also uh it is very highly sensitive um because if you change the dimension everything will be changed because everything is mathematical so it is hyper parameter sensitivity is high and now if you compare with other methods uh learn portion embeddings require you know for you to learn but you know other methods in absolute uh encoding can't like you know learn which means that is an advantage or else disadvantage depends on the use case and then there is relative portion encoding uh which will uh encode relationships between position but absolute encoding can't but absolute encoding can provide unique representation for each por which uh relative potion in coding can't rope uh is more effective uh in capturing relative information but Absol coding is more simpler and similar thing will be applicable to alii and why because uh Alibi is a relative potion embeding uh kind of a method so it it can scale with very long sequences easily but uh Ali is sorry absolute is simpler but can't uh scale a lot so yeah that is about absolute question encoding and if you are wondering okay where is the code code like I said will be covered once all the types uh Theory are covered because the codes alone will have a lot of uh information in it theoretical is different and practical implementation is different so we'll see that you know uh later next we'll see about Rel potion and coding um basically this is how Rel por coding will look like if I have a token here right absolute portion what it will say is it is 0 1 2 3 4 5 6 so six will have some value to it and that is the value which we'll use but when we use relative it will give information on how many tokens were before that how many tokens are after that so all of these uh positions will impact the position of this cook okay that is how a relative portion in coding would work and should work so relative potion encoding if you see is a method of incorporating potion uh information into Transformer model by encoding the relative distances between the tokens rather than the absolute position themselves the name suggests itself so this approach will allow the model to focus on the relationship between the tokens please highlight this okay it will give more importance to the relationships which is existing between the tokens and that will be uh very useful for the task which depends on local context I told you right the context Apple Incorporation here the incorporation is giving context that it is a place or a company or whatever it is so wherever there is a requirement for context relative portion and coding is a better option so mathematically there are two methods one is sha at all so that was for a music Transformer what they would do is uh they'll have learned weight matrices w k w k e r okay and then there is WQ Q is weight matrices key K is key matrices e and r are you know like uh the starting and ending position okay so here is a Formula where you can just assume that X is you know your query so query I will be multiplied with uh w q transpose and then w k uh XJ like you know the tokens at position J that value will be there right usually we'll provide XS q and K and all so that's why I said Q or k if you're considering as tokens that is very good and token embeddings just consider as token embeddings for now the token embedding of I token is Multiplied with weight Q trans weight q and made trans for and then what we're doing is uh wait EK uh is computing on XJ and then this value will be added with wqxi transpose plus w k r and relative position distance okay it is a learned embedding here you can see for the relative position clip to a maximum absolute value it is a learned embedding value so it is you know providing some scalar value which you can uh scale and say like you know this is how uh the intensity of distances so this is not a method which is more commonly used the more commonly used method nowadays is Transformer Excel method and that is a method which is used in models like Transformer Excel T5 and so on and so forth where what you will do is you will compute the sinusoidal embedding but that sinusoidal embedding is now made as a relative embedding where instead of computing it for a position you will compute it for the distance itself okay the relative positions will encoded here if you see the like you know the formula seems similar right position by 10,000 2 I by D okay but here we are Computing like you know for example 10 minus 8 which will be 2 by 10,000 par K the current position by D that is how uh the portion encoding is calculated and here what we do is um we'll be adding those with some learn vectors okay and the formula if you see these are similar okay here if you see we have a d normal value but you know uh here what we'll do is we'll add a learn Vector to it so similarly we also add a learn Vector to here instead of using a learn uh weight Matrix for the multiplication with the Learned uh position embedding you know the distance we'll be using a mathematical formula there that is what is the difference between these two equations um don't worry you know like uh theoretically it will sound very hard uh most of the potion en coding methods will look like that but once you see the code right you'll understand okay this is easy something like that so you can see like you know we are more emphasizing on I minus J right the distance between two tokens that is the emphasis here so that is what is the first reason you need to use relative potion encoding it emphasizes on the relationship between the tokens rather than the absolute position and then it will handle long range dependencies easily because rather than using a number you'll use the distance between tokens all right so it is easily uh more scalable for long range dependencies it can extrapolate easily and then it is also efficient in some way um how it will work so basically you will first calculate the relative distances between all the pair of tokens so one with respect to two one with respect to three one with respect to four like that and then for those you will generate the encodings either learning or in know mathematical function so this is a learning method this is more of a mathematical function method so you will use any of those method and once you compute those embeddings it will be provided to the attention and those attention now will utilize this relative personal information to learn the relationships between the tokens and it will have a better training that is how relative attention works and here you can see uh this is a relative Global attention which is almost the same implementation of Transformer Excel method and don't worry we'll come to this later okay and Pros we have already seen as well as the cons if you see it is very computationally intensive for long sequences all right so imagine I have 1024 so I'll have 512 plus 512 minus if I am in the center Okay so that is computationally a bit complex and then uh since you know like uh you are involving a lot of relative ver it will become uh more complex to implement than absolute portion in coding even I faced a lot of Errors when I was writing the code um so the absolute portion information uh which may be suitable for some tasks you'll lose in those task okay and then the memory usage will be very high for long sequences and it is also hyper parameter sensitive when you want to consider the maximum relative distance so how much maximum relative distance you are considering is a very important hyper parameter and that can decide a lot of things in your performance so this method also can be said as a hyperparameter sensitive method to some extent so what are the comparison uh to other methods so in absolute portion encoding you will encode the absolute question uh which can be beneficial for many tasks but you know at some time you'll uh require those kind of embeddings like I said but relative will focus on the token relationships which is ual useful for many tasks but at some point of time you'll lose the global position information because like I said the maximum relat distance is a hyper parameter and you can't keep it 1024 or 2048 and all and because at run time it will become very hard for you to work with so you know that is something you need to think of and cidal like I said uh can uh be extended but it can't capture complex portion relationship but RP can and it is also more flexible because it is just working with relative distances rope but it is uh like you know also similar to relative rope and uh RP both are uh relative potions embedding but you know RP will use R potion embedding will use separate embeddings but rope what it will do is it will try to integrate the information directly into the token representation itself okay so here you can see right there are separate embeddings here but rope is about you know if you want to transform uh embedding separately let me let me simply explain okay so RP if you see has separate embeddings even rope has a separate calculation that is there but with rope what you'll do is you'll rotate the initial T are directly you know like you will directly impact the Rope information which is po information on the token representation itself the vector itself but here you'll have a separate potion embedding which you will add later okay that is how uh rope and RP differs but like architecturally rope is superior actually okay and that is what is we are going to see next which is Rob so the idea is that if you see with uh other methods you know if you encode it right uh for position one the value will be here position two the value will be here position three the value will be here position four the value will be here like this okay there is not a pattern in which it can understand right because uh the relative information will always uh like you know provide different values but with rope you know uh it works more on the angle know like the distance between two vectors is calculated as an angle and with that angle it is just rotated okay the vector will be rotated at different position and here you can see right if the position is this it will be uh the starting position is this and ending position is this for m is equal to Z it become m equal to 1 m is equal 2 m is equal to 3 like you know I can see a circle kind of structure so it is more simpler and uh like you know it is easier for the model to understand uh things which are going behind in rope so since it is very simpler it is also computationally efficient now let's see about rope okay for all the uh Theory just have all of those in mind because these are just to provide you the context okay once you go to the code you will understand how this works rope if you see is a method for incorporation of poal information and transformation model uh Transformer model where what you'll do is you'll use rotation matrices on top of the token embeddings okay so here rope what it was cre why it was created as you know absolute uh potion encoding were very good with uh like you know capturing the absolute information and also it were very good with computationally being efficient but relative potion encoding were having a relative distance where but we're highly computationally uh intensive okay so I Tred to find the bridge between those that is how uh rope came so what they will do is first each uh value will be considered as a complex number as a vector where what they'll do is they'll do a rotation operation okay so the rotation angle right this angle will be known as Theta okay and Theta will be equal to w i power minus K with w and K being hyper parameters you can Define at you know like you know what rate of angle you want to turn you can do that and here the M which is there right I m m is position index okay I is embedding index that is what is in here practically how you know like rotation will happen is X of 2 i - one which is you know like uh the embedding at 2 I -1 of X it will be rotated with cos M Theta I and minus X2 y similarly like you know this thing will happen how this equation came is you will apply this X2 I -1 and x2y on top of this rotation Matrix okay if you calculate it this will work okay that is how it works um I hope you got that um now in attention what we will do is uh we'll use this rotation Matrix computed information on top of query and key to be multiplied before before Computing the query key transpose okay basically this rotation Matrix is going to provide me the positional information with this rotation and that is applied on top of query and key directly and yeah that is going to provide me all the information which is required okay so that is why I said it works on the token embeddings directly there is no separate position embedding for rope okay so why you should use rope uh it is you know like find both absolute and relative encoding because when you are working with one token to another token right this is the starting token and this is the ending token the distance okay so always this absolute encoding is also there and you're trying to find the relative information uh with some angle okay so both of those information are still there so that is why it is finding uh bridge between absolute and relative it is flexible um it also preserves linear self attention properties it is efficient and it is good at extrapolation extrapolation this is probably the best method we have discussed till now for extrapolation like you know uh it can extend even up to 16 times is what I have heard till now let's say I have trained it for 2,000 tokens it can be extended up to uh like you know at least 120k or even more that is what I'm hearing but I'm not sure okay I have not worked with rope personally so I'm not sure of that but it is also very efficient because it is just simple Matrix operation right you just having a rotation Matrix you're applying that that's all okay the calculation of rotation Matrix is there you need to uh rotate your T are in half splited those those are those are different which we'll see while coding like I said theoretically it will look different but practically it will look very very different we'll see those um but you know like it is very simpler to implement uh and it is computationally more efficient because it is just simple Matrix operation first what you will do is you'll generate a rotation Matrix uh based on the position and you will uh basically rotate your query and key tensar uh or vectors with this rotation Matrix and then you'll compute your attention that is how uh you know rope will impact while training here is a sample uh implementation again we are not going to go inside we'll see that in the coding section the advantages are again unified encoding efficiency extrapolation theoretically it is better and also it is very flexible in nature the cons is that uh it is a bit more complex to understand not to implement implementation uh I'll say like you know it is not as tough as there are some other methods um but still you know comparatively to Absolute encoding and all this is a more complex implementation and complex to understand concept um it is hyper parameter sensitive on the choice of base frequency base frequency is what is the Theta uh going to be defined on and then you have uh like you know how basically this potion encoding is rotated you know you don't have a lot of idea and that because in other methods you can print the embedding uh like you know you can somehow see the embeddings but here uh like how the embeddings are going to impact you can just see with the help of attention so interpretability is bit harder and also uh like you know other than that you have uh numerical issues because with very long sequences again the roten angle might not be enough and uh you know that can be a bit hard okay I'm thinking of making a separate video on rope because uh or else even when we Implement an llm from scratch uh which I'm planning in future we'll be using rope only at that time I'll be even more uh explaining about the theoretical aspects okay because at that time you'll require this and then uh now we have another method which is mixture portion encoding mixture portion encoding as the name suggest it is to combine multiple portion me methods so that you can get the advantages from both for example absolute portion encoding will provide unique representation relative portion encoding will provide relationship between two position you can just add the information from these both where what you need to do is just have an alpha parameter which is a learnable parameter okay or else you can also have it as a fixed parameter that is also fine you know like a scalar value and this will decide how much value should be provided for absolute information and how much value should be provided for relative information if you keep both of those same or both of those will be like you know in equal composition but if you want to provide some weightage more for relative because the task demands that you can provide more for relative okay and here what you can do is you can just use absolute and uh relative person and coding like I said and the advantages and disadvantages are going to be uh like no not a lot actually so if you see the advantages it will capture both uh information which means it is very flexible it will have a better performance than the single one itself it will generalize more for short and long sequences because with absolute encoding it will work very well for short sequences and with relative ption encoding you can handle long sequences very effectively so it will be more generalized and it is adaptable for many tasks because it is adaptable for short and long sequences so how this will work is first you'll calculate the absolute portion and coding and then you will create a related portion and coding for each of those portion you will add those with a mixture parameter or the alpha parameter here and then you'll compute the attention that is how it works and the pros we have seen already the consite it is going to be more complex uh than how it was before the computational overhead is high because you know like you are adding another component if it is relative you are adding an absolute component if it is absolute you are working with relative component which is more computation right um so unless and until an absolute portion embedding related task requires a relative component don't use that and here it is hyper the hyper parameter sensitivity is still there every method has its own hyper parameter sensitivity and here it is the mixture parameter okay how much should I uh provide weightage for each of those methods that is very important and then the conflicts uh might occur you know like absolute information will provide uh import to something and relative will say like you know that is not important another thing is important uh so that conflict room is there for that and memory will be high okay because you need to store at run time both of the encoding needs to be stored um so that is about mixture mixture is not very complicated because now you know how relative works and uh absolute works it is just adding both of those okay last but not the least and probably one of the most simplest method to do relative po en coding and that is attention with linear biases okay so short in short it is known as Alibi where what you will do is uh you will actually have this kind of a metrix being generated for your attention this alivi only works for decoder only model okay so here in transform model you'll just uh invol this and unlike you know the traditional methods uh this is very simple but you know it is very effective because it can handle long sequences it can generalize to unseen sequence length but it is very simple why it is that if you see the formula of attention becomes something like this qk transpose by root of DK it is same but you'll add another component of M uh which is a head specific slope for each head you'll have a different y value and uh that is a slope value okay and here it is just a matrix of relative position okay what are the position in the sequence like you know what is the window you are going to cover and then will just multiply it with v that's all and the slope you'll calculate it in this way okay M of head will be 1 by 2^ 8 by number of heads okay into uh H minus one that is how uh the slope is calculated here so from zero it will be there till eight or 16 or whatever it is you can print the number of HS um so you can see like know probably this has been the shortest time I've have explained an embedding right it is very simple here you can see you can just generate a matrix like this uh because this shows the relative right 0o till here it is minus 4 the distance you'll multiply it with the scalar value M which is the slope and this will be added with the attention you already computed this is the costal attention which happens in the decoder right when you add this the personal information is automatically impacted on the attention Okay so it is very simple efficient can extrapolate a lot it is highly performing and it also doesn't require a lot of memory efficiency because because it is not storing it is just you know Computing at run time so it is not storing right um so here alii Works differently okay first you will calculate the attention score and then you will calculate uh you know like the linear bias term here which is M of I minus J and then you'll add that okay you'll add that to the attention score and then you will apply the softmax you'll multiply the value and that will provide you the attention output right so this is a bit different the other methods first you will compute the portion embedding and then you'll compute for attention here first you'll compute the attention score not the attention output okay attention score and then you'll add the linear bias term okay uh also coding is very simple for this okay here you can see uh this is all it is required okay it is very simple to implement Alby um so that also it's a pro it is very simple efficient it can extrapolate because it is Rel mostly relative embeddings can extrapolate okay uh it it is highly performing algorithm uh it is very memory efficient it is also very interpretable because when it is a linear relationship right you can clearly interpret it for example you know if I give it as minus 5 I know it is going to be something like this and if I have M value which is going to be a mathematical value I know what is the value which is going to be here and I can calculate what is going to be the whole values in here and then if I add this and if I know this output I can easily subtract this to know what happened in here it is more interpretable okay and uh here uh the cons is that it is fixed the linear bias is fixed um so it might provide you limited flexibility uh it is Task sensitive where wherever it requires you know um relative embedding that that is where you can use this okay and here the slope calculation formula uh it is said like you know uh in some that it is required to adjust you know this is one of the formula but you can calculate it in any way but it should require it should involve the number of heads okay and uh as far as the comparison is concerned uh Alibi doesn't require you know like explicit portion in codings and also it can generalize to longer sequences like I said it is not storing anything right so that is again an uh advantage against relative po encoding because Ali is simpler it is memory efficient uh but with relative portion encoding you may capture more complex relationship but but it is not with the case with Alibi but uh rope Alibi is far less uh complex you know like to implement and understand and all but rope will provide uh more po information better um so these are the methods which are there I hope you would have understood you know like uh I know these are a lot of information some might think that like I just read uh this content is written in such a way that it is very uh simpler to understand even if you don't uh like you know understand the mathematical section please try to understand the pros and cons when you should use a method how this method basically works that is very important and for how what better way rather than going through the code itself that is what we are going to do now we are going to go through the code of each of these methods so here are the codes here you can see absolute portion embedding is there uh relative portion embeding is there rot is there mixture portion embedding is also there and also Alibi is there alii has been the shortest one and EAS easiest to code okay but we can go in the order in which we saw already okay first we'll see uh absolute portion in bedding and here you can see it is a multi method uh class I've written here um so what happens in this unit function is that we'll set up the different portion encoding methods and we'll store information Dions like uh model Dimension maximum sequence length for uh regularization uh a Dropout layer is created and the PO encoding method is initialized okay so here you can see the parameters are D model max sequence length uh Dropout and what method you want so all of those are added to self parameter with Dropout all initialized as a layer and then we are initializing the portion embedding method which method we want if it a cidal call the cidal method and if it is learned I'll call the uh learned like you know nn. parameter like I said nn. embedding or nanded parameter both are you know like almost similar you can also use an nend embedding method it will still work okay uh algebraic method is there where like I said you can just provide any math and that should be more than enough and binary um so binary what it will do is it will uh try to provide a value of zero or one based on the position okay so these are the methods now we'll go for each of those methods okay sinusoidal method if you all remember first we'll initialize a 0 tenser we already saw sinusoidal method in detail in the Transformer video itself so I'm not going to go in detail with sinusoidal encoding let me Zoom it for you all a bit so that would be the first step and then what we'll do is we'll create a tensor of positions from zero to Max sequence length and then what we'll do is we'll calculate the div term right that 10,000 Do I by D model that is calculated here and then we'll do that np. log minus np. log right 1 by 10,000 part di 2 I by D model so we have the division term now we need to multiply those across the position where for even numbers we'll use S and for odd numbers we'll use cos okay so that is what is happening at step four and five and at the sixth step we'll just provide the portion encoding with the batch Dimension added at first okay okay so that is how sinal encoding works and algebraic encoding if you see first we are again creating an initial tensor of zeros uh position tensor is Created from zero to uh Max sequence length and for each sequence what we're doing is uh at each position we are uh taking the position okay and then uh we are taking the position value here and for that value we are dividing it by the max sequence length and then here if you see we are multiplying it by the position number by self model minus one okay this is the same value or the same code which we saw you know the theoretical side as well and that is written both of these are similar but here we are using a sign and cosine kind of method here we are using a ma algebraic method okay so that is why it is sinusal and here it is algebraic and if it is binary uh we are just you know um first initializing a zero tensor position so that we can store the position in coding and we are going through the each of those position and that position is converted into a binary position okay so in those tensas what we are filling is we are filling it with zeros and ones and finally we're retaining those okay so here you can see so this is some uh place where we are going through each of those position and then uh for each position we are just checking if it is greater than I the position value okay and if it is greater than one we'll just uh say that you know like that is one something like that so it is just zeros and ones you can also do something like you know even positions are zero and all positions are one that is also something which you can try but it wouldn't conver like you know convey any information okay so in forward what we'll do is basically we'll just compute so here if you notice everything is p okay and here also in self. P we are calling the method okay so that we can have one one forward method that is more than enough so self. P of is 2 comma x. size of one like you know on top of uh this self. portion embedding is applied and then we are calling a Dropout so in attention what happens is first we are getting uh the bat size and sequence length for a given input we are Computing the portion in coding and that is your X here and what now we are doing is this x which has the token embedding as well as the portion encoding is now here you can see those are added here right X Plus silver potion encoding information is sent to the attention which is a normal multi-ad attention Okay so these are the values and here if you see we are plotting uh some important plots one is a heat map heat map is more important and then there is a line plot as well okay so let me show uh the plot how it will look like so guys if you all like these kind of videos see uh like you know just say me like you know you like these kind of videos because I don't think anyone is making this kind of detailed video in YouTube so yeah please let me know if you like these kind of videos it is getting ready and here you can see in cidal if you see uh the curves are there right right here you can see the curves are there this is how sidal encoding will look like and here you can see after certain stage right everything becomes straight lines right so in embedding Dimension till 300 it is giving some value after that it is become some straight lines and here the encoding value of the position is provided okay the encoding value of uh position 0 at Dimension zero is zero okay so basically at Dimension zero everything is zero okay but if I go to for example 153 which is the red one here you can see at position Z it is one at position one it gives a different value this is how a sinusoidal curve will impact okay I said you right uh at each portion it will give a different value at each embedding Dimension this is how it will look like it is complex to see but you know like that should convey convey the information but here if you see the Learned embedding right there is no pattern I guess you can see in here because like I said that is just a parameter which is a learnable one which will learn itself and here you can see again there is no pattern okay okay in algebric if you see uh we have computed our own um and that will always provide this kind of a output because the mathematical equation doesn't change and here you can see for each position what is the encoding value which is obtained this is not at all preferable because if you see the encoding value is becoming 0 to 100 probably you can add a scaling parameter later and then finally there is binary embedding zeros and once that's all you can see either red or blue no um so most simple method and if you want to see in detail right there is zeros and ones here so this is how uh basically it happens with absolute now we'll go for relative here is where uh things get tricky right because absolute is very easy to understand uh from relative it becomes a bit harder now we'll go through those methods which are relative in nature now let's start with the relative Global attention which is the most common method of relative embedding okay here in initialization we are uh Computing the dimensionality for each attention head we are providing the linear layers also we are providing a relational position embedding Matrix okay where there if you see like you know we created something like t. Zer of P right here we are Computing something known as ER and that is the Matrix which will provide the relative potion embedding for each token with respect to the the token and then we are also creating an Cal mask which is you know like for the Triangular mask which is to you know make sure the future is not seen that okay so here you can see linear projections Dropout layer uh learnable relative potion embedding of shape max length to DEH so first it is initialized with a random normal distribution it's not random okay it's random normal it follows a goian distribution that is how it is initialized so it is learnable right so it will adapt itself while while it is training over time okay so here in forward what we'll do is first we'll get the input sequence and then we'll uh reshape the key query value and all uh in such a way that it is ready for calculation uh for attention and also uh calculation of relative portion embedding so the next step is to start Computing the relative portion bding bias okay so once that is done first I'll explain the steps and then we'll go through the steps as code one by one and then next we have calculation of attention score where we will combine the standard query key do product which is qk transpose which is normal with the Rel to relation portion bias which is influenced by the self. ER which we created already and then we'll apply Cal masking which is required and then uh normal procedure okay so how this will work first the sequence length uh should be not greater than max squence length basically you know uh this are just normal checks first what we doing is we are making all of those ready key query value and all and then we are finding the starting place you know like where you want to start your relative embedding from there uh till like you know what is your sequence length both are seen now in the values of er we just transposing it okay so like I said this is the position embeding information okay for each number there will be a value where it is um yeah you can see right maximum length maximum length in which you want the uh relative attention to be like you know for 1,24 positions we are Computing the uh relative position okay so if it is more than that we will not do that like you know for 1024 let's say it is 1025 so we will do from 1 to 1025 okay so that is how it will work so for that only we are getting the ER or Rel po bias ready so what we'll do is first we'll multiply that with query okay and now we'll do an operation in a SK okay skew and this is an operation which will allow you to shift the bias Matrix for L position we'll see how that works okay so given uh an input Matrix you will do a left padding okay like I said you know what if there is a position which is let's say 1026 okay so at the time you need left padding so left padding and now uh you will just you know uh reshape The paded Matrix into rows for each position okay so that will give you batch size uh number of heads number of rows and number of columns uh number of rows is nothing but you know the tokens and number of is your embedding Dimension here actually okay you have reshaped it and now uh you are ready right because if that is before your um position uh of choice you are just making it zero so those are done and that is what happens in this Q operation here okay and now we are multiplying with matrix multiplication and then you're uh Computing your attention and you're filling your mask okay basically this is what is happening and if you see the attention right this will be very fun you know this is one of my favorite attention M uh output which we can see because uh out of all the methods this will provide you one of the best uh Matrix to see okay python relative. py here you can see right how much each affects okay future is already masked so those are not even considered and with past here you can see all right so you can see the previous ones usually have more uh score and then you know like when you go even more previous it will have a very very less score close to zero farther you go closer to zero it is all right so it is query with respect to key okay this are the query position and this is this key position with respect to each of those we are seeing the attention this is relative attention okay now we'll see about roty okay and then we also have mixture um for mixture if you see uh like you know we are not going to do a lot hard things and all what we'll do is we'll have an N embedding which is a relative embedding first we'll compute maybe like you know we'll complete the mixture embedding and then we'll go for roty because mixture is more easier so what basically what we'll do is um we'll create an absolute potion embedding with the help of cidal and then we'll create an embedding layer for relative potion embedding and what we'll do is we'll also have an alpha parameter here it is larable because you are initializing it as parameter so when it starts it is at 50% like you know both are equal but you know down the line it will adapt uh like you know which should be given more information so in forward how it works is you will calculate the Rel portion embedding you'll also calculate the absolute potion and coding both are calculated here okay this is probably the most simplest method to calculate the relative potion embedding okay get the relative positions and then uh call the relative position embedding Matrix okay that n and. embedding Matrix uh em embedding it is not a matrix it is a uh dictionary okay lookup table call the lookup table for the value and you'll get an encoding just multiply your Alpha value on the absolute portion embedding and also relative portion embedding and then add those that is how mixture works and if you see mixture right you will understand actually you know uh how it works you can see right we already saw sinusoidal if you see it was just zeros everywhere you know like here it was just simple lines and for uh relative we didn't even see anything it was just lines everywhere but here if you see I'm also able to see the cidal curve and you know like learnable portion embedding that kind of a structure is Al seen because I've almost provided each uh method like you know 50% so both are visible right so this is how uh you know relative portion embedding uh and mixture uh like you know absolute portion embedding uh together will work and yeah uh three methods are done now we are coming on to the advanced methods with the number one being rope still I think we can keep rope for last because uh rope is the most complex methods out of all of those we have seen um so we'll keep rope later first we'll see Alibi okay so in alii what we are doing is in initialization first we are initializing the linear projections and then uh we are calculating the Al by slope okay so first we are taking the M Matrix which is one to uh the number of hits and the slope here is going to become 1 by 2^ 8 into the value here and then divided by number of hits okay we are registering this slope as buffer because this is not a learnable parameter but when you in initialize is as a torch tensor right it will become a learnable uh parameter part of the training but you know we can't do that so that is why uh we are making M as a uh buffer in forward what it will do is first uh as usual query key values are you know projected to its weight Matrix and then you have the reshape and all so those are done and then your attention score calculation is done right now like I said you know that third step is done the fourth step is adding your Alibi bias on the position difference between each token and then apply the softmax and continue the same procedure now how it is done now adding the AL how it happens is uh first you calculating the absolute value negative absolute value of you know like -4 - 3 -2 minus one that kind of a matrix which we already saw right that Matrix is calculated here okay for the sequence length and then what you'll do is you'll multiply the self. M value we already saw that image right that is what is implemented here okay this will actually create uh 1 2 3 4 something like that and then we are minusing uh like you know subtracting it with the same value so that it will become negative at Dimension One okay so we have here you can see right minus of absolute so it will basically generate a value known as -4 - 3 -2 -1 something like that and here we are Computing the position difference actually okay one squeeze of zero and then squeeze of one this will provide the difference in position at Dimension One and zero um now basically you are multiplying this is your bias value okay and uh now you are multiplying it with self. M which will scale the bias with the slope and then you are adding it with the attention score okay that's all and there is a plot here and if you see the graph this will be a fun graph because like I said for each attention head you'll have uh different kind of an output right um like at initial stages it won't be a lot different but with a lot of training it will become a lot of different values and here if you see with Alibi um yeah it is very hard actually even to put it in here you can see in heads right if I mask this add didn't mask here you can see the Mas fill is not computed here okay um but at you know run time while we do the real time attention right what we'll do is we'll also mask it so basically it will be masked here okay on top of here everything will be blue and this is how it will look like okay um just focus on this here just consider this triangle is there and and that will be the position inviting Value First it will be close to zero and then it will be going like you know close to negative values uh because like I said the upper values will become you know um zero with mask filling this is how it works okay Alibi it is very simple uh but you know very efficient in nature now coming on to the most advanced method out of all of these uh which is rope so how does rope work here uh what we need to do is first we need that inverse frequency okay the frequency at which you need to turn for your potion encoding the Theta value and all for that inverse frequency is important um so that is calculated here and then we are uh keeping that as buffer so that you know like that isn't changed while training and uh here we have the sequence length and all okay in rotary person embedding first what you'll do is you'll compute um the sinusoidal value for each position by the inverse frequency okay so each position you have and then you'll multiply it with the inverse frequency that is what is this insm representing okay so basically this is a matrix multiplication where we are seeing this is the I here and this is the J here and we are seeing just multiply I and J together okay now the sinusoidal input for this sinid input we'll calculate the sign and cos okay so that is done here you can see we compute uh the sinus input by multiplying it with the inverse frequencies and this will provide you the rotation Matrix then rates a matrix and here what you will do is you will uh get the sign and cosine values of the sinusal input and these will be concatenated together the S components will occupy the first half and then cosine components is the second half okay that is how here we have done and here you can see the embedding is returned right but while attention how it works is um rotate half just keep it because we'll use it in a different place while attention uh we are first you know calculating the required values here and also we are initializing the rotary potion embedding basically it's not a embedding tensor so it is a generator so we are initializing that generator for the heading Dimension head Dimension and uh Max sequence length how it happens first uh input X is is obtained the batch value sequence length everything is obtained and then we are projecting it with the query key value and then we are reshaping it next the important step is to uh compute the position embedding so now you have the positions right you are getting the sequence length you will have one to uh n values or 0 to n values for the Z to n values you are getting a rotary embedding Matrix not rotary embedding okay that rotation embedding Matrix is obtained I told you right you'll calculate a rotation embedding Matrix and then we'll apply it on top of the query and key and that is what is happening here right first if you remember the COS of M Theta into Q Plus rotate half which will become negative right negative of Q into sin Theta okay similarly for it is cos Q cos K and uh like you know plus sin Kus k sorry you know like the rotation Matrix is already computed here and that is what is here okay so basically what you'll do is we'll split the tensar into two halves and the first uh negative half will be kept at first okay and the positive Val will be kept kept at second so that when you multiply it that formula which we said right will be applied okay the rotation Matrix is already there but how you want to apply it on top of your query the equation which we saw right that is uh how like you know you need to to compute it here and then now you are uh applying your qk transpose everything is normal okay so if you see aside from the rotation right all it is going to be similar to S and cos so the embedding uh generated or else the attention which you see should be similar to sinusoidal right um rope not there roty yeah um here you can see right see still it is relative right see there it was just uh some random lines which we didn't find any relation unit but here it is relative because if you see uh when it goes like you know towards the left the values are getting slowly you know uh becoming towards negative and also it is absolute in case of sign so it is both uh like you know relative and absolute that is what is shown with this atten I graph okay so I hope you all like this video I hope you all understood at least some Basics towards like you know different potion embedding methods in the next day we are going to see on the different types of attentions you can see the attentions is ready where again here uh we are going to see visualize the attention methods like how much attention each method will provide attention score and all okay the attention graph Heat Maps everything will be seen in the next day I hope you'll all like that video as well if you like this video please hit the like button share it with your friends and also let me know your thoughts in the comment section I'll see you on the next video Until Then happy learning hello everyone welcome to the day six in the 30 days llm Mastery course in day five we saw about the different positional encoding methods and its impact on attention there we would have used a simple atten mechanism known as multi-ad attention mechanism but there are different types of attention mechanism and in this video today we are going to see about the different attention mechanisms that exist and similar to the previous video we'll see the mathematical formulation and this uh video is going to be easier because position encoding is more mathematical and attention is more architectural the types so it will be more easier to understand the rep representations um so there are different types multi-ad multi-query grouped query sliding window disent detention every types will be explained as like in the previous video Pros cons why you should use it how it works and then finally the code with all the visualization everything will be done so stay till the end and have a fun journey ahead let's start with multi-ad detention the multi-ad detention is a key component and does that is the noble component which was introduced in the paper attention is all you need where it extends the idea of using the attention which is scale. product attention um to attend information from different uh representations of spaces at different position so what it means is that with attention right in multi head what we do is instead of focusing on one aspect we split it as multiple heads and then focus on multiple aspect right that is what is said here okay okay so here we'll use multiple heads to focus on multiple uh positions and multiple uh know information relationships and all so that is the core idea of multi-ad attention so what happens in the multi-head attention here I'm not going to speak about scale do product That Remains the Same everywhere right so here we'll be starting with the multi-ad which is concatenation okay from there we are going to start in this uh so here if you see this image will Prov you a lot of context actually okay the multi-head attention is where each query will have a key and then a value query key transpose into value but if you see multi-query this is multi-ad and this is multiquery okay multiple queries will have a single key and a value here a group of queries will have a key and a value that is group query and then this is multiquery okay this will explain uh simply about the three types of uh attention already okay um like you know in a definition kind of a manner we'll see it later but this is the difference between these three okay there are other two uh types which we'll see later but uh these are the main three types at top first okay so the multi-ad is is finally an attention type where we'll have multiple heads and each head will have an attention focus and all of those are concatenated together and will be multiplied with the w o which is the projection right and in each attention uh you will be calculating query key transpose and then uh it will be multiplied uh like you know query uh like you know query with the weight query and then key with the weight key and then value with the weight value all will be provided as inputs and then you'll uh follow this uh normal attention function which is soft Max of qk transpose by root of DK into V this is the conventional one right so why you can use this uh it is parall processing multiple attention heads can be computed simultaneously so it can focus on different uh features so it can uh capture diverse features the representation is better because it works in different subspaces and also the performance is better than uh single head attention always uh there is a saying right uh group is better than a single uh so that also applies here so how this will work first you'll project your qkv into H different sub spaces H different subspaces is number of HS okay and then you will apply scale dot product on each you'll concatenate those output and then a final linear projection will be uh combining all those information uh which you just stacked while you concatenated your output okay that is what happens in multi-ad attention and uh as far as the proc is concern uh it is parallel Crossing diverse feature capture uh improved representation uh enhanced performance also it is very flexible okay you can add any number of heads you want so with number of tasks increase increase the number of heads so that uh for task also it will focus in different manner with multi-ad uh the problems are it is highly complex compared to a single attention and also uh some hits might uh learn similar patterns which means those heads are redundant right like you know if I'm having 10 heads and three heads are say learning the same thing which means only seven hits are required here seven not rather seven eight because three are same one should be the unique one there so eight are the heads required there so there is a potential for redundancy and also uh understanding what each head learned will be kind of difficult but not you know so difficult still you can print the attention map of each head that is also possible uh but you know single letter attention is very direct but here you need to go one layer down and then find it it is memory intensive and uh the number of hits is going to be hyper parameter sensitive as far as comparison with other methods is concern uh across uh other methods versus single head attention this is going to be more uh better performance better feature capturing and all but you know with increased cost complexity while you compare it with multiquery attention uh query multiquery attention will have the same key values across every head so in multi head attention what we do is for each head we'll have a query and key and a value but in multiquery attention the query and uh query alone will be separate for each head but key values are going to be the same but in group query we are going to separate it as group group uh with some projections having Shad uh sliding window it is something very uh separate multi-ad attention will focus on entire sequence but sliding window will focus on local context okay it will have a window of uh View and in that window only it will be able to see uh when you compare it with the Descent angle attention it is another method where descent angle make multi-ad attention uses single attention mechanism but dangle attention will use use two uh attentions one is content based another one is position based okay so these are uh the differences between multi-head and the other mechanisms now let's jump into multiquery like I said you know like these are very simple okay the codes are going to be very simple the attention mechanisms are going to be very simple we actually there is going to be a lot of repeating okay if you see this differences itself right you'll be able to understand what will happen in behind so let's go still you know like let's go deep so multi attention if you see it's a variant of multi-ad attention to reduce the computational and memory requirements because if we see here uh the performance was good but you know like uh the memory intensive uh is an aspect which has been a problematic one so what they did was they said okay I'm going to have different queries know like query I'm not going to say like you know what I want is more important but key and value is going to say about like you know uh what are the things available and what is the value of that right um if I provide it uh even at different head right it doesn't make any sense because the context always Remains the Same what I want differs across no time steps right so what they did was okay will separate query for each but the key will be the same so here you can see right the input for attention head will be for example attention head one will have query head uh query one something like that okay but the key and value always Remains the Same for every attention head okay so here why should you use this it will have reduced memory usage compared to multi attention it will be faster to infer because you don't have uh multiple uh uh keys and values but since you reducing the keys and values right instead of having Eight Keys and values you're having only one it will be similar results but uh there will be a per performance degradation for sure uh comparable performance is what you will achieve you will not achieve the same performance as multi-ad but you know uh that could be a good tradeoff that was a good tradeoff when it came uh to the performance it was achieving okay uh and the speed it gave and then the reduced memory usage and all so how does it work first what you'll do is you'll project the query alone through the different subspaces and you'll project the key and value once to all the head like you know it will be shared across the head you'll just expand it's like you know having the same stack uh repeated against uh for every hits that is what is uh the second step and then you'll compute your dot product concatenate it and then apply a final linear of projection that is common okay so the pros are going to Remains the Same uh said like you know why we use uh reduce memory usage faster inference comparable performance improve throughput and also you need to work with uh fewer projection Matrix and also like you know fewer key KV Matrix right so it is going to be a simplified architecture then how was before but the cons is that uh it will limit the capacity of the model to some extent like I said and it is very Le less flexible because you have only one uh key value across every head and also um like you know interpretability will be challenging because KV uh is Shar so to understand you know how much it impacted that understanding is a bit harder okay other than that uh multiquery is going to remain the same so if you are more performance oriented you can try for multi-ad if you're more uh like you know memory related performance oriented uh you can go for multiquery as far as the comparison is concerned the comparison is going to remain the same okay multiquery reduced memory usage improves inference speed but uh Less in performance um multiquery shares all key values but here group query will have a group of keys and values for each h and that alone will be visible multiquery will have uh entire sequence context but sliding window local context multiquery single attention mechanism but uh disentangled uh content based and potion based attention is there now coming on to group query you see uh why group query came into the picture was multi-query and multi-ad seemed like both extremes okay one was focusing one on you know memory related performance and inference Rel related performance and the other one was you know like more like okay I'll focus on accuracy something like that so there came a method known as gqa uh training generalized multiquery Transformer model so what the method said was you know I'll try to balance between the multiquery and multi-ad I'll try to get the efficiency of memory and speed of multiquery and then I'll also not compromise on the performance still there will be a performance compromise let's say if there is a accuracy drop of 3% with multiquery it will be just 5% with group query okay so what here it will happen is instead of uh combining the heads right you'll combine the groups okay so each group will have number of HS so let's say you have um eight heads right what we'll do is we'll have four groups which means two heads will be there for each group and for those two you know like the attention will be normal so it is like adding a layer in between okay instead of doing uh let's say if it is multihead right it is like one group eight heads so if it is multiquery uh yeah like you know yeah multihead is the way you can compare it better uh with multi-ad it is just one group which has eight heads and all of those are ConEd together later but with uh group it is like you're splitting that one group into four groups which with each group having two queries and two keys and two values okay um so this two keys with same key and same value okay two query with same key and same value sorry for uh the confusion here you can see the mathematical representation right for that group the K and V will be same which means if I have four groups it will be four keys and four values in place of Eight Keys and eight values in case of multi-head attention and one key and one value in case of multi-query attention it is like bridging in between right so they have just added a middle layer kind of a thing uh with number of groups now why should you use group query attention it is balancing both it is improving efficiency it is also maintaining the expressiveness of multihead it is more flexible uh you can add the number of groups or reduce the number of groups based on the tasks you have and also uh you know it is very much compatible with any multi-ad attention pre-rain model you can just convert it easily and how this will work first divide the number of attention heads into groups like I said if it is eight let's say you want two uh key value per group okay so it will be four groups you will uh do query projection across uh each head in the group and then for each uh query it will have same key and value across the same group okay for across groups it will have different key values same group same key same value different group different key different value but everywhere different query okay um attention computation Hereafter is going to be same and what you'll do is you'll concatenate all the groups which means you're concatenating all the outputs of all heads and then final linear projection will be there here the pros and cons pros we already know and cons are going to be like you know the implementation is more complex because you are adding another layer and also finding the optimal number of groups though it is flexible it is harder to do and also uh this has been a relatively new method uh compared to the other two but you know uh nowadays people are using this a lot um but you know like other than this uh there are not a lot of cons so one could be said like you know it is not always that you will achieve the best tradeoff it's not like you will find the 50% at uh efficiency and performance so that is also there okay as far as the comparison to other methods I'm not even going to compare you all know uh right now it is memory uses reduced uh but also expressiveness is almost same uh to some extent so it is like balancing between these two again gqa have entire context length and also it only has one focus of attention this has two focus of attention dis disentangled okay so the three main handling types of uh attentions are done now there are two other types of attention for for your clearance these two are another type of like you know it's a different way of attention it is like different Focus kind of attention where these three can be implemented with sliding window okay so sliding window can be a group query sliding window attention sliding window can be uh multi-query uh attention Okay even sliding window could be a disentangled attention okay so these all five are you know inter usable that's why I said this is more architecturally you know like whenever you want just plug and play whatever you want okay so in sliding window what happens is you'll have a window of context length you want to see let's say you have 1024 you won't see the 1024 tokens rather you will see the tokens in your window okay so this was uh released in a paper sometime before but became more famous with mistr okay this is the image from mral paper itself so if I have a vanilla attention right it will see all the five tokens when it comes to fifth token but if my windows three it will just three see the last three tokens okay from a given position now you might wonder then I might lose the information right but if you remember we have multiple layers in which uh like you know each layer is coming up one above the other right we have multiple layers each layer would have have focused already so for example if I'm seeing uh this layer right the first four tokens would have been seen and that information would have been already there um to the second layer before sending it to the second layer so similarly it will be additive which means even the last head of the last layer will have the context from the like you know first uh token like you know even the last token will have the context of first token now let's see in detail about sliding window attention so what is sliding window attention sliding window attention is an attention mechanism that will restrict your attention computation to a local window around each token like I said if it is this token um always you know like we are not going to see the forward side right with llms mainly being decoder the forward side is always not going to be visible we are going to see the backward side and like you know in the previous tokens I'll be having one the the visibility only for the window of tokens I have now why do this like you know why do we have this actually is there a problem in this yes there are disadvantages which we'll see later but if you see the most of the context even like uh let's say 1024 the first token is not going to impact the, 24th token in odd cases it might but mostly you know like uh a context for a token will be there within like you know let's say 200 500 tokens something like that I'm not saying that small window will be used in training okay uh while training people usually keep around 248 kind of window even for now 496 is a very uh common window say nowadays because they wanted to scale to 128k imagine I want to process 128k and I'm Computing attention for 128k tokens together it will take a lot of computation and a lot more time okay so that is why people use uh sliding window attention in combination with group query please remember that combination with group query is very important okay um now mathematically how this works here uh we'll be applying softmax on keys and values which are local in the window okay my context is the previous one right so those will be localized upon the window s that is that is uh that goes without saying um why should you use Windows uh sliding window first it is efficient it will reduce time and memory complexity for long sequences it will capture the local dependencies uh effectively it is very scalable and also uh it incorporates a locality Biers this is both an advantage and disadvantage okay the tokens which are close to a token okay the token let's say I which is closer to a token J will have more importance and uh Effectiveness rather than a uh token which is let's say Y which is somewhere else okay as far as the compatibility is considered like you know this can be easily com uh combined with global attention Okay so there is a paper even like you know uh this will act as a local attention and there will be a global attention where what they will do is they'll take um the top 1,24 let's say my Global attention context length is 496 okay they'll take the 496 top tokens and that will act as a context there you know like less important words like rant words like uh stop words those all will be removed um but yeah like you know you can still combine with global attention easily how this attention will work first you will Define your window size so first Define the window size and then calculate attention within the that window alone uh make sure that you apply the mask uh to prevent attention before the before and after the window aggregation aggregate information only from the tokens within the window after you mask it right you'll have zeros and ones uh wherever it is one those information will alone be uh combined and if you have an Global attention you will uh have another attention which is global and that will allow certain tokens to be attended globally and those will be combined together added together okay so the pros we all know cons whatever we see as Pro right there is a con in there first limited Global context it can't see long range like I said you know uh yes there will be a generalization like I said here first token will be coming till the last but still the same layer seeing all the tokens is more better okay that will uh provide like you know some struggle to it fixed window size you know uh how can I say like you know the last 100 tokens is the correct window optimal window size will differ for each sequence and different parts of the sequences also uh going to come into the picture right so fixed window size is another issue there is a potential for information loss and this is not suitable for every task for example if it is a summarization right it is better to have an attention over o the whole context rather than a uh like in know window right so tax dependency is an issue uh but it is an underrated issue okay like no we won't consider this a lot and then there is implementation complexity uh it is more complex to implement efficiently you can implement it easily but it is more complex to Implement efficiently with GPU acceleration as you compare it with the other methods all of those are going to focus on local context uh sorry Global context while this is going to focus on local context that's all okay but there is another method which came uh which is also a variant of uh sliding window which came known as attention sync okay there was a research which said that you know like if you have those previous tokens right so the good thing is the first token usually apparently contains the information of all the tokens a lot okay uh in NLB itself uh like you know even with CV if you compare there's a token known as CLS token the first token will have the information of every other token and even to do a classification of image that CS token is more than enough okay um here just consider this token mostly has a context of all the token okay what they do is there will be a window okay this is the sliding window which we see okay this is window attention this is sliding window okay you'll move this window and here you'll add the first token alone along with this window token okay and this apparently provides the best result okay sliding window if you see already it is providing around 5.5 43 okay 5.43 is already a very good uh window but if you see when you go to atten this is for very long context if you want to go for very very long context right at that time screaming Alum which is that ining has 5.40 perity this is lower the better and this has the better uh the most uh best results you can say okay now coming on to the last type which is disentangled attention Okay so if you remember we'll always provide the positional embedding and the context embedding together first We'll add it right and then we'll Pro provided to the attention that is how it works but disentangle attention is a method where you will provide both of those separately okay so here you can see you'll have query key values for context embedding and query and key for location okay so this is a different method and this has been used a lot in document related models okay when you have document related models or models which you need to focus on localized information they'll use disentangled attention because the position embedding information is retained okay you're not just adding it you retaining it you're Computing an attention for the locations and saying like know at this location there is a high possibility that I have my value and that information is provided to the context as well when we add the context attention together this is how it works okay now we'll see uh like you know uh when we go down you will understand and also uh when you go to the code you will understand this even more better first desent angle attention uh is a mechanism where you will separate uh content based and position based components and this approach is to improve the models capability to both capture both content and positional information independently rather than adding both of those together if you keep it separately the position information is retained okay and also the context information both are unaffected right when you go to the attention so that is the advantage when you go to disentangled attention but like I said if you have a disentangled attention with a window it is going to become disentangled sliding window attention and if you have a disentangled attention with the sliding window and a group query it is going to become disentangled sliding window group query attention Okay all of these are mix and match you can't mix and match the first three okay the first three are separate types but with that first three these three these two can be add on so mathematically how it works first it is going to be the same qk transpose by root of d okay but here you will also have an attention for relative position okay there will be a relative position attention Matrix and then you'll uh once you do this uh you'll as usual multiply it with value okay uh the relative position attention Matrix is calculated by Alpha into R of I minus J which is a learned relative potion embedding which is nothing but the nn. embedding layer which we saw previously okay and then you'll add a beta as well now why should you use this desent angled detention first it will provide better representation because both are separate it enhances flexibility and also the interpretability is better because if you add both of those together you don't know what the model is focusing on now you can see how much the position is attended and how much is the context attended like you know how much it is impacting and also the position bias is reduced because you're not adding together okay now how this attention basically Works desent angle attention at uh real time first you will compute the uh content based attention which is the standard scale. product attention then you'll compute attention on the relative portion embeddings and then you'll combine those you'll just add those not like you know the combine combination is not something uh very new word and all it's just addition okay and normalization it is to apply soft Max on the probabilities you got from combination and then you'll do uh multiplication with value and then final attention is obtained okay as far as the pros is concerned uh it is going to remain the same but also uh the learnable parameters if you see it will allow you to uh adjust the importance of content to position okay if you want more learnable content parameters you can have a more embeddings for contents and less embeddings for position but if you follow the same approach as before right you need to add those together which means both of those should follow the same dimension okay but here you can just add a final linear layer of projection from the attention and then it would be easy to add okay so you can easily adjust the parameters where wherever you want to give more importance and the cons is that first it is more complex to implement uh since you are having additional computations for relative potion embedding like the attention computations it is going to be extra computation extra memory uh the sequence length is going to become a very important thing because you know um the relative po embedding which you use it is going to impact the attention a lot so the choice of sequence length is very important and also it is a bit hard to CH train okay and uh here if you see the comparison with other methods versus standard attention which adds the positional information before itself here you are having a potential to improve performance and tasks which require both content and vation that's why he said document related task okay and uh you know if you compare it with other methods as well uh like you know Transformer Excel which uses relative portion embedding here you are attending the relative ption embedding separately so theoretically it is better okay now let's see the programming part of this let's start with the most simplest method which is multi-ad attention Okay so right now we are seeing the multi-head attention which is the most simplest form of all how this works let's walk through the code as far as the initialization is concerned um first you'll initialize the D model which is your embedding Dimension the number of heads and the dimension for each head all of these are three important uh dimensions and you'll have a projection for K V and Q these are weight matrices basically okay guys please before coming into this code make sure that you have watched the Transformers video because because I'm not going to explain the attention computation process rather I'm going to explain the difference between these attention Okay the basic multi-ad and mass multi-ad uh all of those are explained already in the Transformers video in detail and I also code it from scratch the links are in uh the description so please check it out and if you want like you know the links are also in the playlist 30 days L Mastery course please check it out okay so here we are having the weight matrices which which with which we multiply the uh inputs so that it becomes the query key values and then the output projection and then we have a Dropout as well so what we do in the multi-ad attention first what I'll do is first I'll go through these steps and then I'll explain like you know in Step wise what happens in the code first you'll project your input query keys and values with those lineus layers which we created right now and we'll reshape the projected 10 s uh to separate across hits across heads compute the scaled AR attention apply the mask if it is provided so that you prevent attention to certain position for example if it is a cal mask you shouldn't see the future if it is padded you shouldn't see the padded token those kind of things and then calculate the attention probabilities uh which will provide the attention energy or attention score which will be multiplying with values to get the attention output and that atten output of each head is combined together and is projected back to the original dimension okay so that is the flow of multi-ad attention so here if you see first you getting the bat size and you're projecting your input to query Keys values here okay and now the shape is B size num head sequence length head Dimension from uh B size sequence length and embedding Dimension it became like this and then now you'll compute attention score across each HS okay and once it is done if the mask is there we'll apply that mask and then what we'll do is we'll apply a soft Max on this attention score which has the mask uh filled okay now the shape again Remains the Same bad size num heads sequence length sequence length Okay and since now the attention probabilities are calculated now we'll again do matter multiplication on this attention probabilities on the values okay and then we are making sure that those are continuous because all of those are random HS we are combining it together right so make sure that those are continuous once you multiply those and then uh we are viewing it to the same original one okay minus one self. D model again what we'll do is we'll project it to the original Dimension where it is now just stacked but this stack is converted into a meaningful information with this projection okay if you want to visualize the information here it is okay here we have a function to visualize as well python multi attention. pii CD day it is day six python I know we are going slow but you know um initial days it will be slow because it is lot more theoretical but once we move on to a practical section right it will be like so fast and if you want to print the attention values here you can see right the output is there you can just print the output rather than the shape I'm just printing the shape because I can see like you know if it is there or not and here you can see multi-head attention weights for head one at head one this is how the attention has been okay so closer to purple it is going to be zero and closer to Yellow it is going to be one okay so this is how the multi-head attention works now now coming on to the multiquery attention here the initialization of D model it's head Dimension is same but instead of having uh key projection and value projection uh differently here we are just initializing it at same okay so that you'll have two cross D model and then we'll split it later okay other than that initialization is same but here what we'll do is Project the input queries across uh using query projection layer and then key values using linear layer of Q KV projection and then we'll split those and then we'll uh rehap the the keys and values to align with the number of HS okay because if it is eight you need to expand it in such a way that one is available in all the eight HS you will compute your attention score everything Remains the Same after that okay first you'll get the bat size you're uh making sure that input is uh available everywhere now when you have the KV right you will split it as two okay and then uh what you are doing is your expand in it across HS okay that is what happens the same thing will be applied everywhere in every heads it will be available okay that is what happens in here and now if you see the shape will be like bat size num HS sequence length head Dimension this is what it has become here right after this it is going to be the same code okay not a lot of difference right but oh not this one if I put multi-query attention py even the attention Matrix is going to be something like kind of similar but you know like there it was more purple but these are random weights right so like I said all three multiquery multi-head and group query are just uh differentiated based on the performance side which means you can't see a lot of difference in the attention maps of each but when you go for the last two types it will be very interesting okay but with group query uh things gets a little bit complicated let's go through that first you'll make sure that the number of heads is divisible by number of groups because that is when you'll be able to split your uh heads across groups okay and then the normal initializations of Dem model and heads are there and then you are also getting the number of groups first you need to calculate the number of heads per group so what you'll do is you'll calculate the number of HS divided by number of groups which will say in one group how many H should be there and what is a head Dimension you will get it based on the number of hits you calculated okay which is already obtained now uh again we'll have q projection KV projection it is there what we'll do is we'll have the bat size which is normal we are projecting the query across everywhere okay basically across the heads right and then now you have your KV which is again uh available at every specific you know uh HS availabity is available at every hedge but right now what we doing is we are splitting it into groups okay now you are splitting it as groups and in group you'll have as heads okay rather than having it at every place you are having it for groups after that if you see um the touch multiplication of attention and then the uh attention being masked the Dropout uh being applied to the soft Max and then multiplying it with v and making sure that that those are contigous and then mapping it to the original Dimension and projection it uh to the original Dimension all Remains the Same the only difference is this place all right this will make it more computationally efficient I'm not even going to plot it uh you can just uh do Python gqa and it will provide almost the same plot but things get interesting with this method which is sliding window all right so what happens in sliding window first here you'll have Dem model number of H head Dimension that is same but you'll have a window size that is also a parameter other than that the normal uh projections are there how the process happens sliding window is first you'll get the bad size sequence length you will project your tars next you'll create a sliding window mask to restrict the attention this is very important okay and once the mask is there you will combine it uh like you know if there is a mask for padding or else Cal mask any mask like that you will combine it with the vision Window mask and then you'll apply it on the attention uh later okay at this uh step six okay you'll compute your attention score you'll apply the Window mask which is combined with the mask which if it is for attention mask and then uh the procedure remain the same okay you are getting the query key value and then what you're doing is we are creating the sliding window mask so basically what this will do is it will create one for every uh like you know one comma 1 so for example if it is three right 3 cross 3 it will be one everywhere but here what we are doing is we are creating a triu uh Matrix where what it will do is above the primary diagonal everything will become zero okay that is what is happening here we are creating the upper and lower triangular masks and then uh we are just you know um and squeezing it to add the batch and head Dimensions okay now the fourth step like I said if there is a mask already provided you will uh like you know combine those together you can't add it because ones and zeros right so you can't add it you'll multiply it so we are having an and operation here and then there is a normal uh function which is carried on but if you plot this right this will be very interesting to see python sliding window attention. py here you can see the attention is visible only till a window of tokens which is here and what is my window it is 16 I can only see till 16 tokens okay really it is very interesting okay so that is sliding window attention now coming on to the last method which is disentangled attention here if you see the initialization uh everything is similar here but other than that we are having having a position embedding parameter uh with di uh with a torch tensor being initialized of Dimension Max sequence length across head so at each head basically it happens right so that is why it is initialized like that and the weights are initialized with uh zavier uniform weight basically it will be like you know minus 3 to 3 that's all and then we are also providing the alpha and beta parameters the learnable parameters which I said Alpha into relative portioning plus beta that is the formula so we have the learnable parameters as well here and the function goes like this first you'll get the bat size projection and then you'll compute the content based attention score which is the normal scale do product attention and then what you'll do is you'll compute the position based attention score by using the relative positions which is available in this post embit okay and then what we'll do is we'll combine those okay so let's see how that happens first qkv is up you're doing your normal uh like you know qk transpose by root of D model that is done but if you remember this happens before soft Max okay that is how mathematically it was so first we are getting the relative positions and for that there is a function already okay here you can see so what we'll do is the sequence length which to which we want will slice the positional embedding and then what we'll do is we'll compute the relative positions uh scores like you know uh relative position scores by matrix multiplication and that will provide you what are the positions we want for this attention Okay now uh this attention we have it here right the position for attention we'll be multiplying this with Alpha parameter and then added with beta parameter and in turn this relative position attention is added to content attention and then the process Remains the Same okay and if I plot this again you will find something very interesting this Sangle atten um just give me a minute yeah here it is this is the total attention weights okay including content and uh position and here if you see this is the position based attention weights only for position okay you can see how much this has impacted here you can see the diagonal it has become like this previously if you see uh in case of the multiquery everything was just Violet to start with but here right like you know you can see a similar pattern of what you can see in here it has impacted to some extent right so that is what you get with disentangled attention Okay so yeah guys those was are the types of attentions which are there in the market right now popularly if you have any other methods you can just let me know in the comments I'll try to learn about it and I'll make a video if required so I hope you all like this video If you like this video please hit the like button share it with your friends if you haven't subscribed to the Channel please hit the Subscribe button hit the Bell icon I'll see you all in another video Until Then happy learning hello everyone welcome to the day seven of the LM Mastery course in this video I'm going to show you all how you can pre-train your own llm from ground up with the help of this Zero to Hero llm pre-training guide this guide is all you need for you to pre-train your llm today itself Yes you heard it right you can pre-rain your own llm today with the help of this guide even if you are an price or an individual doesn't matter let's start with the video now in pre-training N llm the most important part is creating a training pipeline rather than coding it framing a pipeline which is very scalable and also efficient enough for your model to perform well is very important so that your llm becomes very robust let's see how a training pipeline should be constructed for pre-training any custom l any preing of llm will contain five stages starting with data collection data processing training llm llm evaluation finding llm we'll be seeing these sections in detail where what you need to do in each of the section let's start with data collection what is data collection data collection is the process of gathering large volumes of diverse text Data with pre-training this definition can't be like more app this is the most AP definition when you come to data collection for pre-training because two keywords are very important you need large volumes of data and also it should be very diverse in nature because that is when your llm will understand what is the language and what are the semantics involved in the language how it varies syntactically everything is understood only when you provide diverse variety of large volumes of data so what what is the importance of this like I said this will form the foundation of your language model so if the foundation is not good you are not able to adapt for any of your use case so this is going to form the foundation for your llm what is the goal of the stage it is to acquire a broad range of human knowledge sample data so the data should represent a broad human knowledge and also how to use a language okay it should contain more about knowledge and also say like you know how to construct a language what are the sources in which you can collect a preing data first you can crawl it from the websites by using common crawl libraries or you can also use custom web scraping you can take books data sets uh from guten bag library or also you can go for uh Google Books there are some other options as well which you can uh check it out in open source uh data set hubs like hugging face and kaggle you'll have a wide variety of data those can be considered as well and if you are going for news kind of a data so that your llm gets updated with current affairs you can go for news apis and blogs and when you go for domain related for example what if I want to pre-train my own legal llm okay so I need to First find legal documents there are lots of documents like government reports uh SEC filings there are lots of reports like that which you can cover for your legal llm and when you go for a scientific kind of an llm you can obviously go for arive arive is open source and then you have PubMed as a big source for medical domain now what are the thing you need to consider when you are collecting your data first volume must must be in terabytes usually it will be in terabytes if you collect how it is supposed to be collected so here there are two challenges first you need to collect terabytes of Text data that is one you need to process terabytes of Text data that is two and then you need to store terabytes of Text data that is three okay all of these are something you need to consider and make an uh infrastructure such that it can handle this volume next like I said large volumes of diverse data so that becomes your next consideration make sure that your data is having wide variety of domains and dialects and if you are training multi- language multilingual model so it should contain multiple uh languages and even in multiple languages it should cover uh wide variety of domains and dialects that is important okay and then next we have the quality so when you are providing data like I said this is going to form the foundation of your llm so make sure that the quality is very high by removing the low quality spam content from your pre-training data set or even if you collect it from any data source we won't know right uh the reliability of those data set is always in question so make sure that that is considered and if you are working with uh some organization then lot of ethical considerations will uh come into the picture like for example you need to see the license of the data set and commonly you need to see the Privacy uh it is a very important ethical consideration make sure that no private informations are went for example personal user information and so on and so forth and also when you go for website datas there will be always a lot of bias inside that if you see in real world in Internet itself there are lots of bias which is shown uh in any blogs if you search for okay for example if you search a Content everyone is giving their perspective which means there is an indirect bias induced into it so make sure that those bias are reduced as much as possible so what are the challenges here when you're considering these like I said first storage and processing becomes a big Challenge and then when you have multiple data sources there is a high possibility that your data might be uh available in multiple sources same data in multiple sources so you need to write a complex logic to remove all those duplicates and then write a processing pipeline which will handle like different file formats regardless of how complex it is HTML PDF text CSV doesn't matter what kind of a file format it is and then file encoding so there are possibilities for example if you go to chbt it can generate emojis right so those it are able to generate because it is UTF 8 encoded so if you want your model also to do that so you need to make sure that you can process UTF encoded file format datas and all and then you have asky file format as well that is uh another format next very important thing is data sets with updated information if you want your llm to be relevant to the situation which is there in the current world you need to ensure that it is updated with a lot of information so that is why if you see uh any big organization ition will say this is their cut off date the more recent the cut off date the better it will be adapted to current world scenario and legal and ethics so you need to make sure that you abide by the copyright laws and also make sure that the personal information is anonymized so that you will use it for fair next you know how you can collect the data you know the challenges involved in it but like I said there are lots of processing which is required that is what we discussed here D duplication D duplication is one of the processing methods but there are many other processing because your data source is not always reliable so the next session is about data preprocessing how will you process your data so first what is data preprocessing transforming a raw text Data into a clean structured format which is suitable for model trading is known as data preprocessing again a short very descriptive and self-explanatory definition followed by which you have the importance of the stage I don't think we need to emphasize a lot on this important stage because how much ever we create a very complex architecture with whatever state of art technology it is there at that time if you don't have a good data you are not going to get good model right so imagine you are a very intelligent guy or like you are an intelligent person and you are teach teacher is teaching you something wrong okay you are so intelligent so you'll know okay like this is the teacher guideline so for me that is correct okay if I ask you something about that you'll be able to answer it because you're intelligent you can remember a lot you will answer me correctly according to your teacher but you are fed wrong information which means you're saying something wrong with high confidence that is not a mistake of you it's a mistake of your teacher right so similarly if you give wrong data or low quality data to the llm it is not the mistake of the llm to say wrong it's the mistake of the person who is creating that right so make sure that the data is quality insured okay so that is the most important uh aspect of data prepressing along with that you need to ensure consistency and relevancy your data should be consistent with your um use case in which you want to train it for for example if I want to train it for a legal domain and if I'm training it for a medical domain it doesn't make any sense right and let's say you instruction tuning it on medical domain but you pre-trained it with General domain how it will be able to adapt to the uh small terms which is not something it would have seen in the pretending stage right so to maintain that consistency a simple format for example one might be HTML one might be uh an XML one might be a normal text if you're giving different formats your LM will be confused what to do right so those are the things which are coming under consistency and relevancy like I said you need to make sure that your model is relevant right to any scenario so those are the importance of this stage what are your goals first removing noise and irrelevant information if that is irrelevant you'll just remove it and if it is a noise you'll remove it like I said if that is not something related to your use case just remove it again that is one example of relevance and consistency there are lots of other examples I'm just saying a domain specific nature because if you're going for a general pre-training right um maybe if you want to do a learning out of it you can pre-rain your own llm uh but most of the pre-training of llm will be happening only if you want a custom domain pre training so I'm more emphasizing on custom domain okay probably you know um in days upcoming days will custom pre-train our own llm for a custom domain of our choice right so let's see how that is done as well probably something like a financial domain or legal domain or Healthcare we'll try to do that maybe you can put it in the comment section on which domain you want the video to be okay so that is the first goal second standardized text format this is to ensure the consistency right so if you have a standardized text format it means your data is consistent among its uh samples and then next you have uh preparing data which is efficient for uh processing by the model so this ensures that your model has high data quality and it is very uh process very much processible by your model so in data processing uh one of the most important step is text cleaning okay okay in text cleaning there are lots of steps which you can do for example removing HTML tags removing stop words handling special characters either removing it or encoding it in some different format and then you have standardization of text um and then D duplication of content all of these uh fall under text cleaning and these are simple steps right you can just do uh like each one of these can be done with the help of Rex probably not D duplication D duplication is something you need to do uh with some complex Pro if you do a simple reex it won't work but others can be done with a simple reex itself now the next thing is vocabulary creation vocabulary is something like your models knowledge uh to some extent uh can be understood with help of the vocabulary right it is like if I ask you what are the words you know you list out your words right the more you know the better you can express what you know uh that is the idea of any language right the language is there to convey your knowledge so more the words you have better you'll be able to express what you know that can be understood by vocabulary so vocabulary creation all almost becomes a very crucial stage here what is vocabulary creation first it is a process of compiling unique tokens from the data set so whatever data set you have you'll take all those unique tokens and that becomes your vocabulary here you need to determine your vocabulary size first okay and for example uh only I I can remember only 25,000 words all right I'm speaking in Indian English here so if someone like an American is speaking you'll be able to understand that it is an American who is speaking here there are two reasons for that one is the accent of the person that is one one thing and the another one is the way they use their words it will be very much different than how an Indian will use his words right so vocabulary shows a lot of things and here what differentiates it their vocabulary is different my vocabulary is different I can speak Tamil I can speak English I can speak Hindi to a bit which means my vocabulary is this much okay so if I I'm going to learn new words I'll forget some of my old words right so determining that vocabulary size is very important how much words I can remember efficiently so common uh vocabulary size now it has become around 128k um but it differs um among the llms and what the use case demands so for example if it is a legal domain or a medical domain you need to make sure that though those technical terms are there as a single token right that will reduce the complexity of the llm to generate tokens correctly and also understand a sequence of tokens correctly for example if it is pneumonia in medical domain if you're having new as one uh token and then monia as another token may be mo and then yeah three tokens right three tokens understanding three tokens in a sequence is harder than understanding a single token so make sure that your vocabulary has you know uh the correct number of tokens because that is going to determine the knowledge of the llm and then uh hand out of VAB vocabulary words is very important so for this there are special tokens first padding you need to add those tokens padding is used for uh adding sequences to a fixed length because llms can be trained only with the fixed length across the bad size so if there is a length mismatch you need to add pad tokens and then you have Unk tokens which is to which is used to represent unknown and rare words and then we have the CLS token which is often used to uh as a special classification token basically this token will have an overall information about the whole sequence and then SCP token which is to separate different parts of the input for example I want to separate that this is a paragraph and then this is the next paragraph to show that we'll have a separate token next we have another important stage which is tokenization we have already made a detailed video on different types of tokenization which you can do so if you haven't checked that video please make sure that you have checked it because this is a very important stage um because you need to create your own tokenizer vocab creation also comes under tokenization because based on your vocabulary only tokenization will happen Okay so make sure that you watch that video The Links will be in the description so what is tokenization tokenization is the process of breaking the text into smaller units and this smaller units is known as tokens what are the types of tokenization which are there word based subword based and then character based these are the most three common uh formats of tokenization so while you're creating your tokenizer what you'll do is first you'll take the data you'll train it on your domain and then you will reduce the number of words to your vocabulary and then what you will do is you will uh pre- tokenize your sentence okay so it can involve uh normalization reducing the sequence length or so on and so forth once that is done you will perform your splits and then your vocabulary will be created okay and based on this vocabulary you will tokenize your uh data set at runtime and also at training okay so I hope you understood uh why vocabulary creation and tokenization are together so what is the importance of tokenization in the model performance first it will balance the vocabulary size with the token's informativeness like I said if it is a domain specific one right you need to make sure that your vocabul covers like you know the token information correctly if there is a technical term make sure that those are single tokens as much as possible or even at two tokens okay so and then we have another important step uh reason which is to handle out of vocabulary tokens so there are lots of ways you can handle out of vocabulary words and one of those ways is to use bite par en coding method again if you haven't watched the video please watch it because I've shown how B per encoding Works theoretically and also I've it programmatically okay so please make sure that you have watched that video Once you complete this video next you need to implement your tokenizers first either you can use prein tokenizers which are there in hugging face nltk Spacey depends on your use case so if you're going for word based or character based you have your options in Spacey and nltk but if you're going for subw kind of advanced to dooners you have your options in hugging face or you can use your own custom tokenizer that is more than fine next it is to encode your input at run time because you are now created your tokenizer you need to use this to encode your input at real time so what is this step all about it is about creating your tokens into numerical IDs so you'll have your vocabulary in your vocabulary each token will have a number to it a numerical ID so to speak which will be commonly turned termed as input ID okay and so basically it is to create the input IDs for your tokens and then once that is done you'll create the input sequence for the model itself where raw text will be converted into sequences of tokens IDs and uh then what you'll do is you'll handle the variable length inputs so like I said an llm needs every sequence to be of the same length in the batch okay across batches it is not important but inside the batch you need to have every input to be at the same length so if it is lesser than the length you want so it should be padded so if it is greater than the length you want it should be truncated and if you do padding you need to make sure the attention masks are there because that is the one which will say like you know you have padded these tokens so that becomes very important so when you're handling variable length inputs these will come into the picture and then next is to implement efficient encoding pipelines when you're having a large number of data for training it is very important that you paralyze this encoding process because when you're pre-training it the encoding process happens in CPU so you need to make sure that that is fast enough and it doesn't slow your pre-training pipeline okay and then another thing is when you have large number of data sets like I said you're using CPU and RAM you need to make sure that this process is optimized for memory uh which is RAM and then Crossing speed how to use the CPU efficiently so these are like you know runtime tokenization step or which is nor known as encoding input stage what are the ways you can augment your data because like I said diversity is very important right so how you can create that diversity that stage is named as data augmentation first what you can do is you can do synonym replacement so for that you can use something like word Nets or wordings to find the words which are closer to it which uh Place those words but doesn't change the semantic meaning okay semantically both are same but the words are different so that the model will understand okay it is not like I should always use the one word there I can also use another word to show that okay and then there is another stage known as back translation most of these data augmentation are for increasing the amount of data and make it more diverse okay so for example you have uh data what you can do is you you can translate to another language for example I have something in English I can translate it to Tamil and then back translate into English so depends on the knowledge of uh llm or whatever model I have for translation it will try to translate it in a different way okay same meaning is converted in a different uh sequence but both are right right so in this case you have two data which means you're increasing the volume of data and you're increasing the diversity of the data as well and then next comes random insertion or deletion so this step is known as know like noising or deising kind of stage where what you will do is you'll just insert random words from the vocabulary uh insert into your data set okay so it shouldn't make any sense because in real time when you work you don't explain your things correctly right for example if you give to char with some spelling mistakes or even some filler words it is able to understand it because it is trained in a robust manner that it can handle those kind of sequences as well okay that is what is going to say your different writing styles like I said Indians will have a lot of fillers in it and their English will be very simple but uh an Australian English is something very different than an Indian English which is very very different than an American English because they will be more eloquent right so to simulate these different writing styles you can just randomly insert and delete some words from the vocabulary but make sure that you do it uh correctly because if you do something wrong your llm is going to be like you know uh I'm not able to understand what you're trying to say here something like that now uh what are the importance for you to do the data augmentation you need to increase your data sitze and diversity that is one like I said already and then uh when you do these two your model like asset it will be able to handle various inputs so it is becoming more robust and since you have a variety it prevents overfitting as well and here I left a point for Generation you can also use bigger llms okay so for example proprietary apis like charb or Claud make sure that you follow their licenses that is there but those can be used as well now we are completed with the main stage which is data related data collection and data procing the next stage is to train your L the first stage in training the llm is designing your architecture so what can you do to design an LM architecture for you to know that you need to First understand the evaluation evolution of your language model architecture first it was just NR model just predict the next uh gr like you know two two words you will just try to understand and predict the next two words something like that then it came with the neural network architecture and then came Transformer architecture in 2017 and the world doesn't have revolutionized with llms like Lama 2 Mistral and then llama 3 there are a lot of things L lately there was a model relas known as Minal something like that there are lots of models which are released on these lines all right these are the ways the architectures are revolutionized so you need to make sure that you follow the evolution as well and adapt your llm to those Evolutions so the design goals so whatever goals uh these llms have to revolutionize the architectures and revolutionize the worlds again is first it needs to capture the long dependencies long range dependencies which is nothing but the Contex length longer the Contex length better it can remember a lot of things for a longer time right so that is very important second it should be scalable okay it should be scalable for billions of parameters the architecture should be scalable for billion parameters what does it mean here you see uh when you training your llm right make sure that you know why Transformers were released Transformers were released because it has this paralyzation capability okay without paralyzation if you're going to make it everything sequential it is not going to be uh easy for you to train a billion parameters okay make sure that your architecture is scalable in that way for example if you go through codes of Facebook right they would have used something known as row parallel linear column parallel linear so they would have done a lot of paralyzation they would have emphasized on that that is why they are one of the market leaders right and also your model should be able to process large amounts of data so what does that mean so if you are having a large amount of data right your model should be able to remember that much so the approximate SI that might involve the approximate sizing of your number of layers the embedding Dimension lot of things will come under finding the efficient processing of large amounts of data so as an overview for Transformer architecture it contains embeddings layer which will convert token IDs into D Vector representations which is nothing but embeddings personal encoding which will provide the sequence order information seeing like this token comes after this token and then you have multi-ad attention mechanism which will allow you to focus on different parts of the input and then you have feed forward uh which will process the attention output combin the information which was obtained from multiple attention heads together and then finally layer normalization which is to stabilize the training process so I'm just giving an overview here because I've made two detailed videos on Transformer architecture one was to make you understand how Transformers work and then another one is to implement that Transformers from scratch line by line where I would have shown the math equation and then I would have converted into code so if you haven't checked those two videos as well please make sure that you check those two videos for all the videos the playlist link is in the description So lately there have been a lot of architectural advancements so here are some of those the main components in feed forward netor sorry in your Transformers are positional encoding attention and feed forward right in all of these components there have been uh let's say huge number of advancements to increase the performance and the efficiency of the architecture what does it what does the efficiency of the architecture mean it is about uh the latency and the memory consumption when it comes to the efficiency and performance means the accuracy okay so here the position encoding for a uh for po encoding there are varieties of Po encoding and some of those important types are absolute encoding in which there are types like cidal binary algebra and then relative you have um Transformer Excel method shal method and so on and so forth in there and in those relative encoding methods there have been a huge number of advancements because with relative encoding only you'll be able to handle a longrange dependency of text all right so for those came rope and Alibi two of the most commonly used and most advanced methods and then uh to bridge the gap between absolute and relative there is mixture portion and coding as well don't worry about these types if you don't know about these types no worries Again the video links are in the description please make sure that you watch those whatever I speak about architectural advancements now for all of those architectural advancements I've explained it detailed in detail in the previous days okay so make sure that you watch that same goes for attention multi-ad multiquery Multi Group query so multier and group query came for increasing the efficiency of the architecture and sliding window and dled came for increasing the performance of the architecture okay in feet forward directly there is not a lot of optimization because feed forward is just linear layers but feet forward consists of activation functions and lately there have been a lot of work around activation functions there is a family of activation function which came recently that known as glue activation functions there is J glue and S glue which is common which are just blue versions of jelu and swish okay so you can also use jelu and swish those are very common but if you want you can also drive with j jlu and sgo the second thing is using those feed forward as Moe okay this is something very common right now if you have multiple languages to handle those people go for a mixture of export kind of an approach that can be done as well all right choice of advancement can be customized as well your requirement if you want it to be more on the performance side you'll go for a more aggressive approach and if you want it to be the more on the efficient side you'll be more focused on like you know reducing the complexity of the architecture reducing the memory memory consumption and so on and so forth make sure that you select the appropriate components next comes the model scaling now you have a basic architecture you need to scale the model right in that the first parameters to increasing the model size large language model the name suggest that it should be large right so what are you going to do to increase the model size first thing is to add more layers to increase the depth why you need to do that whenever you do something make sure that you ask the question why what when how if there is an answer possible for all the questions and in these kind of questions as steps right you need to ask why you need to do that first why you need to do adding of more layers to increase the depth I can just increase the embedding Dimension it would have been uh bigger right why you need to increase the depth it is to increase the reasoning level the more deep it goes it can drill down to different nuances to the minute bit okay so that is why you need to increase the depth if you see uh the architecture which are very good with reasoning right they'll have a very deep Network so even with small parameters people are now using 20 to 25 layers why is it important to have that much layers because the more down you go you'll go to the minute details okay but it is also important that you increase the embedding size or the hidden size because with increasing of the embedding or hidden size you'll capture more information you'll remember more information all right so that becomes a very important step as well so you need to find a tradeoff between these two like you know I have an optimal number of let's say three billion parameters I need to find an optimal number of uh layers and the hidden size of it okay these These are under the consideration that you want to create your own architecture if you're referring someone's architecture they have been already optimized so you need not do a lot of things there but if you want to do something by yourself these are some you need to consider and then next is to make sure that the trade-offs are correct between the model size and the computational requirements because with the increase in the model size your tradeoffs uh are going to be high because the computation requirements are always going to be a bottleneck right so make sure that those trade-offs are correctly managed and then we have efficient scaling techniques so don't worry if you are just like you know confused how I can make sure that these trade-offs are uh efficiently done because we are going to prein our own llm very soon so one once we do that you will understand uh as a code part how I'll do that so something like that okay next efficient scaling techniques when you have a large number of parameters you need to make sure that your model is scalable right what are the things you do you can do for that first add SP attention mechanisms if you go for a dense method always it is going to be more complex to compute so you can go for a sparse attention mechanism and then you can share your parameters across layers and also like I said is a very good method because at wrun time Only The Limited number of experts is going to be loaded in the memory now this is how uh you need to design your architecture you have your scalings uh part completed you need what are you know like what are the architectural advancements you need to include in your pipeline based on your requirement all of those are done right you're making it scalable next what you need to train this model right that is what we are going to see now so introduction to llm training so here we are focusing on pre trining because pre-training is the process of teaching the model to understand and generate language that is the main idea of pre-training okay any task you do in pre-training the aim of this task should be to understand and generate language SL domain because there are lots of scenarios where you will pre-train it just for the domain okay so for example I want to preate for medical domain something like that so it is to either pre train uh to understand and generate tokens related to The Domain or the language what is the importance of this step this is the core step in creating a versatile language model charity is very versatile right how it is possible because they correct they covered wide range of domain wide range of uh languages everything wide range of dialects and so on and so forth and they trained it on top of those versatile data set right so that is why again if you see I'm talking about training and emphasizing on data set so that is why data processing uh section had that much emphasis by having more number of uh time dedicated to it so what is the goal of pre-training here it is to develop a model which has a broad language understanding and you need to enable zero shot and few shot learning capabilities so if you for example if you give to CH if Char doesn't know something right even if it is not in the training data set it will be able to answer your question even if it is not able to answer it directly if you give some references to it it will be able to generalize itself so that is what is enabling zero shot and few shot learning capabilities next is to achieve good performance across various NLP tasks in NLP there are uh tasks like for example you can put um something like summarization translation question answering and so on and so forth as stask tasks you need to adaptable you need to be adaptable for all those tasks all right so that is very important what are the tasks in which you can prein on first there is mask language modeling where you'll uh randomly mask tokens in your input and the model will be trained to predict those tokens next is to predict the sequence so here the model will predict which is the sequence which will follow one another so by this it will have a sentence level understanding which marks language modeling it will have a word level understanding or token level understanding for that matter and then there is cost language modeling where what it will do is it will just try to predict the next token based on the previous token so now M language model also is about predicting random tokens and here it is about next token that is the difference in Mass language modeling you will do it use it for encoder models okay so what the task is is that the llm will be expected to understand and say what is the token based on the tokens before and after the mask token so that can be used with architectures like word t and so on and so forth but costal language modeling is about ensuring that your llm can say what is the next best token based on the previous token okay now let's see about cost language modeling in detail because that is the task when which we'll pre-rain our own llm what is cost language modeling but cost language modeling is a task where the model model predicts the next token in a sequence using the preceding tokens so whatever tokens are there before it those are the costs see that is what is the good thing about technical terms mostly it will be um you know self-explanatory so to speak cost sell language modeling based on the cost your language model will say the next token right example the sky is if I say the sky is right most of you would say like the sky is blue the sky is beautiful something like that all of you say that because the sky is is a common sentence but if in future I would have said something like the sky is blue but with some Cloud covering mask Cloud covering okay now you'll say like black right how why is that but with some if I just left it without cloud covering you would say something else so if you give a context to what is the future it becomes MK language modeling which is an easier task to do because it has the context of what it needs to generate and then it will generate accordingly or predict rather here it is to generate right so it is a bit harder next why is this task important this is the task which is core to many NLP tasks and almost any task can be uh ptle down to a auto regressive task some of the most common tasks in NLP uh which can be done with the help of cost language modeling are normal text generation you can make chat Bots you can make Auto compilations and so on and so forth why you need to do this like I said this is cod to many LP Tas that is one and it will also enable your model to learn the structure grammar and context because if you see if you are speaking a language right you'll say it in a sequence you'll not think it forward and then answer it right you you won't do that you think it and then you'll just deliver it so your understanding of language as a structure as a grammar as a context is like a decoder model which will work on a cal language modeling task not like an encoder model okay so how does this work like I said it is an auto regressive process so it will predict tokens one by one so here in this task we'll train the model to minimize the cross entropy loss the cross entropy loss will say how much uh your model is uh differing from predicting the best token at a given time stamp okay because if you see language modeling if you uh see it is just classification at the final layer okay it will try to classify what is the next token okay um here we'll use only past tokens like I said that is what it makes it auto regressive and applications it can be text generation you can use it with uh Auto completion and so on and so forth but the challenge is like I said it is a more harder task because it has limited context and when you speak only the past there is an inbuilt uh uh biased to it okay if you don't explain why you are saying one thing in the future you are going to be biased to it all right so there is an inbuilt bias as well now with training of the nlm right in a training loob it is not just that you'll have your uh llm you'll have uh other things like loss functions optimizers your uh learning rate schedulers everything will be there you need to make sure that you have initialized all of those correctly so the first thing is cross entropy law for LW we'll use cross entropy loss because that is a standard loss function for classification tasks and here it is just that your model is going to predict or classify the next token based on the previous token okay so that becomes the standard loss for your uh costal language modeling training and here uh this will be applied for token wise in language modeling okay so at each token you'll try to apply this and that is very important okay and then next you have optimization algorithm optimization algorithm is responsible for the back propagation and updation of weights and here there are two options either you can use Adam which is a normal uh Optimizer or a variant of it with the we DK and that is admw and for learning rate scheduling we just change the learning rate schedule uh learning rate in the future for example even if you are learning right at initial stages you learn it faster but going uh forward you'll learn it slower because you know a lot of things so you'll take time to understand those complex Concepts uh slowly and for that you'll have a learning rate slower right so that is done with the help of the learning rate scheduling method and for that uh you need to choose your warming warm-up period which will uh gradually increase across uh time and then you have your DK strategies as well which will basically reduce your learning rat SCH learning rates okay uh linear cosine step DK there are lot of methods for that as far as the training process is concerned um this is how a simple pyos training Lo will look like first you'll pass the data through the model for a number of EPO and then uh you will pass that data through the model which will perform the forward function forward function is to just uh get the model output and based on the model output which is known as logits you'll calculate the loss values you'll uh make sure that the optim optimizer gradients are zero and then uh you will perform your back propagation once that is done you'll step Optimizer which will update the uh model parameters based on the law gradients you calculated with the l. backward and then the process will repeat this is how a typical pyas training Loop will look like for any training uh processes even if it is an llm or not doesn't matter the process Remains the Same and for llm also the training Loop Remains the Same now for the training process details in detail right we already saw uh the training Loop in simple what happens the first step is to batching and input preparation so for this you'll use data set and data loader okay so the data set and data loader is basically used to create many batches of sequences which are processed tokenized and so on and so forth uh which is applied with padding masks attention masks uh everything like you know everything is done and it is ready for model next uh you coming on to the for pass where you will send the model which sorry send the data which you created uh by batching and then you'll send it to the model right that is forward pass you'll get the model predictions which are loggs and then you'll calculate the loss with the help of it comparing it to the ground truth and what you'll do is you will aggregate the loss across the batch so for a batch let's say you have 10 sequences for 10 sequences together you'll have a loss and then your back propagation will start where you will compute gradients for the loss with respect to the model par meters and for this you can use automatic differentiation Frameworks like pyto senser flow so pyto senser flow uh they by default have that automatic differentiation running behind that is what it makes them a good deep planning framework right because uh back propagation is one of the most important aspects of uh deep learning and here this makes them uh very good framework for deep learning next we have parameter update where your back propagation is done your gradients are calculated now you are ready to update your model parameters which is nothing but the weights and biases so that your loss is ready here what you'll do is you'll Implement a gradient clipping as well so that you can prevent exploding gradient issues now we are coming on to the some real stuffs um those training Loop we discussed till now it is very common right but with pre-training you have a thing which is very high uh computation okay so you have a lot of bottleneck to it there comes the distributed training strategies you will train it in a distributed manner uh with a lot of gpus being involved in there okay so for example if I'm not wrong uh 32,000 gpus were used for llama 3 training if I'm not wrong 32,000 H okay so the hardware requirements are very high so you need high performance gpus or dpus and also you need fast interconnective Network bands for your distribut training because the data will be distributed through these networks so there are uh methods like like you know at behind in torch and all there is NV link and all okay and there is infin band that is another option and you also require a large amount of RAM and fast storage because let me tell you how DDP Works in simple so DDP is a method where you'll create instances of your llm across gpus and you will create instances of your data sets across gpus again all right so across gpus now your data is shared right when you have 10 data tokenized together you're using the same CPU okay so it will consume a large amount of ram large amount of storage and so on and so forth so your Hardware re requirements are going to be so high next coming on to the software stock what is the stack you need to you're going to use for your training first you need to choose your deep learning framework what is the Deep learning framework in which you're going to train your llm on so like I said there is options like py tens of flow pyou being my favorite you can also go for tensorflow that is completely your wish next comes an important uh library in your stat which is distributed training Library what is what are the things you're going to use for your distributor training there is Torin which is a default distributor training library of by touch but my favorite is deep speed Because deep speed handles a lot of things like for example if your memory is not enough they'll do something like CPU offloading and all making sure that uh your model is fitting in those consumer grade limited gpus that is why I said we'll pre your llm regardless of what is your GPU because we are going to try to pre train an llm in collab go collab okay we'll write the code in uh vs code but we'll train it in Google collab um next uh these are going to be trained for months uh weeks or even months like I said so you need to track your experiments right how the loss converges how is your gradient uh coming up so all of these needs to be noted for that you can use tracking tools like ml flow weights and biases or the most common one which is densa board also you need to decide whether you're going for cloud or on premises infrastructure which on your flexibility cost and control okay so if you want more flexibility less cost less control you'll go for cloud if you want less flexibility more control cost is doesn't matter then you'll go for a cloud uh on premise infrastructure that depends on your requirement right next like I said you'll do a lot of tracking right what are the things you'll track you'll track the training LW validation loss perplexity perplexity is a parameter which will say how much your model is confused when it says a new word for example if I'm saying uh let's say you understand Hindi uh or any languages to some extent certain extent right but people are now speaking it with English a lot so if I see some word new for example in French okay I I have a limited knowledge in French let's say if I see a new word I'll try to understand what could be the new word based on The Words which was available to me and if there is some English words in between I'll get the context from there right so my confusion rate will be lesser right so that is perplexity how much your model is perplexed seeing a new word so lesser the value better the performance and then you will monitor your learning rates lesser the better and then gradients as well how can you debug your model first you can check the gradients are correct okay it's not exploded or something like that let's say like if it is hundreds and thousands it means there is something going on wrong so you need to normalize something like that and also you can visualize the attention weights so mostly like you know uh it is more easier to visualize the attention weights with images with text it is it is a bit harder okay in attention Maps you'll see like you know where your llm is focusing because uh to see like you know where your model is focusing we are using C like the word as C which means you are having some Vision to it so visualization of attention weights with uh images are more easier than text but still you can do it for text as well and then there are possibilities for training instabilities so how you can handle that is first you can use gradient accumulation steps so gradient accumulation steps will ensure that uh your training time is getting lesser so basically the idea is that instead of doing back progression at every step you'll do it uh let's say after four steps okay something like that uh is what gradient accomplishment steps until then the gradient will be accumulated just uh accumulation is nothing but just getting added together okay and then you have mixed pression training so like again this is to make it more efficient and then uh make sure that your initialization of model parameters is very uh carefully you have initialized it because if you don't initialize it very well you'll get a lot of issues later and your loss will be something like n okay loss will be n or zero like you know if the gradient becomes n your loss will be zero so so your model will not get trained another important aspect while you do pre-training is making sure that there is a checkpointing and resuming training uh capability in the pipeline because regardless of how much you handle the training pipeline correctly there is a high possibility that you will get some issues and the pre training will be stopped right so you should be able to resume your training from that stage for example you trained it for 2 weeks okay and then there is a sudden issue and then your model got stopped maybe it is because of data maybe it is because of ram overload maybe it is because someone restart your server anything right so you should be able to restart it so for that you need to First ensure that there is a regular model checkpointing done where you need to save your model weights along with Optimizer States because without op Optimizer States you can't uh resume your training okay so the frequency concentation should be there because reading and writing is always something you need to consider if you are writing let's say in every 100 steps it is going to take some time for it right so make sure that there is an optimal uh number where you are trying to save mostly I feel like you know uh one or like let's say 2 to 3% of steps is an optimal number is what I feel let's say you are training it for 10,000 steps right 1 person would be 100 if I'm not wrong yeah so 200 uh steps once is something very good number while it comes to pre trining okay and then uh make sure that you check you save the checkpoints in an efficient format for example shter checkpoints is something uh very good because uh it will be able to load efficiently across gpus and uh preing is something we do it in multi GPU right so that is something you need to consider and also make sure that you have compression techniques to store across uh based on your storage requirement and then like I said resuming training uh should be uh something you need to enable in your pipeline if you're using trainers like hugging phase or pouch uh uh lightning those are something which have that capability by deta by default but if you're doing something by yourself then you need to make sure that that is handled correctly so for that you need to handle your learning gr schul data iterators and also making sure that the reproducibility of results is there like setting up the seed values correctly now we are now we are done with llm training but like I said llm pre-training is a very big process all right what if your model is not performing accordingly like you know you're expecting a result but down the line you're seeing after let's say 5 weeks or six weeks of training you're seeing that your model is not working your six weeks are wasted right so that is why people evaluate the llm at run time so there is a checkpoint saved they'll try to run on an evaluation pipeline how we need to do that we'll see now so before we go into LM evaluation I would just like to ask you all to just subscribe to the Channel if you haven't subscribed and hit the Bell icon to get the notifications to get uh workshops kind of video like these because here you can see we have almost if you'll think like you know we have almost come to the end of the video right yeah like evaluation is come but no no no you haven't just see you out of 60 62 slides just 36 is completed which means you are around halfway there right that is why I said this video is all you need for you to pre-train your own llm right so what are the methods in which you can pre-train your llm sorry evaluate your llm which is pre-train yeah that is that is the correct one there are five main methods one it is to use human evaluation the second one is to use llm assisted evaluation third one is to use match based evaluation the fourth one is to use benchmarks and then four fifth one is to use use common metrics okay thanks to arra AI for uploading such a good image uh to make my life easier explaining you all about evaluation because if you see um if it is green it means it is very easy if it is yellow it is like not easy not tough and if it is red it is bad okay when it comes to cost if you see the most efficient method is to use metrics in every way it is to use metrics versatility because you must the the metric you have right for example if it is accuracy or your recall you can rely on it but it is not versatile enough right so that is metrix for you all when it come to humans the cost is going to be very high imagine hiring a human to evaluate the llm outputs for let's say 6 months you'll be paying him monthly salary right so human evaluation is always costly but if you see that is the most versatile method right it is the most versatile method and the most reliable in my opinion okay after you have something like metrics and all because metrics are something which you have defined so you'll rely on that all as much as you rely on metrics you can also rely on human but human is given yellow here because human has their bias all right for example if I'm asking you who is your favorite cricketer or something like if your favorite cricketer let's say x is a worst cricketer is what the llm says what you will do no how can he say that it's wrong right so that's where humans become less reliable to some extent because the bias is there right when you go to llm assist State evaluation the cost becomes lesser than human the scalability becomes less issue than uh humans um still it is very versatile as much as human but the bias will be a bit higher right because here there is the bias of 1 person but the llms which are already there and trained it has the bias of Y people for example if Yen people worships one person as a good person if the reality is that he's a bad person that LM won't accept even if our llm says like know it is a bad person right so that is the issue with llm assisted evaluation next we have match based um as much as uh metrics is there match based is also a very good method um it is like exact match that's all okay using reg X or exact matches is what uh match based methods are and then benchmarks it's like um how can I put it um if you have no way right to incorporate any of these methods then you'll go for benchmarks benchmarks should be your like you know least starting point so to speak because it is average in everything it will be average in cost it will be average in scalability it will be average in versatility average in reliability right because all are done by someone now what is the importance of llm evaluation it is to assess the model performance and capabilities because if you don't do that what is the point of training you know if I'm if I'm going to say I just trained it correctly without any backing for it assessing the model performance and it fails in real time what is the point of training right and while you evaluate efficiently only you will know the strengths and weaknesses how is that because when you're evaluating and if you find that your model is failing at one place and it is working at one place it means failing is weakness and working is strength right so identifying that is an issue and then you have have uh guiding further improvements and iteration because even if in if you see in Lama 3.2 paper they would have emphasized on iterative training right so iterative training works on the fact that you will identify the strengths and weaknesses and based on those weaknesses you will suggest improvements and that improvements will be uh used for further stages of training and then you should ensure that there is a responsible a development for example NSFW content should not be generated something like that right so ensuring these also comes under llm evaluation because uh you are um obligated to follow some mythics right so that is why next what are perplexity and what are the other loss metrics which you can use like I said uh while monitoring you can use perplexity and perplexity is defined as technically uh exponential of the Cross entropy loss okay so the interpretation is that um lower the perplexity better the model performance is here the limitations is that it is not always correlated to the downstream task right um because it is just going to see how much it is confused how much it follows the task it doesn't matter for it so that becomes a limitation for it so that is why perplexity is used as just say another Advanced version of Cl function okay and then you'll also monitor your training and validation loss that is there what are the Benchmark data sets and leaderboards you can use first you can use glue and super glue that is something very common it is to uh show show like you know see how much your model understands natural language it includes uh task like sentiment analysis textual entailment and question answering and so on and so forth there is another variant of this data set itself which is super glue and that is a more challenging uh form of glue let's put it that way because it will emphasize more on the complex reasoning tasks okay so here you know the understanding next how much it has reading comprehension like you know how much it can understand the context and then answer based on it for that you can do a question answering kind of an evaluation based on context and for that you have Squad Stanford question answering data set next there is Lambada which is language modeling Benchmark I don't know why they kept it as Lambada still I don't know I call it as Lambda actually so what is this Lambada Benchmark used for it is used to test the models capability to understand long range dependencies because if you see one of the aspects of or the goals of LM pretraining is to make sure that the llm understands and holds context for a long length right so that can be ensured with the help of Lambada next you want to make your model multilingual all right so multilingual benchmarks are there extreme X glue these are benchmarks which are used for crosslingual understanding and then you have uh emerging benchmarks new benchmarks uh like big bench which will see like you know how much your model has a wide range of knowledge because big bench has different uh tasks uh and that is on different domains okay so big bench you can just see it as uh domain related uh Benchmark because if you see multilingual related Benchmark is Extreme glue X glue and then uh for long context you have Lambada for um natural language understanding you have glue for reading comprehension you have Squad and then for domain understanding you have a big bench and uh adapting to wide variety of Downstream tasks can be done with the help of hel which is known as a holistic evaluation uh framework for language models right it has around 40 tasks apparently now if you see what and all goals we had for pre-training right all metrics are evaluated and now when you have a task specific evaluation and a downstream level right when you have question answering you can use metrics like exact match or F1 score exact match is nothing but accuracy score okay and for data sets you have Squad natural questions trivia QA Hotpot QA and so on and so forth and here you will evaluate both answers correctness and relevance for text summarization you have Rog which is abbreviated as recall oriented understood for gting evaluation so so this is basically actually created for uh summarization itself this metric was created for that and this can be used for you to assess uh how much the data is compressed and what content is selected is to uh like you know to make it compressed and the fluency in which the compressed content is expressed so for that you can use Rog and for data sets of evaluation you can use CN and daily XM MLM if you are using multilingual tasks right next if it is translation you can use blue mutar and then uh for data sets you can use WMT for name recognition Precision recall Fone score these can be used uh which is to identify the models ability to identify and classify named entities for this you can use conal data set now coming on to human evaluation here if you see actually we are going through each of those method right we already seen benchmarks we already see some metrics now we are seeing human evaluation so why is human evaluation or human judgment important because humans can capture uh something which automated metrics might miss for example I might have shared a word which represents the same answer but the exact answer would have been different right so the automated metric will say no I'm wrong but technically I'm right correct so that is why human judgment is important and also assessing overall quality and coherence of model output is done better with the help of human rather than an LM or any other methods for that matter anything which even comes closer to assessing the overall quality or coherence is llm evaluation that is why these two are very versatile all right as far as evaluation Protocols are concerned you need to First have a clear evaluation criteria making sure that there is very less room for bias or margin of error because human tends to make mistakes same goes for llms so make sure that your evaluation criteria is clear and also you need to make sure that your evaluators are trained for consistency they should provide a consistent result right for one thing they're saying one uh kind of result and then for a similar thing you should have said a similar result but they're saying out another result so that consistency Miss should not be there so make sure that your evaluators know what kind of evaluation should be done that consistency is very important now types of IM evalation what can you do first direct assessment so whatever model like you know for example our LM provides output we'll evaluate those that is one and then comparative evaluation so we'll have different models ranking the outputs of ourm for example let's say charb CLA AI and then uh CL mean Sonet and then gini all of these will evaluate ourm and we'll evaluate those evaluation okay based on like that we'll be just ranking it and then uh error analysis where and where and all our model failed we'll just analyze and categorize those mistakes so that we can retrain accordingly here the challenges are uh the subjectivity because humans are uh subjective not objective they have their own biases and then their own uh principles philosophies so to speak and that will always have a impact on their judgment and then it is highly cost uh like you know costly and then highly time intensive and also scaling human evaluation to last dat asset is very hard the ethical considerations you have is eval evaluating your model outputs for gender racial and other biases because these kind of bias will be there in common we website datas so you need to ensure that your model is not bi biased on these ways and then you have data sets for these like you know to check if your model is biased for that you can check it against V bias kind of data set and all and then uh you need to see how much it is uh Fair across different demography groups so you can just say like you know I'll be just doing English even if you go for multilingual yes there will be a low Source language but still you should be demographically balanced to some extent then comes uh privacy and security implications so you need not you should not leak any training data to the user and also uh it should be vulnerable uh like you know your vulnerability should be very very less to prompt injunctions like for example Jail uh jail breaking is a method where it will uh break the characteristics of the llm and it will be allowing the llm to speak in the way it was uh not allowed to speak so it was not uh protective and was enough like you know it was vulnerable to that prompt so you need to assess those vulnerabilities and then also it should be truthful and factually accurate so that you can uh reduce hallucination for that you can use benchmarks like truthful QA which will allows you to assess the model tendency to generate false or misleading information now why you need to do evaluation it is to iteratively improve your l so for that first you need to analyze the evaluation results where you'll try to identify the patterns in the model errors and weaknesses and you will prioritize those areas which require Improvement based on your goals okay and based on that you'll say like you know these is are the categories I need to improve and you'll say that to the training people and then they'll train accordingly and then there is also data Centric Improvement so what you will do is wherever you find that there is a weakness you'll augment those training data in those uh situations and then you'll train it for more number of times there for example in multilingual let's say one language is not doing well you will increase the distribution of the that language so that the model gives more important to that language and then also if you identify some Biers you will filter those data so that those bias are not there after that when you come to model Centric improvements you will find unit for specific domains or tasks uh whenever you see see in the future and then also you will uh experiment with uh artical architectural modifications so like I said whenever there is a new architecture advancement comes you can try to plug in play with your llm also hyperparameter optimization so it is about uh finding the optimal number of layers optimal uh embedding Dimension and so on and so forth that comes under optim hyper parameter optimization and you need to find a balance between your computational efficiency and your performance continuous evaluation like I said uh if you do all of these as a pipeline it will be an automated evaluation Pipeline and then you'll get the model performance and then you will uh improve it over time okay so that is what is uh your evaluation site now the aim of any llm it is to fine tune to a downstream task right you need to adapt it to a scenario even to check how much it has uh adaptability right what people do is they'll just fine-tune the llm for a downstream task okay so here in our case like I said we'll follow this guide and then we'll try to pre-rain our own llm okay so what you what we'll do is we'll use the same llm to fine tune llm for a classification task so fine tuning is a uh stage where you will adapt a pre-trained LM for a specific task so what is advantage of using fine tuning over a model which is stained from scratch already is that you'll have a good language understanding already uh SP task specific data will be less required because it already knows the language and syntax and semantics which is involved in the language and since uh these are there there will be a faster convergence and and then there is a potential for better performance okay so here our goal of fing is to determine the sentiment and this can be used in customer feedback analysis social media Etc so what we can do is maybe you know uh try to make a real time emotion analysis kind of a model okay so for this what we can do is uh we can prepare a sentiment data set uh where what we can what we need to do is we need to collect a label sentiment data set and then uh we need to process those and we need to make sure that there is uh imbalance being handled create rain test and then we'll find unit so for find uning what we need to do is usually we need to add a classification head because initially it will be uh projecting it to the end tokens which are possible next so if we are using it as just a classifier right next token I mean like what classify it is we need to add a classification head that is usual okay but we won't do that we'll make that classification itself a generator one and there is another option which is to freeze some layers so either you can freeze or unfreeze the layers that is completely upon choice and also choosing appropriate learning grades like I said those are hyper parameter optimization so if you are having uh pre-trained layers you can keep a lower learning rate because it is already trained it will take some time to adapt but if you are having some new layers attached have a higher learning rate for those and then selecting your batch size and number of eox uh that is also there keep your number of eox very less okay and when you are training the sentiment classifi your loss function should be cross and loss uh like I said we are making it generative so we'll use cross and loss but what if you want to just say like you know the class alone so if it is just binary you can use uh BC loss binary cross and loss and if there is a lot of class imbalance you can try to use focal loss to handle that class imbalance I mean as far as the loss function is concerned but you know uh for handling class imbalance like I said one is focal loss another one is to adjust the class weights itself so there is a method where you can provide weights for lesser uh number of values being in the class for that uh there is a method you can do that as well so here also we'll uh monitoring monitor the training process so here we'll not monitor the perplexity kind of metrics well here we'll monitor the training laws validation laws and then we'll see the classification accuracy and relevant metrics like F1 score Precision recall and so on and so forth um here we have early stopping medic models early stopping at all where you know since we are pretraining the model to avoid war fting we'll have early stopping at all so some of the advanced uh functioning strategies are like I said like you know uh will train the whole LM okay but if you are freezing right you can gradually unfreeze few of the layers uh in future so that is something which you can do and then also you can do mix pression training knowledge distillation is something uh a very good method we'll see that later okay we have a separate day for that um so these are some of the methods I'm not going to focus on a lot on fine tuning because fine tuning we have a separate section for that as well but we are considering that also in the pre-training because that is how you'll be able to uh verify that your model has the capability to adapt to a downstream task okay that is the aim of this fine tuning so the metrics we have are accuracy precision confusion metrics and you can do cross validation as well if you want to do that now let's say we are uh ready with the model what are the considerations I have for deployment of this model first I need to see if the model size is good okay if it is not good enough I need to prune to reduce less important weights I I can quantize to reduce the Precision of the weights this all will reduce the size of the model or what I can do is I can already create a smaller model and then train uh by distillation method so that it becomes Deployable in real time inference uh plays a lot of important role um so optimizing it for latency uh is very important so like you know latency should be very less and thrw put should be very high uh making that to be ensured is very important and also making sure that uh Hardware acceleration is leveraged for that you need to use Library like tensor RT onx R time there is llama CVP and so on and so forth right when you are deploying right in real time there there might be uh existing systems so you need to make sure that your API which you are developing uh like for your model serving that should be compatible with those systems and also uh you should employ uh logging and monitoring to see your model working at the real time and also monitor like you know whenever it fails you can update it uh continuously so you can Implement strategies for that as well now coming on to the challenge challenges and what can be done in the future the challenges in the llm development are that uh you have a computational resource uh and energy consumption as an issue like I said it is a large scale training so computational resources an issue bias in the training data is there will be there how you can handle that is another important issue usually llms are blackbox so making it very explainable and interpretable understanding how the model decides that this is my next token that is a challenge and Hallucination is another challenge so what are are the trends and future direction in which the world is moving right now one is continuous learning right so we have Laura where it will try to address the challenge of uh catastrophic forgetting so that you can continuously make your model learn for the future and then more uh uh adaptable to few shot and zero shot so so that you know like with less samples you can adapt it for new tasks and then U making the model more ethically responsible finally the conclusion as far as the recap is concerned we have seen from data collection to fine tuning a sentiment classifier everything is covered and the key T if you would have seen we have given more importance to high quality and diverse data making sure that you have a careful pre-processing and tokenization because those serve as your foundation data for your model and then uh we see the complexity of the architecture being balanced with your competition environment and then you are creating a training procedure which is um let's say scalable in nature and then uh we have seen the significance of evaluation and uh like you know improve of improvement over time like iterative Improvement that significance is there so what are the future of llm U like know it is going to have a potential impact on every industry which is there and there are lots of ongoing research and challenge challenges and opportunities are there we can see uh what we can grab out of those but now if you see see you have literally overloaded information of the whole pre-training pipeline of you know like anything when everything which is related to pre-training like you know in the 62 slides you have all the information which is related to training uh process so I hope you all would have liked this video If you like this video please hit the like button share it with your friends if you haven't subscribed to the Channel please the Subscribe button hit the Bell icon I'll see you all in the next video Until Then happy learning
